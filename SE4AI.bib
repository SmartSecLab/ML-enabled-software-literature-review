@misc{:17445,
  title = {Software {{Engineering}} for {{AI-Enabled Systems}} ({{SE4AI}})},
  urldate = {2022-12-13},
  howpublished = {https://ckaestne.github.io/seai/S2020/},
  file = {/Users/guru/Zotero/storage/JM2WJ96K/S2020.html}
}

@article{abdulqadder2020:multilayered,
  title = {Multi-Layered Intrusion Detection and Prevention in the {{SDN}}/{{NFV}} Enabled Cloud of {{5G}} Networks Using {{AI-based}} Defense Mechanisms},
  author = {Abdulqadder, Ihsan H and Zhou, Shijie and Zou, Deqing and Aziz, Israa T. and Akber, Syed Muhammad Abrar},
  year = {2020},
  month = oct,
  journal = {Computer Networks},
  volume = {179},
  pages = {107364},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2020.107364},
  urldate = {2023-01-15},
  abstract = {Software defined networking (SDN), network function virtualization (NFV), and cloud computing are receiving significant attention in 5G networks. However, this attention creates a new challenge for security provisioning in these integrated technologies. Research in the field of SDN, NFV, cloud computing, and 5G has recently focused on the intrusion detection and prevention system (IDPS). Existing IDPS solutions are inadequate, which could cause large resource wastage and several security threats. To alleviate security issues, timely detection of an attacker is important. Thus, in this paper, we propose a novel approach that is referred to as multilayered intrusion detection and prevention (ML-IDP) in an SDN/NFV-enabled cloud of 5G networks. The proposed approach defends against security attacks using artificial intelligence (AI). In this paper, we employed five layers: data acquisition layer, switches layer, domain controllers (DC) layer, smart controller (SC) layer, and virtualization layer (NFV infrastructure). User authentication is held in the first layer using the Four-Q-Curve algorithm. To address the flow table overloading attack in the switches layer, the game theory approach, which is executed in the IDP agent, is proposed. The involvement of the IDP agent is to completely avoid a flow table overloading attack by a deep reinforcement learning algorithm, and thus, it updates the current state of all switches. In the DC layer, packets are processed and classified into two classes (normal and suspicious) by a Shannon Entropy function. Normal packets are forwarded to the cloud via the SC. Suspicious packets are sent to the VNF using a growing multiple self-organization map (GM-SOM). The proposed ML-IDP system is evaluated using NS3.26 for different security attacks, including IP Spoofing, flow table overloading, DDoS, Control Plane Saturation, and host location hijacking. From the experiment results, we proved that the ML-IDP with AI-based defense mechanisms effectively detects and prevents attacks.},
  langid = {english},
  keywords = {And artificial intelligence,Intrusion detection and prevention,Multilayered architecture,SDN/NFV Cloud of 5G},
  file = {/Users/guru/Zotero/storage/LAER2FGT/S1389128619310205.html}
}

@inproceedings{Abdulqadder2020499,
  type = {Conference Paper},
  title = {Bloc-Sec: {{Blockchain-based}} Lightweight Security Architecture for {{5G}}/{{B5G}} Enabled {{SDN}}/{{NFV}} Cloud of {{IoT}}},
  author = {Abdulqadder, Ihsan H. and Zhou, Shijie and Zou, Deqing and Aziz, Israa T. and Akber, Syed Muhammad Abrar},
  year = {2020},
  series = {International {{Conference}} on {{Communication Technology Proceedings}}, {{ICCT}}},
  volume = {2020-October},
  pages = {499--507},
  doi = {10.1109/ICCT50939.2020.9295823},
  abstract = {Lightweight security provisioning is a recent topic in resource-constrained software-defined networking (SDN). Integration between SDN, internet of things (IoT), network function virtualization (NFV) provides massive application services for 5G/B5G communications. Current state-of-the-art security solutions were caused by higher resource consumption, weaker key distribution, and forged credentials. In this paper, we proposed a blockchain-based lightweight security architecture (Bloc-Sec) in SDN/NFV that enabled cloud of IoT networks with 5G/B5G communication. Firstly, we authenticate all IoT devices to the blockchain server using multiple factors Blake-256 hashing algorithm. Secondly, we select the optimal virtual network function (VNF) using the cuttlefish optimization algorithm. Thirdly, blockchain is invoked to keep store the hashed flow rules that are deployed in VNF. Fourthly, the controller is involved in the packet classification by proposing the packet header inspection and packet content inspection using spiking dual fuzzy neural networks. For testing, NS3.26 is implemented and the performance evaluated. \textcopyright{} 2020 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{abubakar2020interplay,
  title = {Interplay of Machine Learning and Software Engineering for Quality Estimations},
  booktitle = {2020 International Conference on Communications, Computing, Cybersecurity, and Informatics ({{CCCI}})},
  author = {Abubakar, Hamza and Obaidat, Mohammad S and Gupta, Aaryan and Bhattacharya, Pronaya and Tanwar, Sudeep},
  year = {2020},
  pages = {1--6},
  organization = {{IEEE}},
  file = {/Users/guru/Zotero/storage/L5Z33RBF/Abubakar et al. - 2020 - Interplay of machine learning and software enginee.pdf}
}

@inproceedings{aggarwal2019user,
  title = {User Privacy Risk Analysis for the {{Internet}} of {{Things}}},
  booktitle = {2019 Sixth International Conference on Internet of Things: {{Systems}}, Management and Security ({{IOTSMS}})},
  author = {Aggarwal, Akash and Asif, Waqar and Azam, Habibul and Markovic, Milan and Rajarajan, Muttukrishnan and Edwards, Peter},
  year = {2019},
  pages = {259--264},
  organization = {{IEEE}}
}

@inproceedings{Agrawal2020,
  type = {Conference Paper},
  title = {The next Generation of Human-Drone Partnerships: {{Co-designing}} an Emergency Response System},
  author = {Agrawal, Ankit and Abraham, Sophia J. and Burger, Benjamin and Christine, Chichi and Fraser, Luke and Hoeksema, John M. and Hwang, Sarah and Travnik, Elizabeth and Kumar, Shreya and Scheirer, Walter and {Cleland-Huang}, Jane and Vierhauser, Michael and Bauer, Ryan and Cox, Steve},
  year = {2020},
  series = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},
  doi = {10.1145/3313831.3376825},
  abstract = {The use of semi-autonomous Unmanned Aerial Vehicles (UAV) to support emergency response scenarios, such as fire surveillance and search and rescue, offers the potential for huge societal benefits. However, designing an effective solution in this complex domain represents a "wicked design" problem, requiring a careful balance between trade-offs associated with drone autonomy versus human control, mission functionality versus safety, and the diverse needs of different stakeholders. This paper focuses on designing for situational awareness (SA) using a scenario-driven, participatory design process. We developed SA cards describing six common design-problems, known as SA demons, and three new demons of importance to our domain. We then used these SA cards to equip domain experts with SA knowledge so that they could more fully engage in the design process. We designed a potentially reusable solution for achieving SA in multi-stakeholder, multi-UAV, emergency response applications. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{aheleroff2021:mass,
  title = {Mass {{Personalisation}} as a {{Service}} in {{Industry}} 4.0: {{A Resilient Response Case Study}}},
  shorttitle = {Mass {{Personalisation}} as a {{Service}} in {{Industry}} 4.0},
  author = {Aheleroff, Shohin and Mostashiri, Naser and Xu, Xun and Zhong, Ray Y.},
  year = {2021},
  month = oct,
  journal = {Advanced Engineering Informatics},
  volume = {50},
  pages = {101438},
  issn = {1474-0346},
  doi = {10.1016/j.aei.2021.101438},
  urldate = {2023-01-15},
  abstract = {The Fourth Industrial Revolution (Industry 4.0) leads to mass personalisation as an emerging manufacturing paradigm. Mass personalisation focuses on uniquely made products to individuals at scale. Global challenges encourage mass personalisation manufacturing with efficiency competitive to mass production. Driven by individualisation as a trend and enabled by increasing digitalisation, mass personalisation can go beyond today's mass customisation. This paper aims to introduce Mass Personalisation as a Service (MPaaS) to address unique and complex requirements at scale by harnessing Industry 4.0 technologies, including Internet of Things, Additive Manufacturing, Big Data, Cloud Manufacturing, Digital Twin, and Blockchain. A case study for the implementation of MPaaS in personalised face masks is presented. The workforce with constant exposure to contaminants requires personal protective equipment (PPE), such as facemasks, for longer hours resulting in pressure-related ulcers. This prolonged use of PPE highlights the importance of personalisation to avoid ulcers and other related health concerns. Most studies have used Additive Manufacturing for individualisation and cloud capabilities for large-scale manufacturing. This study develops a framework and mathematical model to demonstrate the capability of the proposed solution to address one of the most critical challenges by making personalised face masks as an essential PPE in the critical industrial environment.},
  langid = {english},
  keywords = {Additive Manufacturing,Cloud Manufacturing,Industry 4.0,Internet of Things,Personalisation,Service Oriented Architecture},
  file = {/Users/guru/Zotero/storage/E4ZJICB3/S1474034621001907.html}
}

@article{Ahmed20151193,
  type = {Article},
  title = {High-Throughput Transmission-Quality-Aware Broadcast Routing in Cognitive Radio Networks},
  author = {Ahmed, Ejaz and Qadir, Junaid and Baig, Adeel},
  year = {2015},
  journal = {Wireless Networks},
  volume = {21},
  number = {4},
  pages = {1193--1210},
  doi = {10.1007/s11276-014-0843-6},
  abstract = {Cognitive radio is an enabling technology of dynamic spectrum access (DSA) networking. In DSA, unlicensed secondary users can coexist with primary licensed users and can share the radio spectrum opportunistically. Broadcasting is an important networking primitive that is useful for many CRN applications such as control information dissemination, warning notification, etc. Unfortunately, the sporadic channels availability degrades the performance of broadcast routing. The quality of a broadcast transmission on a particular channel depends on the channel quality of all the receivers for the same transmitter. Current broadcast routing protocols lack transmission quality awareness. In this paper, we develop a transmission quality-aware broadcasting framework, comprising algorithm for transmission quality-aware broadcast routing in multi-radio dynamic-spectrum-access CRNs, and formulate a transmission quality metric wherein we consider a receiver-centric view rather than a transmission-centric view. We perform a detailed simulation performance evaluation of our proposed framework using OMNeT++. The proposed broadcast routing algorithm is validated by comparing results with state-of-the-art routing algorithms. Analysis of the results shows average performance gains of approximately 40 \% in throughput and packet delivery ratio. \textcopyright{} 2014, Springer Science+Business Media New York.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Akbar2017,
  type = {Article},
  title = {A Meta-Model of Software Development Project States Behavioral Study of Software Projects},
  author = {Akbar, Rehan and Hassan, Mohd Fadzil and Abdullah, Azrai},
  year = {2017},
  journal = {Journal of Software: Evolution and Process},
  volume = {29},
  number = {4},
  doi = {10.1002/smr.1820},
  abstract = {Software development project during its lifecycle passes through various states. These states describe the condition, status, and behavior of software projects. In the present study, these states are defined based on the various activities performed during project lifecycle like initial environment setup, requirement analysis, coding, testing, problem resolution, and completion. The taxonomy of these states and substates is defined, and a project states meta-model is designed. The meta-model is composed of states and substates of the software projects. Detailed case studies of real projects have been conducted to validate the states of the meta-model. Evidences are collected; and events and observations are recorded about existence of the states, execution flow, duration, and behavior. The evidences, events, and observations are presented in the sequence to translate them into results. Results show that project states exist in all projects such that each software project passes through these states serially and in particular cases, a few states may exist in parallel. Project states show the status and progress of the software projects. It is found that issues in software projects can effectively be resolved by performing micro project management activities of the projects states. Project states meta-model provides basic structure for deriving new models. Copyright \textcopyright{} 2016 John Wiley \& Sons, Ltd.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Albuquerque202221,
  type = {Conference Paper},
  title = {Comprehending the Use of Intelligent Techniques to Support Technical Debt Management},
  author = {Albuquerque, Danyllo and Guimaraes, Everton and Tonin, Graziela and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  series = {Proceedings - {{International Conference}} on {{Technical Debt}} 2022, {{TechDebt}} 2022},
  pages = {21--30},
  doi = {10.1145/3524843.3528097},
  abstract = {Technical Debt (TD) refers to the consequences of taking shortcuts when developing software. Technical Debt Management (TDM) becomes complex since it relies on a decision process based on multiple and heterogeneous data, which are not straightforward to be synthesized. In this context, there is a promising opportunity to use Intelligent Techniques to support TDM activities since these techniques explore data for knowledge discovery, reasoning, learning, or supporting decision-making. Although these techniques can be used for improving TDM activities, there is no empirical study exploring this research area. This study aims to identify and analyze solutions based on Intelligent Techniques employed to sup-port TDM activities. A Systematic Mapping Study was performed, covering publications between 2010 and 2020. From 2276 extracted studies, we selected 111 unique studies. We found a positive trend in applying Intelligent Techniques to support TDM activities, being Machine Learning, Reasoning Under Uncertainty, and Natu-ral Language Processing the most recurrent ones. Identification, measurement, and monitoring were the more recurrent TDM ac-tivities, whereas Design, Code, and Architectural were the most frequently investigated TD types. Although the research area is up-and-coming, it is still in its infancy, and this study provides a baseline for future research. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{alizadeh2019:refbot,
  title = {{{RefBot}}: {{Intelligent}} Software Refactoring Bot},
  booktitle = {2019 34th {{IEEE}}/{{ACM}} International Conference on Automated Software Engineering ({{ASE}})},
  author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
  year = {2019},
  month = nov,
  pages = {823--834},
  issn = {2643-1572},
  doi = {10.1109/ASE.2019.00081},
  abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
  keywords = {Bot (Internet),Manuals,Measurement,Object oriented modeling,Pipelines,refactoring,Software,Software bot,Software quality,Tools},
  file = {/Users/guru/Zotero/storage/9LQ72XWF/Alizadeh et al_2019_RefBot_2019 34th IEEEACM international conference on automated software engineering (ASE).pdf}
}

@inproceedings{Alkubaisy202180,
  type = {Conference Paper},
  title = {{{ConfIs}}: {{A}} Tool for Privacy and Security Analysis and Conflict Resolution for Supporting {{GDPR}} Compliance through Privacy-by-Design},
  author = {Alkubaisy, Duaa and Piras, Luca and {Al-Obeidallah}, Mohammed Ghazi and Cox, Karl and Mouratidis, Haralambos},
  year = {2021},
  series = {International {{Conference}} on {{Evaluation}} of {{Novel Approaches}} to {{Software Engineering}}, {{ENASE}} - {{Proceedings}}},
  volume = {2021-April},
  pages = {80--91},
  abstract = {Privacy and security requirements, and their potential conflicts, are increasingly having more and more importance. It is becoming a necessary part to be considered, starting from the very early stages of requirements engineering, and in the entire software engineering cycle, for the design of any software system. In the last few years, this has been even more emphasized and required by the law. A relevant example is the case of the General Data Protection Regulation (GDPR), which requires organizations, and their software engineers, to enforce and guarantee privacy-by-design to make their platforms compliant with the regulation. In this context, complex activities related to privacy and security requirements elicitation, analysis, mapping and identification of potential conflicts, and the individuation of their resolution, become crucial. In the literature, there is not available a comprehensive requirement engineering oriented tool for supporting the requirements analyst. In this paper, we propose ConfIs, a tool for supporting the analyst in performing a process covering these phases in a systematic and interactive way. We present ConfIs and its process with a realistic example from DEFeND, an EU project aiming at supporting organizations in achieving GDPR compliance. In this context, we evaluated ConfIs by involving privacy/security requirements experts, which recognized our tool and method as supportive, concerning these complex activities. Copyright \textcopyright{} 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
  publication_stage = {Final},
  source = {Scopus}
}

@article{alnafessah2021:qualityaware,
  title = {Quality-Aware {{DevOps}} Research: {{Where}} Do We Stand?},
  author = {Alnafessah, Ahmad and Gias, Alim Ul and Wang, Runan and Zhu, Lulai and Casale, Giuliano and Filieri, Antonio},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {44476--44489},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3064867},
  abstract = {DevOps is an emerging paradigm that reduces the barriers between developers and operations teams to offer continuous fast delivery and enable quick responses to changing requirements within the software life cycle. A significant volume of activity has been carried out in recent years with the aim of coupling DevOps stages with tools and methods to improve the quality of the produced software and the underpinning delivery methodology. While the research community has produced a sustained effort by conducting numerous studies and innovative development tools to support quality analyses within DevOps, there is still a limited cohesion between the research themes in this domain and a shortage of surveys that holistically examine quality engineering work within DevOps. In this paper, we address the gap by comprehensively surveying existing efforts in this area, categorizing them according to the stage of the DevOps lifecycle to which they primarily contribute. The survey holistically spans across all the DevOps stages, identify research efforts to improve architectural design, modeling and infrastructure-as-code, continuous-integration/continuous-delivery (CI/CD), testing and verification, and runtime management. Our analysis also outlines possible directions for future work in quality-aware DevOps, looking in particular at AI for DevOps and DevOps for AI software.},
  keywords = {artificial intelligence,Artificial intelligence,CI/CD,Computer architecture,DevOps,infrastructure as code,primary,Production,Software,Software architecture,testing,Testing,Tools,verification},
  file = {/Users/guru/Zotero/storage/6JA74Y95/Alnafessah et al_2021_Quality-aware DevOps research_IEEE access practical innovations, open solutions.pdf}
}

@article{Alshammari2022,
  type = {Article},
  title = {Analytical Evaluation of {{SOA}} and {{SCRUM}} Business Process Management Approaches for {{IoT-Based}} Services Development},
  author = {Alshammari, Fahad H.},
  year = {2022},
  journal = {Scientific Programming},
  volume = {2022},
  doi = {10.1155/2022/3556809},
  abstract = {The SCRUM approach and Service-Oriented Architecture (SOA) framework are critical in assessing the factors that influence the efficiency of a business process and ensuring that business objectives are fulfilled, and the process is on track to meet those objectives. Flexibility and change adoption are critical features for both SCRUM and SOA approaches. Even though both sides encourage agility, the integration of the two independent concepts (SOA is the architectural framework while SCRUM is the development process) should be considered before being used in software management and development projects. This study assessed and analyzed both SCRUM and SOA's diverse and different software architectural frameworks and development methodologies as well as their environment, which is integrated with the context of software project management and development setup for the software development industry. In addition, this study explores the similarities between the SCRUM process model and the SOA architectural framework to see if they are compatible and, if so, how they may be combined to enhance SOA-based projects. This research also looks at how to build and use a SCRUM methodology for large-scale SOA projects. As a result, SCRUM was chosen as the software development methodology for a research and development project based on SOA. In terms of project development and implementation, the complete project structure is made up of eight main parts. \textcopyright{} 2022 Fahad H. Alshammari.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{amershi2019:software,
  title = {Software Engineering for Machine Learning: {{A}} Case Study},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st International Conference on Software Engineering: {{Software}} Engineering in Practice ({{ICSE-SEIP}})},
  author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  year = {2019},
  month = may,
  pages = {291--300},
  doi = {10.1109/ICSE-SEIP.2019.00042},
  abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be "entangled" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
  keywords = {artifical intelligence,Buildings,data,Data models,machine learning,Machine learning,Organizations,primary,process,repeated_study,Software,software engineering,Software engineering},
  file = {/Users/guru/Zotero/storage/KGDKNELG/Amershi et al_2019_Software engineering for machine learning_2019 IEEEACM 41st international conference on software engineering Software engineering in practice (ICSE-SEIP).pdf}
}

@inproceedings{amershi2019:softwarea,
  title = {Software Engineering for Machine Learning: A Case Study},
  shorttitle = {Software Engineering for Machine Learning},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}}},
  author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  year = {2019},
  month = may,
  series = {{{ICSE-SEIP}} '19},
  pages = {291--300},
  publisher = {{IEEE Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1109/ICSE-SEIP.2019.00042},
  urldate = {2023-01-15},
  abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components --- models may be "entangled" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.},
  keywords = {AI,data,primary,process,software engineering},
  file = {/Users/guru/Zotero/storage/3X2SYXM4/Amershi et al_2019_Software engineering for machine learning_Proceedings of the 41st International Conference on Software Engineering Software Engineering in Practice.pdf}
}

@inproceedings{Amershi2019291,
  type = {Conference Paper},
  title = {Software Engineering for Machine Learning: {{A}} Case Study},
  author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  year = {2019},
  series = {Proceedings - 2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}}, {{ICSE-SEIP}} 2019},
  pages = {291--300},
  doi = {10.1109/ICSE-SEIP.2019.00042},
  abstract = {Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be 'entangled' in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations. \textcopyright{} 2019 IEEE.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary,repeated_study}
}

@inproceedings{amershi2019software,
  title = {Software Engineering for Machine Learning: {{A}} Case Study},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st International Conference on Software Engineering: {{Software}} Engineering in Practice ({{ICSE-SEIP}})},
  author = {Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas},
  year = {2019},
  pages = {291--300},
  organization = {{IEEE}},
  file = {/Users/guru/Zotero/storage/QJ7SC6DZ/Amershi et al. - 2019 - Software Engineering for Machine Learning A Case .pdf}
}

@inproceedings{anh2021:imbalanced,
  title = {An Imbalanced Deep Learning Model for Bug Localization},
  booktitle = {2021 28th Asia-Pacific Software Engineering Conference Workshops ({{APSEC}} Workshops)},
  author = {Anh, Bui Thi Mai and Luyen, Nguyen Viet},
  year = {2021},
  month = dec,
  pages = {32--40},
  doi = {10.1109/APSECW53869.2021.00017},
  abstract = {Debugging and locating faulty source files are tedious and time-consuming tasks. To improve the productivity and to help developers focus on crucial files, automated bug localization models have been proposed for years. These models recommend buggy source files by ranking them according to their relevance to a given bug report. There are two significant challenges in this research field: (i) narrowing the lexical gap between bug reports which are typically described using natural languages and source files written in programming languages; (ii) reducing the impact of imbalanced data distribution in model training as a far fewer of source files relate to a given bug report while the majority of them are not relevant. In this paper, we propose a deep neural network model to investigate essential information hidden within bug reports and source files through capturing not only lexical relations but also semantic details as well as domain knowledge features such as historical bug fixings, code change history. To address the skewed class distribution, we apply a focal loss function combining with a bootstrapping method to rectify samples of the minority class within iterative training batches to our proposed model. We assessed the performance of our approach over six large scale Java open-source projects. The empirical results have showed that the proposed method outperformed other state-of-the-art models by improving the Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) scores from 3\% to 11\% and from 2\% to 14\%, respectively.},
  keywords = {bootstrapping,bug localization,Computer bugs,Conferences,Deep learning,deep neural network,imbalanced data-set,Location awareness,Neural networks,Productivity,Training},
  file = {/Users/guru/Zotero/storage/UPCQUKEM/Anh and Luyen - 2021 - An imbalanced deep learning model for bug localiza.pdf}
}

@inproceedings{Antonini2022,
  type = {Conference Paper},
  title = {Tiny-{{MLOps}}: {{A}} Framework for Orchestrating {{ML}} Applications at the Far Edge of {{IoT}} Systems},
  author = {Antonini, Mattia and Pincheira, Miguel and Vecchio, Massimo and Antonelli, Fabio},
  year = {2022},
  series = {{{IEEE Conference}} on {{Evolving}} and {{Adaptive Intelligent Systems}}},
  volume = {2022-May},
  doi = {10.1109/EAIS51927.2022.9787703},
  abstract = {Empowering the Internet of Things devices with Artificial Intelligence capabilities can transform all vertical applications domains within the next few years. Current approaches favor hosting Machine Learning (ML) models on Linux-based single-board computers. Nevertheless, these devices' cost and energy requirements limit the possible application scenarios. Conversely, today's available 32-bit microcontrollers have much lower costs and only need a few milliwatts to operate, making them an energy-efficient and cost-effective alternative. However, the latter devices, usually referred to as far edge devices, have stringent resource constraints and host non-Linux-based embedded real-time operating systems. Therefore, orchestrating such devices executing portions of ML applications represents a major challenge with current tools and frameworks. This paper formally introduces the Tiny-MLOps framework as the specialization of standard ML orchestration practices, including far edge devices in the loop. To this aim, we will tailor each phase of the classical ML orchestration loop to the reduced resources available onboard typical IoT devices. We will rely on the proposed framework to deliver adaptation and evolving capabilities to resource-constrained IoT sensors mounted on an industrial rotary machine to detect anomalies. As a feasibility study, We will show how to programmatically re-deploy ML-based anomaly detection models to far edge devices. Our preliminary experiments measuring the system performance in terms of deployment, loading, and inference latency of the ML models will corroborate the usefulness of our proposal. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Apaza201827,
  type = {Conference Paper},
  title = {{{ERS-Tool}}: {{Hybrid}} Model for Software Requirements Elicitation in {{Spanish}} Language},
  author = {Apaza, Ren{\'a}n Dar{\'i}o Gonzales and Barrios, Jhon Edilberto Monroy and Becerra, Diego Alonso Iquira and Quispe, Jos{\'e} Alfredo Herrera},
  year = {2018},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {27--30},
  doi = {10.1145/3220228.3220255},
  abstract = {The nature of the software requirements is subjective and varied. For this reason the level of complexity increases according to the volume, especially when the requirements are made in a natural language. Therefore obtain quality software requirements that are understandable and unambiguous in the Spanish language becomes a necessity. First, a controlled syntax was proposed to express software requirements taking into account the static and dynamic behavior among the different actors of the system, where the expressions are elaborated based on the Backus\textendash Naur form (BNF). Then a set of writing rules were adapted to the Spanish language, creating four additional rules. Finally, the results of the case study had high accuracy in understandability; also the ambiguity of requirements elicitation was reduced. In addition to improving the development of software engineering activities, since there are no tools available for the elicitation of software requirements with language Spanish. \textcopyright{} 2018 Association for Computing Machinery.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{atif2021:chapter,
  title = {Chapter 1 - {{Artificial Intelligence}} ({{AI}})-Enabled Remote Learning and Teaching Using {{Pedagogical Conversational Agents}} and {{Learning Analytics}}},
  booktitle = {Intelligent {{Systems}} and {{Learning Data Analytics}} in {{Online Education}}},
  author = {Atif, Amara and Jha, Meena and Richards, Deborah and Bilgin, Ayse A.},
  editor = {Caball{\'e}, Santi and Demetriadis, Stavros N. and {G{\'o}mez-S{\'a}nchez}, Eduardo and Papadopoulos, Pantelis M. and Weinberger, Armin},
  year = {2021},
  month = jan,
  series = {Intelligent {{Data-Centric Systems}}},
  pages = {3--29},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-823410-5.00013-9},
  urldate = {2023-01-15},
  abstract = {Advancements in Artificial Intelligence (AI) have potentially created new ways to teach and learn, such as the use of Learning Analytics (LA) to monitor and support students using data captured in Learning Management Systems (LMS). To add human rather than data-based support, in this chapter, we present our use of AI-enabled Pedagogical Conversational Agents (CAs) over the past 12 months in multiple units/subjects across two universities. These CAs play a role similar to a teacher or peer learner by sharing the expertise they have acquired from the knowledge contained in student\textendash teacher social interactions in LMS forums and grade book teacher feedback. Unlike teachers or peers, these CAs can be contacted anonymously at any time, they don't mind being asked the same question repeatedly and they can empower students to explore options and outcomes. We conclude the chapter with a discussion of the potential of LA to automate CA interactions.},
  isbn = {978-0-12-823410-5},
  langid = {english},
  keywords = {artificial intelligence,higher education,learning analytics,learning management system,Pedagogical conversational agents,remote teaching and learning,student engagement},
  file = {/Users/guru/Zotero/storage/WP34S6DH/B9780128234105000139.html}
}

@inproceedings{Ausmanas2012175,
  type = {Conference Paper},
  title = {Operations Engineering and Management in {{OHMS}} Company},
  author = {Ausmanas, Naglis and Bargelis, Algirdas},
  year = {2012},
  series = {International {{Conference}} on {{Industrial Logistics}}, {{ICIL}} 2012 - {{Conference Proceedings}}},
  pages = {175--181},
  abstract = {Global industrial logistics tasks are to seek and help better operations engineering and management in order handled manufacturing systems (OHMS). The industrial logistics in lean manufacturing approach is considered in this paper. The collaboration in-bound, internal and out-bound areas among Lithuanian producers are considered in this research. The forecasting model for manufacturing process, time and cost has been examined in this research. Appropriate knowledge base and intelligent software has been applied and tested for manufacturing cost forecasting at the early stage of a new order engineering stage.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Ayala2022,
  type = {Conference Paper},
  title = {{{DOGO4ML}}: {{Development}}, Operation and Data Governance for {{ML-based}} Software Systems},
  author = {Ayala, Claudia and Bilalli, Besim and G{\'o}mez, Cristina and {Mart{\'i}nez-Fern{\'a}ndez}, Silverio},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3144},
  abstract = {Machine Learning based Software Systems (MLSS) are becoming increasingly pervasive in today's society and can be found in virtually every domain. Building MLSS is challenging due to their interdisciplinary nature. MLSS engineering encompasses multiple disciplines, of which Data Engineering and Software Engineering appear as most relevant. The DOGO4ML project aims at reconciling these two disciplines for providing a holistic end-to-end framework to develop, operate and govern MLSS and their data. It proposes to combine and intertwine two software cycles: the DataOps and the DevOps lifecycles. The DataOps lifecycle manages the complexity of dealing with the big data needed by ML models, while the DevOps lifecycle is in charge of building the system that embeds these models. In this paper, we present the main vision and goals of the project as well as its expected contributions and outcomes. Although the project is in its initial stage, the progress of the research undertaken so far is detailed. \textcopyright{} 2021 The Authors.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/4KRPVF92/RP-paper3.pdf;/Users/guru/Zotero/storage/TA6RWSUI/Ayala et al. - 2022 - DOGO4ML Development, operation and data governanc.pdf}
}

@article{bader2021ai,
  title = {{{AI}} in Software Engineering at {{Facebook}}},
  author = {Bader, Johannes and Kim, Sonia Seohyun and Luan, Frank Sifei and Chandra, Satish and Meijer, Erik},
  year = {2021},
  journal = {IEEE Software},
  volume = {38},
  number = {4},
  pages = {52--61},
  publisher = {{IEEE}}
}

@article{bamhdi2021:requirements,
  title = {Requirements Capture and Comparative Analysis of Open Source versus Proprietary Service Oriented Architecture},
  author = {Bamhdi, Alwi},
  year = {2021},
  month = feb,
  journal = {Computer Standards \& Interfaces},
  volume = {74},
  pages = {103468},
  issn = {0920-5489},
  doi = {10.1016/j.csi.2020.103468},
  urldate = {2023-01-15},
  abstract = {Service Oriented Architecture (SOA) integrates information systems towards an agile and reusable service-based connectivity. It is an approach amalgamating large scale private/public computer systems and other resources with continuous phenomenal advent evolution and leveraging of the World Wide Web (WWW, commonly referred to as the Web) social media, mobile communications, Big Data (BD), data analytics, Machine Learning (ML) based optimisation, Cloud Computing (CC) and Internet of Things (IoT), commonly known as Advanced Technologies (AT). Implementing SOA, whether Open Source Software (OSS) or proprietary or absolute freeware is a choice to be made which depends on the organisation's requirements in light of AT as well as a host of delivery and security concerns. In this paper, a comparative analysis of an open source vs. proprietary SOA for large scale computer systems servicing AT is presented by examining their main efficacies, features, advantages and disadvantages and capturing their generic technical functional and non-functional requirements in a unified manner. Furthermore, the SOA evaluation criteria, recommendations and conclusions are also presented.},
  langid = {english},
  keywords = {Advanced technologies,Closed source proprietary,Open source,Requirements capture,SOA,Web 1.0/2.0/3.0/4./5.0/6.0,Web services access},
  file = {/Users/guru/Zotero/storage/9B7H9EIE/S0920548920303548.html}
}

@inproceedings{barati2019enhancing,
  title = {Enhancing User Privacy in {{IoT}}: Integration of {{GDPR}} and Blockchain},
  booktitle = {International Conference on Blockchain and Trustworthy Systems},
  author = {Barati, Masoud and Rana, Omer},
  year = {2019},
  pages = {322--335},
  organization = {{Springer}}
}

@inproceedings{Bartsch2010495,
  type = {Conference Paper},
  title = {Supporting Authorization Policy Modification in Agile Development of Web Applications},
  author = {Bartsch, Steffen},
  year = {2010},
  series = {{{ARES}} 2010 - 5th {{International Conference}} on {{Availability}}, {{Reliability}}, and {{Security}}},
  pages = {495--500},
  doi = {10.1109/ARES.2010.19},
  abstract = {Web applications are increasingly developed in Agile development processes. Business-centric Web applications need complex authorization policies to securely implement business processes. As part of the Agile process, integrating domain experts into the development of RBAC authorization policies improves the policies, but remains difficult. For policy modifications, high numbers of options need to be considered. To ease the management task and integrate domain experts, we propose an algorithm and prototype tool. The AI-based change-support algorithm helps to find the suitable modification actions according to desired changes that are given in policy test cases. We also present a prototype GUI for domain experts to employ the algorithm and report on early results of non-security experts using the tool in a real-world business Web application. \textcopyright{} 2010 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{bashari2018:selfadaptation,
  title = {Self-Adaptation of Service Compositions through Product Line Reconfiguration},
  author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
  year = {2018},
  month = oct,
  journal = {Journal of Systems and Software},
  volume = {144},
  pages = {84--105},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2018.05.069},
  urldate = {2023-01-15},
  abstract = {The large number of published services has motivated the development of tools for creating customized composite services known as service compositions. While service compositions provide high agility and development flexibility, they can also pose challenges when it comes to delivering guaranteed functional and non-functional requirements. This is primarily due to the highly dynamic environment in which services operate. In this paper, we propose adaptation mechanisms that are able to effectively maintain functional and non-functional quality requirements in service compositions derived from software product lines. Unlike many existing work, the proposed adaptation mechanism does not require explicit user-defined adaptation strategies. We adopt concepts from the software product line engineering paradigm where service compositions are viewed as a collection of features and adaptation happens through product line reconfiguration. We have practically implemented the proposed mechanism in ourMagus tool suite and performed extensive experiments, which show that our work is both practical and efficient for automatically adapting service compositions once violations of functional or non-functional requirements are observed.},
  langid = {english},
  keywords = {Feature model,Self adaptation,Service composition,Software product lines}
}

@inproceedings{Basu202116,
  type = {Conference Paper},
  title = {Designing a Bot for Efficient Distribution of Service Requests},
  author = {Basu, Arkadip and Banerjee, Kunal},
  year = {2021},
  series = {Proceedings - 2021 {{IEEE}}/{{ACM}} 3rd {{International Workshop}} on {{Bots}} in {{Software Engineering}}, {{BotSE}} 2021},
  pages = {16--20},
  doi = {10.1109/BotSE52550.2021.00011},
  abstract = {The tracking and timely resolution of service requests is one of the major challenges in agile project management. Having an efficient solution to this problem is a key requirement for Walmart to facilitate seamless collaboration across its different business units. The Jira software is one of the popular choices in industries for monitoring such service requests. A service request once logged into the system by a reporter is referred to as a (Jira) ticket which is assigned to an engineer for servicing. In this work, we explore how the tickets which may arise in any of the Walmart stores and offices distributed over several countries can be assigned to engineers efficiently. Specifically, we will discuss how the introduction of a bot for automated ticket assignment has helped in reducing the disparity in ticket assignment to engineers by human managers and also decreased the average ticket resolution time- thereby improving the experience for both the reporters and the engineers. Additionally, the bot sends reminders and status updates over different business communication platforms for timely tracking of tickets; it can be suitably modified to provision for human intervention in case of special needs by some teams. The current study conducted over data collected from various teams within Walmart shows the efficacy of our bot. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{batarseh2018:chapter,
  title = {Chapter 9 - {{A Deployment Life Cycle Model}} for {{Agricultural Data Systems Using Kansei Engineering}} and {{Association Rules}}},
  booktitle = {Federal {{Data Science}}},
  author = {Batarseh, Feras A. and Yang, Ruixin},
  editor = {Batarseh, Feras A. and Yang, Ruixin},
  year = {2018},
  month = jan,
  pages = {141--159},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-812443-7.00009-0},
  urldate = {2023-01-15},
  abstract = {The early promise of artificial intelligence that was sought by many is now partially drifting toward data science and big data analytics. Software engineers have been increasingly overwhelmed by the amounts of data available to them during development, but their main quandary is that they are unable to draw solid conclusions from that data without proper analytics. As a result of the recent evolution of software engineering, analytics have been directly applied to solve different challenges of the software development life cycle. Rarely however, have analytics been applied directly for deployment or user acceptance. This chapter introduces a life cycle model that uses the power of data analytics (i.e., association rules and Kansei) to guide a federal team through software testing, deployment, and user acceptance. The model is evaluated through an agricultural data system and is deployed to federal employees; experimental results are collected and presented.},
  isbn = {978-0-12-812443-7},
  langid = {english},
  keywords = {Association rules,Data analytics,Federal analyst,Kansei engineering,Software testing,System deployment,User adoption},
  file = {/Users/guru/Zotero/storage/U5FKQHSN/B9780128124437000090.html}
}

@incollection{batarseh2020:10,
  title = {10 - {{The}} Application of Artificial Intelligence in Software Engineering: A Review Challenging Conventional Wisdom},
  shorttitle = {10 - {{The}} Application of Artificial Intelligence in Software Engineering},
  booktitle = {Data {{Democracy}}},
  author = {Batarseh, Feras A. and Mohod, Rasika and Kumar, Abhinav and Bui, Justin},
  editor = {Batarseh, Feras A. and Yang, Ruixin},
  year = {2020},
  month = jan,
  pages = {179--232},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-818366-3.00010-1},
  urldate = {2023-01-15},
  abstract = {The field of artificial intelligence (AI) is witnessing a recent upsurge in research, tools development, and deployment of applications. Multiple software companies are shifting their focus to developing intelligent systems; and many others are deploying AI paradigms to their existing processes. In parallel, the academic research community is injecting AI paradigms to provide solutions to traditional engineering problems. Similarly, AI has evidently been proved useful to software engineering (SE). When one observes the SE phases (requirements, design, development, testing, release, and maintenance), it becomes clear that multiple AI paradigms (such as neural networks, machine learning, knowledge-based systems, natural language processing) could be applied to improve the process and eliminate many of the major challenges that the SE field has been facing. This survey chapter is a review of the most commonplace methods of AI applied to SE. The review covers methods between years 1975\textendash 2017, for the requirements phase, 46 major AI-driven methods are found, 19 for design, 15 for development, 68 for testing, and 15 for release and maintenance. Furthermore, the purpose of this chapter is threefold; firstly, to answer the following questions: is there sufficient intelligence in the SE lifecycle? What does applying AI to SE entail? Secondly, to measure, formulize, and evaluate the overlap of SE phases and AI disciplines. Lastly, this chapter aims to provide serious questions to challenging the current conventional wisdom (i.e., status quo) of the state-of-the-art, craft a call for action, and to redefine the path forward.},
  isbn = {978-0-12-818366-3},
  langid = {english},
  keywords = {Artificial intelligence paradigms,Design,Lifecycle phase,Requirements,Testing},
  file = {/Users/guru/Zotero/storage/ELSX763V/B9780128183663000101.html}
}

@article{bates2020:literature,
  title = {Literature {{Listing}}},
  author = {Bates, Susan},
  year = {2020},
  month = mar,
  journal = {World Patent Information},
  volume = {60},
  pages = {101946},
  issn = {0172-2190},
  doi = {10.1016/j.wpi.2019.101946},
  urldate = {2023-01-15},
  abstract = {The quarterly Literature Listing is intended as a current awareness service for readers indicating newly published books, journal and conference articles on: patent search techniques, databases, analysis and classifications; patent searcher certification; patents relating to a) life sciences and pharmaceuticals and b) software; patent policy and strategic issues; trade marks; designs; domain names; and articles reviewing historical aspects of intellectual property or reviewing specific topics/persons. The current Literature Listing was compiled end-November 2019. Key resources used are Scopus, Digital Commons, publishers' RSS feeds, and serendipity! Please feel free to send the author details of newly published reports/monographs/books for potential inclusion.},
  langid = {english},
  keywords = {Current awareness,Designs,Literature listing,Patent analysis,Patents,Trade marks},
  file = {/Users/guru/Zotero/storage/I3D5TY6R/Bates_2020_Literature Listing_World Patent Information.pdf;/Users/guru/Zotero/storage/JM5RGJSR/S017221901930153X.html}
}

@article{Bauer2020445,
  type = {Article},
  title = {Modular Change Impact Analysis in Factory Systems: {{Guideline}} for Individual Configuration},
  author = {Bauer, Harald and Haase, Paul and Sippl, Fabian and Ramakrishnan, Robert and Schilp, Johannes and Reinhart, Gunther},
  year = {2020},
  journal = {Production Engineering},
  volume = {14},
  number = {4},
  pages = {445--456},
  doi = {10.1007/s11740-020-00979-4},
  abstract = {Shorter product innovation cycles, high variant products, and demand fluctuation, as well as equipment life cycles and technology life cycles force manufacturing companies to regularly change their manufacturing system. In order to address this challenge, an efficient and structured change management is required. As change causes and factory elements are connected via a complex network of relations and flows, an essential step in change management is the evaluation of considered adjustments with regard to their effects on the current production system. Depending on the context of the application, change impact analysis must process specific inputs and deliver different results. Current approaches, however, each focus only on selected aspects of the versatility of change effects. To address this challenge, this paper presents a modular approach for the individual design of change impact analysis. \textcopyright{} 2020, The Author(s).},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{bayar2022:comprehensive,
  title = {A Comprehensive Big Data Framework for Energy Markets: {{Oil}} and Gas Demand Forecasting},
  booktitle = {2022 3rd International Informatics and Software Engineering Conference ({{IISEC}})},
  author = {Bayar, Alp and Ko{\c c}, Burcu and G{\"o}kalp, Mert Onuralp and {\"O}zden, Baran and Yeldan, Yi{\u g}it and Eren, P. Erhan and Ko{\c c}yi{\u g}it, Altan},
  year = {2022},
  month = dec,
  pages = {1--6},
  doi = {10.1109/IISEC56263.2022.9998216},
  abstract = {Demand forecasting in the energy sector is essential for both countries and companies to plan their supply and demand. Agents in the highly volatile oil markets have to act fast and data-driven. In the literature, studies on oil or gasoline demand forecasting are carried out using traditional econometric and AI-based models, using static data for long periods. In this paper, short-term gasoline demand forecasting literature has been investigated. We focus on short-term demand prediction based on big data analytics and investigate potential data sources and architectures to collect data. To this end, several iterative meetings were conducted between the Data Science department, the Oil Trading department of an Oil \& Gas company, and researchers within an industry-academia cooperation project. Traditional data sources used for the problem are presented, and the applicability of real-time data to the problem is discussed. A big data architecture is proposed that can be used to predict the demand for petroleum products, mainly for gasoline in the U.S., for the transparency, amplitude, and availability of open data.},
  keywords = {Big Data,big data analytics,Companies,Computer architecture,demand,Demand forecasting,forecasting,gasoline,oil,Oils,Soft sensors,Supply and demand}
}

@inproceedings{begel2019:best,
  title = {Best Practices for Engineering {{AI-Infused}} Applications: {{Lessons}} Learned from Microsoft Teams},
  booktitle = {2019 {{IEEE}}/{{ACM}} Joint 7th International Workshop on Conducting Empirical Studies in Industry ({{CESI}}) and 6th International Workshop on Software Engineering Research and Industrial Practice ({{SER}}\&{{IP}})},
  author = {Begel, Andrew},
  year = {2019},
  month = may,
  pages = {1--1},
  issn = {2575-4793},
  doi = {10.1109/CESSER-IP.2019.00008},
  abstract = {Artificial intelligence and machine learning (AI/ML) are some of the newest trends to hit the software industry, compelling organizations to evolve their development processes to deliver novel products to their customers. In this talk, I describe a study in which we learned how Microsoft software teams develop AI/ML-based applications using a nine-stage AI workflow process informed by prior experiences developing early AI applications (e.g. search and NLP) and data science tools (e.g. application telemetry and bug reporting). Adapting this workflow into their pre-existing, well-evolved, Agile-like software engineering processes and job roles has resulted in a number of engineering challenges unique to the AI/ML domain, some universal to all teams, but others related to the amount of prior AI/ML experience and education the teams have. I tell you about some challenges and the solutions that teams have come up with. The lessons that Microsoft has learned can help other organizations embarking on their own path towards AI and ML.},
  keywords = {AI,Conferences,Industries,Industry Practice,Machine learning,Machine Learning,Organizations,primary,Software,Software engineering},
  file = {/Users/guru/Zotero/storage/99EEE6WJ/Begel - 2019 - Best practices for engineering AI-Infused applicat.pdf}
}

@inproceedings{begel2019:besta,
  title = {Best Practices for Engineering {{AI-infused}} Applications: Lessons Learned from {{Microsoft}} Teams},
  shorttitle = {Best Practices for Engineering {{AI-infused}} Applications},
  booktitle = {Proceedings of the {{Joint}} 7th {{International Workshop}} on {{Conducting Empirical Studies}} in {{Industry}} and 6th {{International Workshop}} on {{Software Engineering Research}} and {{Industrial Practice}}},
  author = {Begel, Andrew},
  year = {2019},
  month = may,
  series = {{{CESSER-IP}} '19},
  pages = {1},
  publisher = {{IEEE Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1109/CESSER-IP.2019.00008},
  urldate = {2023-01-15},
  abstract = {Artificial intelligence and machine learning (AI/ML) are some of the newest trends to hit the software industry, compelling organizations to evolve their development processes to deliver novel products to their customers. In this talk, I describe a study in which we learned how Microsoft software teams develop AI/ML-based applications using a nine-stage AI workflow process informed by prior experiences developing early AI applications (e.g. search and NLP) and data science tools (e.g. application telemetry and bug reporting). Adapting this workflow into their pre-existing, well-evolved, Agile-like software engineering processes and job roles has resulted in a number of engineering challenges unique to the AI/ML domain, some universal to all teams, but others related to the amount of prior AI/ML experience and education the teams have. I tell you about some challenges and the solutions that teams have come up with. The lessons that Microsoft has learned can help other organizations embarking on their own path towards AI and ML.},
  keywords = {AI,industry practice,machine learning},
  file = {/Users/guru/Zotero/storage/N44JEYPF/Begel_2019_Best practices for engineering AI-infused applications_Proceedings of the Joint 7th International Workshop on Conducting Empirical Studies in Industry and 6th International Workshop on Software En.pdf}
}

@article{behera2023:responsible,
  title = {Responsible Natural Language Processing: {{A}} Principlist Framework for Social Benefits},
  shorttitle = {Responsible Natural Language Processing},
  author = {Behera, Rajat Kumar and Bala, Pradip Kumar and Rana, Nripendra P. and Irani, Zahir},
  year = {2023},
  month = mar,
  journal = {Technological Forecasting and Social Change},
  volume = {188},
  pages = {122306},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2022.122306},
  urldate = {2023-01-15},
  abstract = {Businesses harness the power of natural language processing (NLP) to automate processes and make data-driven decisions. However, NLP raises concerns on a number of fronts due to its potential for disruption, which can be addressed with the assignment of responsibility. Therefore, responsible NLP (RNLP) can be designed as a principlist framework to ensure NLP systems are used in an ethical manner. The study proposes a principlist framework with the formulation of eight principlist ethical principles to ensure NLP is safe, secure and reliable for responsible decision making and subsequently results in social benefits. Using snowball sampling, data are collected from 15 informants, who represent senior-level positions in diversified industries. The analysis is performed with qualitative research methodology. The result produces two ethical practices. First is the adoption of RNLP as a disruptive technology for ethical decision making for social benefits and second is the creation of a culture of responsibility.},
  langid = {english},
  keywords = {Ethical work climate,Ethics,Responsible decision making,Responsible NLP,Social benefits},
  file = {/Users/guru/Zotero/storage/22EBQJQB/S0040162522008277.html}
}

@inproceedings{belani2019requirements,
  title = {Requirements Engineering Challenges in Building {{AI-based}} Complex Systems},
  booktitle = {2019 {{IEEE}} 27th International Requirements Engineering Conference Workshops ({{REW}})},
  author = {Belani, Hrvoje and Vukovic, Marin and Car, {\v Z}eljka},
  year = {2019},
  pages = {252--255},
  organization = {{IEEE}}
}

@article{bharosa2022:rise,
  title = {The Rise of {{GovTech}}: {{Trojan}} Horse or Blessing in Disguise? {{A}} Research Agenda},
  shorttitle = {The Rise of {{GovTech}}},
  author = {Bharosa, Nitesh},
  year = {2022},
  month = jul,
  journal = {Government Information Quarterly},
  volume = {39},
  number = {3},
  pages = {101692},
  issn = {0740-624X},
  doi = {10.1016/j.giq.2022.101692},
  urldate = {2023-01-15},
  abstract = {As GovTech solutions are steadily entering the public sector, they have yet to find their way into the mainstream literature. GovTech refers to socio-technical solutions \textendash{} that are developed and operated by private organisations \textendash{} intertwined with public sector components for facilitating processes in the public sector. GovTech solutions promise a superior customer journey for citizens and businesses compared to current government portals and front desks. GovTech solutions can be a blessing in disguise for governments struggling in their digital transformation journey, carrying the burden of public service innovation and replacing legacy systems with modern GovTech solutions. Yet, there are also concerns that GovTech solutions are a Trojan horse, exploiting the lack of technical knowledge at public agencies and shifting decision-making power from public agencies to market parties, thereby undermining digital sovereignty and public values. This paper develops a research agenda for GovTech based on a conceptual framework. This framework reveals four interrelated design areas for GovTech: institutional, governance, technical and human-centred design. Governments can employ the conceptual framework to further align and develop their strategies by focussing on GovTech governance, referring to the ability to manage the various interdependencies between the four design areas.},
  langid = {english},
  keywords = {Co-creation,Digital transformation,GovTech,Multiple-helix,Public service innovation,Trust frameworks},
  file = {/Users/guru/Zotero/storage/AJ38BWLR/Bharosa_2022_The rise of GovTech_Government Information Quarterly.pdf;/Users/guru/Zotero/storage/IMG7JHKY/S0740624X22000259.html}
}

@article{biesialska2021:big,
  title = {Big {{Data}} Analytics in {{Agile}} Software Development: {{A}} Systematic Mapping Study},
  shorttitle = {Big {{Data}} Analytics in {{Agile}} Software Development},
  author = {Biesialska, Katarzyna and Franch, Xavier and {Munt{\'e}s-Mulero}, Victor},
  year = {2021},
  month = apr,
  journal = {Information and Software Technology},
  volume = {132},
  pages = {106448},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2020.106448},
  urldate = {2023-01-15},
  abstract = {Context: Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity. Objective: Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA). Method: As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019. Results: In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics. Conclusions: As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.},
  langid = {english},
  keywords = {Agile software development,Artificial intelligence,Data analytics,Literature review,Machine learning,Software analytics},
  file = {/Users/guru/Zotero/storage/EQNE33MC/S0950584920301981.html}
}

@inproceedings{Bilgaiyan2016112,
  type = {Conference Paper},
  title = {A Review of Software Cost Estimation in Agile Software Development Using Soft Computing Techniques},
  author = {Bilgaiyan, Saurabh and Mishra, Samaresh and Das, Madhabananda},
  year = {2016},
  series = {Proceedings - {{International Conference}} on {{Computational Intelligence}} and {{Networks}}},
  volume = {2016-January},
  pages = {112--117},
  doi = {10.1109/CINE.2016.27},
  abstract = {For a successful software project, accurate prediction of its overall effort and cost estimation is a very much essential task. Software projects have evolved through a number of development models over the last few decades. Hence, to cover an accurate measurement of the effort and cost for different software projects based on different development models having new and innovative phases of software development, is a crucial task to be done. An accurate prediction always leads to a successful software project within the budget with no delay, but any percentage of misconduct in the overall effort and cost estimate may lead to a project failure in terms of delivery time, budget or features. Software industries have adopted various development models based on the project requirements and organization's capabilities. Due to adaptability to changes in a software project, agile software development model has become a much successful and popular framework for development over the last decade. The customer is involved as an active participant in the development using an agile framework. Hence, changes can occur at any phase of development and they can be dynamic in nature. That is why an accurate prediction of effort and cost of such projects is a crucial task to be done as the complexity of overall development structure is increased with the time. Soft computing techniques have proven that they are one of the best problem solving techniques in such scenarios. Such techniques are more flexible and presence of bio-intelligence increases their accuracy. Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Artificial Neural Network (ANN), Fuzzy Inference Systems (FIS), etc. are applied successfully for estimation of cost and effort of agile based software projects. This paper deals with such soft computing techniques and provides a detailed and analytical overview of such methods. It also provides the future scope and possibilities to explore such techniques on the basis of survey provided by this paper. \textcopyright{} 2016 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{blackwell2019:fifty,
  title = {Fifty Years of the Psychology of Programming},
  author = {Blackwell, Alan F. and Petre, Marian and Church, Luke},
  year = {2019},
  month = nov,
  journal = {International Journal of Human-Computer Studies},
  series = {50 Years of the {{International Journal}} of {{Human-Computer Studies}}. {{Reflections}} on the Past, Present and Future of Human-Centred Technologies},
  volume = {131},
  pages = {52--63},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2019.06.009},
  urldate = {2023-01-15},
  abstract = {This paper reflects on the evolution (past, present and future) of the `psychology of programming' over the 50 year period of this anniversary issue. The International Journal of Human-Computer Studies (IJHCS) has been a key venue for much seminal work in this field, including its first foundations, and we review the changing research concerns seen in publications over these five decades. We relate this thematic evolution to research taking place over the same period within more specialist communities, especially the Psychology of Programming Interest Group (PPIG), the Empirical Studies of Programming series (ESP), and the ongoing community in Visual Languages and Human-Centric Computing (VL/HCC). Many other communities have interacted with psychology of programming, both influenced by research published within the specialist groups, and in turn influencing research priorities. We end with an overview of the core theories that have been developed over this period, as an introductory resource for new researchers, and also with the authors' own analysis of key priorities for future research.},
  langid = {english},
  keywords = {Attention investment,Programming languages,Psychology of programming}
}

@article{blake2021:impact,
  title = {Impact of {{Artificial Intelligence}} on {{Engineering}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Impact of {{Artificial Intelligence}} on {{Engineering}}},
  author = {Blake, Robert W. and Mathew, Robins and George, Abraham and Papakostas, Nikolaos},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {54th {{CIRP CMS}} 2021 - {{Towards Digitalized Manufacturing}} 4.0},
  volume = {104},
  pages = {1728--1733},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2021.11.291},
  urldate = {2023-01-15},
  abstract = {Recent advancements in cloud computing and software technology have resulted in the development of powerful Artificial Intelligence (AI) tools for engineering applications. However, the impact of AI in future engineering jobs remains ambiguous. This paper discusses recent AI developments, AI applications, the influence of AI on the Engineering profession, and the productivity of engineers. In addition, ethics, and professional impacts to be considered with the introduction of AI are addressed. The results of a survey conducted among people from Engineering colleges across Ireland are also presented.},
  langid = {english},
  keywords = {Artificial Intelligence,Deep Learning,Ethics,Industry 4.0,Machine Learning},
  file = {/Users/guru/Zotero/storage/HTNQU6SJ/Blake et al_2021_Impact of Artificial Intelligence on Engineering_Procedia CIRP.pdf;/Users/guru/Zotero/storage/GN7IKAYB/S2212827121011896.html}
}

@inproceedings{Boerstra2022305,
  type = {Conference Paper},
  title = {Stronger Together: {{On}} Combining Relationships in Architectural Recovery Approaches},
  author = {Boerstra, Evelien and Ahn, John and Rubin, Julia},
  year = {2022},
  series = {Proceedings - 2022 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}, {{ICSME}} 2022},
  pages = {305--316},
  doi = {10.1109/ICSME55016.2022.00035},
  abstract = {Architecture recovery is the process of obtaining the intended architecture of a software system by analyzing its implementation. Most existing architectural recovery approaches rely on extracting information about relationships between code entities and then use the extracted information to group closely related entities together. The approaches differ by the type of relationships they consider, e.g., method calls, data dependencies, and class name similarity. Prior work shows that combining multiple types of relationships during the recovery process is often beneficial as it leads to a better result than the one obtained by using the relationships individually. Yet, most, if not all, academic and industrial architecture recovery approaches simply unify the combined relationships to produce a more complete representation of the analyzed systems. In this paper, we propose and evaluate an alternative approach to combining information derived from multiple relationships, which is based on identifying agreements/disagreements between relationship types. We discuss advantages and disadvantages of both approaches and provide suggestions for future research in this area. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Borg202222,
  type = {Conference Paper},
  title = {Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice},
  author = {Borg, Markus and Bengtsson, Johan and Osterling, Harald and Hagelborn, Alexander and Gagner, Isabella and Tomaszewski, Piotr},
  year = {2022},
  series = {Proceedings - 1st {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}, {{CAIN}} 2022},
  pages = {22--32},
  doi = {10.1145/3522664.3528592},
  abstract = {Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{börsting2022:software,
  title = {Software {{Engineering}} for {{Augmented Reality}} - {{A Research Agenda}}},
  author = {B{\"o}rsting, Ingo and Heikamp, Markus and Hesenius, Marc and Koop, Wilhelm and Gruhn, Volker},
  year = {2022},
  month = jun,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {EICS},
  pages = {155:1--155:34},
  doi = {10.1145/3532205},
  urldate = {2023-01-15},
  abstract = {Augmented reality changes the way we perceive reality and how we interact with computers. However, we argue that to create augmented reality solutions, we need to rethink the way we develop software. In this paper, we review the state of the art in software engineering for augmented reality applications, derive open questions, and define a research agenda. For this purpose, we consider different engineering phases and evaluate conventional techniques regarding their applicability for AR development. In requirements engineering, we found the integration of AR experts and the associated collaboration between actors to be of key aspect in the development process. Additionally, requirements about the physical world must be considered, which in turn has a huge impact on UI design. The relevance of the physical environment is not yet sufficiently addressed in applicable techniques, which also applies to current implementation frameworks and tools, complicating the AR development process. When evaluating AR software iterations, we found interaction testing and test automation to have great potential, although they have not yet been sufficiently researched. Our paper contributes to AR research by revealing current core challenges within the AR development process and formulating explicit research questions that should be considered by future research.},
  keywords = {augmented reality,frameworks,methods,software engineering,tools}
}

@article{brad2014:smart,
  title = {Smart {{Deployment}} of {{Demonstrators}} into {{Successful Commercial Solutions}}},
  author = {Brad, Stelian and Fulea, Mircea and Brad, Emilia and Mocan, Bogdan},
  year = {2014},
  month = jan,
  journal = {Procedia CIRP},
  series = {24th {{CIRP Design Conference}}},
  volume = {21},
  pages = {503--508},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2014.03.137},
  urldate = {2023-01-15},
  abstract = {Product or service concepts based on emerging technologies are usually results of research projects, be they performed by academic groups or by research departments of companies. Many times, the prototypes or demonstrators that result from such projects are supposed to evolve into commercial products or services, but \textendash{} at least in the first stage - there are more focus on proving key features of a technology, or the effectiveness / efficiency / applicability of various concepts or algorithms. However, evolving into commercial products is many times at least as challenging as building the prototypes. In case of software-based projects, this means changes in architecture, a lot of code rewriting and important usability improvements. This paper introduces a software-concept product design algorithm which aims to minimize the effort required in turning a demonstrator into a commercial product. This is done by generating two functionality sets: a pure demonstrator and a pure commercial one, then generating a hybrid functionality set with the corresponding architecture, and then assessing each functionality for the demonstrator and the commercial version in terms of development and improvement effort. Through iterations, in which the original functionality sets are improved, the difference between the two perspectives will be reduced until it gets below a reasonable limit in terms of effort. The paper presents a case study in which the algorithm is applied for planning a software platform for supporting SMEs in their innovation processes.},
  langid = {english},
  keywords = {innovation,methodology,product deployment,product planning,software engineering},
  file = {/Users/guru/Zotero/storage/34UMSTP5/Brad et al_2014_Smart Deployment of Demonstrators into Successful Commercial Solutions_Procedia CIRP.pdf;/Users/guru/Zotero/storage/U7DAI4DH/S2212827114006775.html}
}

@inproceedings{Brad2014503,
  type = {Conference Paper},
  title = {Smart Deployment of Demonstrators into Successful Commercial Solutions},
  author = {Brad, Stelian and Fulea, Mircea and Brad, Emilia and Mocan, Bogdan},
  year = {2014},
  series = {Procedia {{CIRP}}},
  volume = {21},
  pages = {503--508},
  doi = {10.1016/j.procir.2014.03.137},
  abstract = {Product or service concepts based on emerging technologies are usually results of research projects, be they performed by academic groups or by research departments of companies. Many times, the prototypes or demonstrators that result from such projects are supposed to evolve into commercial products or services, but - At least in the first stage - There are more focus on proving key features of a technology, or the effectiveness / efficiency / applicability of various concepts or algorithms. However, evolving into commercial products is many times at least as challenging as building the prototypes. In case of software-based projects, this means changes in architecture, a lot of code rewriting and important usability improvements. This paper introduces a software-concept product design algorithm which aims to minimize the effort required in turning a demonstrator into a commercial product. This is done by generating two functionality sets: A pure demonstrator and a pure commercial one, then generating a hybrid functionality set with the corresponding architecture, and then assessing each functionality for the demonstrator and the commercial version in terms of development and improvement effort. Through iterations, in which the original functionality sets are improved, the difference between the two perspectives will be reduced until it gets below a reasonable limit in terms of effort. The paper presents a case study in which the algorithm is applied for planning a software platform for supporting SMEs in their innovation processes. \textcopyright{} 2014 Published by Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{bruneliere2022:aidoart,
  title = {{{AIDOaRt}}: {{AI-augmented Automation}} for {{DevOps}}, a Model-Based Framework for Continuous Development in {{Cyber}}\textendash{{Physical Systems}}},
  shorttitle = {{{AIDOaRt}}},
  author = {Bruneliere, Hugo and Muttillo, Vittoriano and Eramo, Romina and Berardinelli, Luca and G{\'o}mez, Abel and Bagnato, Alessandra and Sadovykh, Andrey and Cicchetti, Antonio},
  year = {2022},
  month = oct,
  journal = {Microprocessors and Microsystems},
  volume = {94},
  pages = {104672},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2022.104672},
  urldate = {2023-01-15},
  abstract = {The advent of complex Cyber\textendash Physical Systems (CPSs) creates the need for more efficient engineering processes. Recently, DevOps promoted the idea of considering a closer continuous integration between system development (including its design) and operational deployment. Despite their use being still currently limited, Artificial Intelligence (AI) techniques are suitable candidates for improving such system engineering activities (cf. AIOps). In this context, AIDOaRT is a large European collaborative project that aims at providing AI-augmented automation capabilities to better support the modeling, coding, testing, monitoring, and continuous development of CPSs. The project proposes to combine Model Driven Engineering principles and techniques with AI-enhanced methods and tools for engineering more trustable CPSs. The resulting framework will (1) enable the dynamic observation and analysis of system data collected at both runtime and design time and (2) provide dedicated AI-augmented solutions that will then be validated in concrete industrial cases. This paper describes the main research objectives and underlying paradigms of the AIDOaRt project. It also introduces the conceptual architecture and proposed approach of the AIDOaRt overall solution. Finally, it reports on the actual project practices and discusses the current results and future plans.},
  langid = {english},
  keywords = {AIOps,Artificial Intelligence,Continuous development,Cyber–Physical Systems,DevOps,Model Driven Engineering,Software engineering,System engineering},
  file = {/Users/guru/Zotero/storage/94A582QU/S0141933122002022.html}
}

@article{byrne2018:biologicalisation,
  title = {Biologicalisation: {{Biological}} Transformation in Manufacturing},
  shorttitle = {Biologicalisation},
  author = {Byrne, Gerald and Dimitrov, Dimitri and Monostori, Laszlo and Teti, Roberto and {van Houten}, Fred and Wertheim, Rafi},
  year = {2018},
  month = may,
  journal = {CIRP Journal of Manufacturing Science and Technology},
  volume = {21},
  pages = {1--32},
  issn = {1755-5817},
  doi = {10.1016/j.cirpj.2018.03.003},
  urldate = {2023-01-15},
  abstract = {A new emerging frontier in the evolution of the digitalisation and the 4th industrial revolution (Industry 4.0) is considered to be that of ``Biologicalisation in Manufacturing''. This has been defined by the authors to be ``The use and integration of biological and bio-inspired principles, materials, functions, structures and resources for intelligent and sustainable manufacturing technologies and systems with the aim of achieving their full potential.'' In this White Paper, detailed consideration is given to the meaning and implications of ``Biologicalisation'' from the perspective of the design, function and operation of products, manufacturing processes, manufacturing systems, supply chains and organisations. The drivers and influencing factors are also reviewed in detail and in the context of significant developments in materials science and engineering. The paper attempts to test the hypothesis of this topic as a breaking new frontier and to provide a vision for the development of manufacturing science and technology from the perspective of incorporating inspiration from biological systems. Seven recommendations are delivered aimed at policy makers, at funding agencies, at the manufacturing research community and at those industries involved in the development of next generation manufacturing technology and systems. It is concluded that it is valid to argue that Biologicalisation in Manufacturing truly represents a new and breaking frontier of digitalisation and Industry 4.0 and that the market potential is very strong. It is evident that extensive research and development is required in order to maximise on the benefits of a biological transformation.},
  langid = {english},
  keywords = {Bio-inspired,Bio-integrated,Bio-intelligent,Biological transformation,Biologicalisation in Manufacturing,Cyber-physical systems,Digitalisation,Industrie 4.0,Industry 4.0,International perspective,Manufacturing},
  file = {/Users/guru/Zotero/storage/C6CGA3ND/Byrne et al_2018_Biologicalisation_CIRP Journal of Manufacturing Science and Technology.pdf;/Users/guru/Zotero/storage/F6PY3BXA/S1755581718300129.html}
}

@article{byrne2022:study,
  title = {A {{Study}} of the {{Organisational Behaviour}} of {{Software Test Engineers}}, {{Contributing}} to the {{Digital Transformation}} of {{Banks}} in the {{Irish Financial Sector}}},
  author = {Byrne, Darren and Tuite, Aisling and Organ, John},
  year = {2022},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {21st {{IFAC Conference}} on {{Technology}}, {{Culture}} and {{International Stability TECIS}} 2022},
  volume = {55},
  number = {39},
  pages = {259--264},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2022.12.031},
  urldate = {2023-01-15},
  abstract = {This initial exploratory paper will endeavour to understand theoretically how Software Test Engineers (STEs) behave within the organisational framework of an Irish financial institution (IFI), in terms of the development/execution of test cases, based on variables such as experience, the nature of the project in question and the environment of the IFI. Whist the research is currently in the initial stages, it will focus on the exploration of insider research and auto-ethnographic research methods, subsequently progressing to the delineation of how such knowledge relating to the various methods/tools used by STEs can be leveraged to improve performance and returns on investment for both STEs/IFIs respectively. The research will employ qualitative, inductive, and interpretive methods/methodologies to achieve its aim/objectives. This paper will focus on one specific research objective - the examination of individual STE behaviour within an IFI. Interviews will be conducted with participants working in a variety of roles (including STEs, Project Managers, Business Analysts, etc.,) to gather stories relating to the behaviour of STEs within a major IFI. Insider research methods will be used, supported by auto-ethnographic methods of reflexivity to help ensure research validity. Once the primary data collection phase is completed, it will be analysed/interpreted through theoretical frameworks within the discipline of organisational behaviour (OB). As this is inductive research, contributions to both theory and practice will emerge as the research process develops, but is expected to contribute to practice by highlighting the OB of a specific team with growing prominence/importance within the organisation, as it goes through a process of change to digital-first customer interactions. Additionally, the research will contribute to the methods of insider research through transparent reflection regarding the practical/philosophical challenges of this form of ethnographic research.},
  langid = {english},
  keywords = {digital transformation,financial technology,fintech,Irish financial industry,organisational behaviour,software testing},
  file = {/Users/guru/Zotero/storage/3EYSJK3E/S2405896322030713.html}
}

@article{cachero2023:influence,
  title = {Influence of Personality and Modality on Peer Assessment Evaluation Perceptions Using {{Machine Learning}} Techniques},
  author = {Cachero, Cristina and {Rico-Juan}, Juan Ram{\'o}n and Maci{\`a}, Hermenegilda},
  year = {2023},
  month = mar,
  journal = {Expert Systems with Applications},
  volume = {213},
  pages = {119150},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.119150},
  urldate = {2023-01-15},
  abstract = {The successful instructional design of self and peer assessment in higher education poses several challenges that instructors need to be aware of. One of these is the influence of students' personalities on their intention to adopt peer assessment. This paper presents a quasi-experiment in which 85 participants, enrolled in the first-year of a Computer Engineering programme, were assessed regarding their personality and their acceptance of three modalities of peer assessment (individual, pairs, in threes). Following a within-subjects design, the students applied the three modalities, in a different order, with three different activities. An analysis of the resulting 1195 observations using ML techniques shows how the Random Forest algorithm yields significantly better predictions for three out of the four adoption variables included in the study. Additionally, the application of a set of eXplainable Artificial Intelligence (XAI) techniques shows that Agreeableness is the best predictor of Usefulness and Ease of Use, while Extraversion is the best predictor of Compatibility, and Neuroticism has the greatest impact on global Intention to Use. The discussion highlights how, as it happens with other innovations in educational processes, low levels of Consciousness is the most consistent predictor of resistance to the introduction of peer assessment processes in the classroom. Also, it stresses the value of peer assessment to augment the positive feelings of students scoring high on Neuroticism, which could lead to better performance. Finally, the low impact of the peer assessment modality on student perceptions compared to personality variables is debated.},
  langid = {english},
  keywords = {eXplainable Artificial Intelligence (XAI),Machine learning (ML),Peer assessment (PA),Personality,Quasi-experiment,Use behaviour},
  file = {/Users/guru/Zotero/storage/DI9JCCSR/Cachero et al_2023_Influence of personality and modality on peer assessment evaluation perceptions_Expert Systems with Applications.pdf;/Users/guru/Zotero/storage/ZESBLKD5/S0957417422021686.html}
}

@inproceedings{campeanu2017:runtime,
  title = {Run-Time Component Allocation in {{CPU-GPU}} Embedded Systems},
  booktitle = {Proceedings of the {{Symposium}} on {{Applied Computing}}},
  author = {Campeanu, Gabriel and Saadatmand, Mehrdad},
  year = {2017},
  month = apr,
  series = {{{SAC}} '17},
  pages = {1259--1265},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3019612.3019785},
  urldate = {2023-01-15},
  abstract = {Nowadays, many of the modern embedded applications such as vehicles and robots, interact with the environment and receive huge amount of data through various sensors such as cameras and radars. The challenge of processing large amount of data, within an acceptable performance, is solved by employing embedded systems that incorporate complementary attributes of CPUs and Graphics Processing Units (GPUs), i.e., sequential and parallel execution models. Component-based development (CBD) is a software engineering methodology that augments the applications development through reuse of software blocks known as components. In developing a CPU-GPU embedded application using CBD, allocation of components to different processing units of the platform is an important activity which can affect the overall performance of the system. In this context, there is also often the need to support and achieve run-time component allocation due to various factors and situations that can happen during system execution, such as switching off parts of the system for energy saving. In this paper, we provide a solution that dynamically allocates components using various system information such as the available resources (e.g., available GPU memory) and the software behavior (e.g., in terms of GPU memory usage). The novelty of our work is a formal allocation model that considers GPU system characteristics computed on-the-fly through software monitoring solutions. For the presentation and validation of our solution, we utilize an existing underwater robot demonstrator.},
  isbn = {978-1-4503-4486-9},
  keywords = {component allocation,component-based development,CPU-GPU,dynamic allocation,embedded systems,GPU,GPU monitoring,monitor},
  file = {/Users/guru/Zotero/storage/MTEGLIMB/Campeanu_Saadatmand_2017_Run-time component allocation in CPU-GPU embedded systems_Proceedings of the Symposium on Applied Computing.pdf}
}

@article{Canedo20221527,
  type = {Article},
  title = {Creativity and Design Thinking as Facilitators in Requirements Elicitation},
  author = {Canedo, Edna Dias and Calazans, Angelica Toffano Seidel and Silva, Geovana Ramos Sousa and Costa, Pedro Henrique Teixeira and De Mesquita, Rodrigo Pereira and Masson, Eloisa Toffano Seidel},
  year = {2022},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  volume = {32},
  number = {10},
  pages = {1527--1558},
  doi = {10.1142/S0218194022500607},
  abstract = {Context: The use of Creativity and Design Thinking (C\&DT) techniques favor the generation of new ideas based on the needs of users and stakeholders, and can support software developers during the process of requirements elicitation. Objectives: In this work, we aim to identify C\&DT techniques to perform requirements elicitation proposed in the literature and in the industry and investigate the perception of software developers about using these techniques. Methods: We conducted a systematic literature review (SLR) to identify the C\&DT techniques in the literature and a regional survey with software development teams from several companies in Brazil to identify which techniques found in the literature are currently being used by organizations. The survey also investigated the level of knowledge that software developers have regarding the C\&DT techniques, and whether they agree that the use of these techniques can help to achieve a more effective process of requirements elicitation. Results: In the SLR, we identified 86 C\&DT techniques that support requirement elicitation activities. In the survey, most developers outlined that C\&DT techniques facilitate requirements elicitation and stated that they have more knowledge and usage experience with DT techniques than creativity techniques. The most used DT techniques mentioned by survey participants were: interview, brainstorming, uses cases, activity analysis, user story, and rapid prototyping, whereas for creativity techniques were: analogies, creativity workshops, focus group, questions list, clarification, none and combining ideas. Conclusions: The results showed that despite the existence of a large number of techniques in the literature, the developers' lack of knowledge about these techniques makes them not used in the industry. However, the developers' responses showed that the use of C\&DT techniques helps to make requirements elicitation more effective. \textcopyright{} 2022 World Scientific Publishing Company.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{carbone2020:radically,
  title = {Radically Simplifying Game Engines: {{AI}} Emotions \& Game Self-Evolution},
  booktitle = {2020 International Conference on Computational Science and Computational Intelligence ({{CSCI}})},
  author = {Carbone, John N. and Crowder, James and Carbone, Ryan A.},
  year = {2020},
  month = dec,
  pages = {464--472},
  doi = {10.1109/CSCI51800.2020.00085},
  abstract = {Today, video games are a multi-billion-dollar industry, continuously evolving through the incorporation of new technologies and innovative design. However, current video game software content creation requires extensive and often-times ambiguous planning phases for developing aesthetics, online capabilities, and gameplay mechanics. Design elements can vary significantly relative to the expertise of artists, designers, budget, and overall game engine/software features and capabilities. Game development processes are often extensively long coding sessions, usually involving a highly iterative creative process, where user requirements are rarely provided. Therefore, we propose significantly simplifying game design and development with novel Artificial Cognition Architecture real-time scalability and dynamic emotion core. Rather than utilizing more static emotion state weighting emotion engines (e.g. ExAI), we leverage significant ACA research in successful implementation of analog neural learning bots with Maslowan objective function algorithms. We also leverage AI- based Artificial Psychology software which utilizes ACA's fine grained self-evolving emotion modeling in humanistic avatar patients for Psychologist training. An ACA common cognitive core provides the gaming industry with wider applications across video game genres. A modular, scalable, and cognitive emotion game architecture implements Non-Playable Character (NPC) learning and self-evolution. ACA models NPC's with fine grained emotions, providing interactive dynamic personality traits for a more realistic game environment and enables NPC self-evolution under the influence of both other NPC's and players. Furthermore, we explore current video game design engine architecture (e.g. Unity, Unreal Engine) and propose an ACA integration approach. We apply artificial cognition and emotion intelligence modeling to engender video games with more distinct, realistic consumer gaming experiences, while simultaneously minimizing software gaming development efforts and costs.},
  keywords = {Artificial Cognition,Artificial Intelligence,Artificial Psychology,Autonomy,Computational modeling,Computer architecture,Emotion Modeling,Games,Industries,Psychology,Self-Evolving,Software,Software Development,Training,Video Games},
  file = {/Users/guru/Zotero/storage/W3F2T9C9/Carbone et al. - 2020 - Radically simplifying game engines AI emotions & .pdf}
}

@article{caron2016internet,
  title = {The {{Internet}} of {{Things}} ({{IoT}}) and Its Impact on Individual Privacy: {{An Australian}} Perspective},
  author = {Caron, Xavier and Bosua, Rachelle and Maynard, Sean B and Ahmad, Atif},
  year = {2016},
  journal = {Computer Law \& Security Review},
  volume = {32},
  number = {1},
  pages = {4--15},
  publisher = {{Elsevier}}
}

@article{carvalho2020:computation,
  title = {Computation Offloading in {{Edge Computing}} Environments Using {{Artificial Intelligence}} Techniques},
  author = {Carvalho, Gon{\c c}alo and Cabral, Bruno and Pereira, Vasco and Bernardino, Jorge},
  year = {2020},
  month = oct,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {95},
  pages = {103840},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2020.103840},
  urldate = {2023-01-15},
  abstract = {Edge Computing (EC) is a recent architectural paradigm that brings computation close to end-users with the aim of reducing latency and bandwidth bottlenecks, which 5G technologies are committed to further reduce, while also achieving higher reliability. EC enables computation offloading from end devices to edge nodes. Deciding whether a task should be offloaded, or not, is not trivial. Moreover, deciding when and where to offload a task makes things even harder and making inadequate or off-time decisions can undermine the EC approach. Recently, Artificial Intelligence (AI) techniques, such as Machine Learning (ML), have been used to help EC systems cope with this problem. AI promises accurate decisions, higher adaptability and portability, thus diminishing the cost of decision-making and the probability of error. In this work, we perform a literature review on computation offloading in EC systems with and without AI techniques. We analyze several AI techniques, especially ML-based, that display promising results, overcoming the shortcomings of current approaches for computing offloading coordination We sorted the ML algorithms into classes for better analysis and provide an in-depth analysis on the use of AI for offloading, in particular, in the use case of offloading in Vehicular Edge Computing Networks, actually one technology that gained more relevance in the last years, enabling a vast amount of solutions for computation and data offloading. We also discuss the main advantages and limitations of offloading, with and without the use of AI techniques.},
  langid = {english},
  keywords = {Artificial Intelligence,Computation offloading,Edge Computing,Machine Learning},
  file = {/Users/guru/Zotero/storage/T5G8VER9/S0952197620302050.html}
}

@article{casillo2022:detecting,
  title = {Detecting Privacy Requirements from {{User Stories}} with {{NLP}} Transfer Learning Models},
  author = {Casillo, Francesco and Deufemia, Vincenzo and Gravino, Carmine},
  year = {2022},
  month = jun,
  journal = {Information and Software Technology},
  volume = {146},
  pages = {106853},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.106853},
  urldate = {2023-01-15},
  abstract = {Context: To provide privacy-aware software systems, it is crucial to consider privacy from the very beginning of the development. However, developers do not have the expertise and the knowledge required to embed the legal and social requirements for data protection into software systems. Objective: We present an approach to decrease privacy risks during agile software development by automatically detecting privacy-related information in the context of user story requirements, a prominent notation in agile Requirement Engineering (RE). Methods: The proposed approach combines Natural Language Processing (NLP) and linguistic resources with deep learning algorithms to identify privacy aspects into User Stories. NLP technologies are used to extract information regarding the semantic and syntactic structure of the text. This information is then processed by a pre-trained convolutional neural network, which paved the way for the implementation of a Transfer Learning technique. We evaluate the proposed approach by performing an empirical study with a dataset of 1680 user stories. Results: The experimental results show that deep learning algorithms allow to obtain better predictions than those achieved with conventional (shallow) machine learning methods. Moreover, the application of Transfer Learning allows to considerably improve the accuracy of the predictions, ca. 10\%. Conclusions: Our study contributes to encourage software engineering researchers in considering the opportunities to automate privacy detection in the early phase of design, by also exploiting transfer learning models.},
  langid = {english},
  keywords = {Deep learning,Natural Language Processing,Transfer Learning,User Stories},
  file = {/Users/guru/Zotero/storage/TGJKW5MA/S0950584922000246.html}
}

@article{cassoli2022:frameworks,
  title = {Frameworks for Data-Driven Quality Management in Cyber-Physical Systems for Manufacturing: {{A}} Systematic Review},
  shorttitle = {Frameworks for Data-Driven Quality Management in Cyber-Physical Systems for Manufacturing},
  author = {Cassoli, Beatriz Bretones and Jourdan, Nicolas and Nguyen, Phu H. and Sen, Sagar and {Garcia-Ceja}, Enrique and Metternich, Joachim},
  year = {2022},
  month = jan,
  journal = {Procedia CIRP},
  series = {15th {{CIRP Conference}} on {{Intelligent Computation}} in {{ManufacturingEngineering}}, 14-16 {{July}} 2021},
  volume = {112},
  pages = {567--572},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2022.09.062},
  urldate = {2023-01-15},
  abstract = {Recent advances in the manufacturing industry have enabled the deployment of Cyber-Physical Systems (CPS) at scale. By utilizing advanced analytics, data from production can be analyzed and used to monitor and improve the process and product quality. Many frameworks for implementing CPS have been developed to structure the relationship between the digital and the physical worlds. However, there is no systematic review of the existing frameworks related to quality management in manufacturing CPS. Thus, our study aims at determining and comparing the existing frameworks. The systematic review yielded 38 frameworks analyzed regarding their characteristics, use of data science and Machine Learning (ML), and shortcomings and open research issues. The identified issues mainly relate to limitations in cross-industry/cross-process applicability, the use of ML, big data handling, and data security.},
  langid = {english},
  keywords = {Artificial Intelligence (AI),Cyber-Physical Systems (CPS),Framework,Quality Management,Systematic Literature Review}
}

@article{cerdeiral2019software,
  title = {Software Project Management in High Maturity: {{A}} Systematic Literature Mapping},
  author = {Cerdeiral, Cristina T and Santos, Gleison},
  year = {2019},
  journal = {Journal of Systems and Software},
  volume = {148},
  pages = {56--87},
  publisher = {{Elsevier}}
}

@inproceedings{Chagas2020101,
  type = {Conference Paper},
  title = {On the Reuse of Knowledge to Develop Intelligent Software Engineering Solutions},
  author = {Chagas, Jos{\'e} Ferdinandy Silva and {de Sousa Neto}, Ademar Fran{\c c}a and Almeida, Hyggo and Silva, Luiz Antonio Pereira and Albuquerque, Danyllo and Perkusich, Mirko and Valadares, Dalton C{\'e}zane Gomes and Perkusich, Angelo},
  year = {2020},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {PartF162440},
  pages = {101--106},
  doi = {10.18293/SEKE2020-157},
  abstract = {Intelligent Software Engineering (ISE) is currently a hot topic in research. Besides being a promising field, it brings many challenges. Therefore, there is a need for guidelines to help researchers to build an ISE solution. The goal of this study is to identify patterns in developing ISE solutions. For this purpose, we analyzed 42 studies, using a thematic analysis approach, to understand how they reused knowledge and applied it to solve a SE task. As a result, we developed a thematic network composed of the main concepts related to knowledge reuse for ISE. Further, we identified that researchers use external and internal knowledge sources, and mostly rely on structured data to develop ISE solutions. Despite this, there are alternatives such as eliciting data from humans and literature to identify metrics or build knowledge-based systems. Overall, we concluded that there many research opportunities to guide the construction of ISE solutions. \textcopyright{} 2020 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/YS2IMQ6E/Chagas et al. - 2020 - On the reuse of knowledge to develop intelligent s.pdf}
}

@inproceedings{Chanda201953,
  type = {Conference Paper},
  title = {Ai Based Data Architecture Impact Analysis},
  author = {Chanda, Debasis},
  year = {2019},
  series = {{{EPiC Series}} in {{Computing}}},
  volume = {63},
  pages = {53--62},
  doi = {10.29007/fkhl},
  abstract = {Enterprises today are technology driven and comprise of plethora of applications that may be categorized based on the technology that they are developed and deployed on. For enterprises that have existed across years and across multiple business cycles, the technologies may be classified as legacy, mature or emerging. The challenge lies in interoperability within and without the organization, especially with respect to the business objects that are required across business functions, to realize the capabilities of the organization. This is also true for scenarios of M\&As (Mergers \& Acquisitions) and also during creation of JVs (Joint Ventures). Enterprise Architecture (EA) defines the Business-Technology alignment in organizations, and is an established methodology for business transformation and establishing enterprise maturity in the keenly competitive business world. Business objects are defined as Data Architecture artifacts within the ambit of EA. The challenges to business object interoperability arises due to the incompatibility of technologies used by the applications. This leads to the well explored n*(n-1) scenario, where n is the number of application interfaces. This has serious implications towards business health of the organization, and risk to the BAU (Business As Usual) of the organization. This is because in a complex mesh like n*(n-1) scenario, it becomes practically impossible to identify the impact of changes to business capabilities in an inconspicuous attribute of a business object in an application domain. Thus the impact analysis of business objects / data as defined by traditional description is a challenge to business sustainability of organizations. These challenges in data architecture impact analysis may be mitigated by the AI (Artificial Intelligence) paradigm, by taking recourse to the very powerful features of AI, by defining predicate calculus based knowledge bases. In our paper we consider the Banking domain for carrying out our discussions. \textcopyright{} 2019, EasyChair. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{chang2016:review,
  title = {A Review on Exception Analysis},
  author = {Chang, Byeong-Mo and Choi, Kwanghoon},
  year = {2016},
  month = sep,
  journal = {Information and Software Technology},
  volume = {77},
  pages = {1--16},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2016.05.003},
  urldate = {2023-01-15},
  abstract = {Context: Exception handling has become popular in most major programming languages, including Ada, C++, Java, and ML. Since exception handling was introduced in programming languages, there have been various kinds of exception analyses, which analyze exceptional behavior of programs statically or dynamically. Exception analyses have also been applied to various software engineering tasks such as testing, slicing, verification and visualization. Objective: This paper aims at providing a comprehensive view of studies on exception analysis. We conducted a review on exception analysis to identify and classify the studies. Method: We referred to the literature review method, and selected a comprehensive set of 87 papers on exception analysis from 515 papers published in journals and conference proceedings. The categorization and classification were done according to the research questions regarding to when they analyze, what they analyze, how to analyze, and applications of exception analysis. Results: We first identify three categories of static exception analysis and two categories of dynamic exception analysis together with the main applications of the exception analyses. We also discuss the main concepts, research methods used and major contributions of the studies on exception analysis. Conclusion: We have provided the comprehensive review of exception analysis. To the best of our knowledge, this is the first comprehensive review on exception analysis. As a further work, it would be interesting to see how the existing exception analysis techniques reviewed in this paper can be applied to other programming languages with exception handling mechanism, such as C\#, Scala, and Eiffel, which have been rarely explored.},
  langid = {english},
  keywords = {Debugging,Dynamic analysis,Exception analysis,Exception flow,Static analysis,Testing},
  file = {/Users/guru/Zotero/storage/TU43UUZU/S0950584916300830.html}
}

@article{Chang2018,
  type = {Article},
  title = {Situation Analytics \textemdash{} at the Dawn of a New Software Engineering Paradigm},
  author = {Chang, Carl K.},
  year = {2018},
  journal = {Science China Information Sciences},
  volume = {61},
  number = {5},
  doi = {10.1007/s11432-017-9372-7},
  abstract = {In this paper, I first review the seminal work by Thomas Kuhn \textemdash{} The Structure of Scientific Revolutions \textemdash{} and elaborate my view on paradigm shifts in software engineering research and practice as it turns 50 years old in 2018. I then examine major undertakings of the computing profession since early days of modern computing, especially those done by the software engineering community as a whole. I also enumerate anomalies and crises that occurred at various stages, and the attempts to provide solutions by the software engineering professionals in the past five decades. After providing such a background, I direct readers' attention toward emerging anomalies in software engineering, at a severity level that is causing another software engineering crisis, and suggest a set of criteria for feasible solutions. The main theme of this paper is to advocate that situation analytics, equipped with necessary definitions of essential concepts including situation and intention as parts of a new computational framework, can serve as the foundation for a new software engineering paradigm named the Situation-Centric Paradigm. In this framework, situation is considered a new abstraction for computing and is clearly differentiated from the widely accepted existing abstractions, namely function and object. I argue that the software engineering professionals will inevitably move into this new paradigm, willingly or unwillingly, to empower Human-Embedded Computing (HEC) and End-User Embedded Computing (EUEC), much more than what they have done with traditional humancentered or user-centric computing altogether. In the end, I speculate that an ultimate agile method may be on the rise, and challenge readers to contemplate ``what if'' hundreds of thousands ``end-user developers'' emerge into the scene where the boundaries between end users and developers become much more blurred. \textcopyright{} 2018, Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{chang2020:chapter,
  title = {Chapter 5 - {{Machine}} and {{Deep Learning}}},
  booktitle = {Intelligence-{{Based Medicine}}},
  author = {Chang, Anthony C.},
  editor = {Chang, Anthony C.},
  year = {2020},
  month = jan,
  pages = {67--140},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-823337-5.00005-6},
  urldate = {2023-01-15},
  abstract = {Machine learning along with data mining comprise sub-disciplines under data science. There are several schools of machine learning, including symbolists, connectionists, revolutionaries, Bayesians, and analogizers. Machine learning with its sometimes tedious workflow differs significantly from conventional programming. Classical machine learning consists of supervised (classification and regression) and unsupervised (clustering and generalization) learning but also semi-supervised and ensemble learning. Deep learning consists of a range of methods including convolutional neural networks, recurrent neural networks, generative adversarial networks, and their derivatives. Deep reinforcement learning (such as deep Q network, or DQN) is becoming a valuable deep learning tool in biomedicine. Evaluation of these models includes methods such as receiver operating characteristic, precision-recall curve, and the F-1 measure in the confusion matrix. Finally, issues such as explainability, bias and variance, fitting, curse of dimensionality, and correlation vs causation are discussed.},
  isbn = {978-0-12-823337-5},
  langid = {english},
  keywords = {bias and variance,classification,cluster analysis,convolutional neural network,deep learning,deep reinforcement learning,fitting,machine learning,precision recall curve,receiver operating characteristic,recurrent neural network,regression,supervised learning,unsupervised learning},
  file = {/Users/guru/Zotero/storage/DEM4NRRN/B9780128233375000056.html}
}

@article{Chelouati2023,
  type = {Article},
  title = {Graphical Safety Assurance Case Using {{Goal Structuring Notation}} ({{GSN}}) \textemdash{} Challenges, Opportunities and a Framework for Autonomous Trains},
  author = {Chelouati, Mohammed and Boussif, Abderraouf and Beugin, Julie and El Koursi, El-Miloudi},
  year = {2023},
  journal = {Reliability Engineering and System Safety},
  volume = {230},
  doi = {10.1016/j.ress.2022.108933},
  abstract = {The development of fully autonomous vehicles is an ambition that took seed in the automotive industry a few years ago and is now growing in the railways considering their benefits. The main objective of autonomous train is to perform its operations and assure its mission with an acceptable safety level in all possible operational conditions. Such an objective needs to be supported by a safety demonstration. In order to authorize the operations of railway systems, they must be proven safe. This requires a technical and operational safety assessment, and also a safety assurance process during the system's whole life-cycle. The goal of such activities is to ensure that designed systems comply with railway safety standards and regulations. Both safety arguments and evidences are required to demonstrate that this compliance is achieved. These sets of evidence are documented in a so-called safety case. Recently, graphical safety cases, such as Goal Structuring Notation (GSN)-based safety case, have become an interesting alternative to narrative reports and plain texts. The graphical structure and visual properties improve the presentation and comprehension of the safety arguments. In this paper, we firstly review the use of the GSN for building graphical safety case for different transportation systems, with a focus on the railway domain. Then, we discuss the opportunities and challenges of considering such an approach in railway and we propose a high-level framework for building the GSN-based safety assurance case for the autonomous trains. \textcopyright{} 2022 Elsevier Ltd},
  publication_stage = {Final},
  source = {Scopus}
}

@article{chemingui2019:product,
  title = {Product {{Line Configuration Meets Process Mining}}},
  author = {Chemingui, Houssem and Gam, Ines and Mazo, Ra{\'u}l and Salinesi, Camille and Ghezala, Henda Ben},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {{{CENTERIS}} 2019 - {{International Conference}} on {{ENTERprise Information Systems}} / {{ProjMAN}} 2019 - {{International Conference}} on {{Project MANagement}} / {{HCist}} 2019 - {{International Conference}} on {{Health}} and {{Social Care Information Systems}} and {{Technologies}}, {{CENTERIS}}/{{ProjMAN}}/{{HCist}} 2019},
  volume = {164},
  pages = {199--210},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.12.173},
  urldate = {2023-01-15},
  abstract = {Product line engineering is a new production paradigm that provides organizations a competitive edge by improving productivity and decreasing costs. The purpose with this new production paradigm is no longer to develop a single product but to develop a product family and to generate the products of the line through configuration processes. However, the potential benefits of product line engineering can be missed when dealing with large product lines because the configuration processes become error prone tasks. Consequently, guiding stakeholders during such complex configuration processes and recommending the best configuration alternatives until leading to a satisfying experience becomes a challenge. This paper focuses on enhancing product line configuration processes through process mining techniques. Therefore, user's actions of previous product line configurations are logged, mined and analyzed. We conducted a preliminary research to motivate the advantages of process mining in product line configuration and to explore what can process mining bring for configuration processes and how to use it to enhance configuration processes. Thus, guidance questions are sketched in order to position process mining as a solving tool for the configuration difficulties. Furthermore, we propose a reference architecture that considers process mining for configuring product lines.},
  langid = {english},
  keywords = {configuration difficulties,configuration process,enhancing,process mining,Product line engineering}
}

@inproceedings{Chemingui2019199,
  type = {Conference Paper},
  title = {Product Line Configuration Meets Process Mining},
  author = {Chemingui, Houssem and Gam, Ines and Mazo, Ra{\'u}l and Salinesi, Camille and Ghezala, Henda Ben},
  year = {2019},
  series = {Procedia {{Computer Science}}},
  volume = {164},
  pages = {199--210},
  doi = {10.1016/j.procs.2019.12.173},
  abstract = {Product line engineering is a new production paradigm that provides organizations a competitive edge by improving productivity and decreasing costs. The purpose with this new production paradigm is no longer to develop a single product but to develop a product family and to generate the products of the line through configuration processes. However, the potential benefits of product line engineering can be missed when dealing with large product lines because the configuration processes become error prone tasks. Consequently, guiding stakeholders during such complex configuration processes and recommending the best configuration alternatives until leading to a satisfying experience becomes a challenge. This paper focuses on enhancing product line configuration processes through process mining techniques. Therefore, user's actions of previous product line configurations are logged, mined and analyzed. We conducted a preliminary research to motivate the advantages of process mining in product line configuration and to explore what can process mining bring for configuration processes and how to use it to enhance configuration processes. Thus, guidance questions are sketched in order to position process mining as a solving tool for the configuration difficulties. Furthermore, we propose a reference architecture that considers process mining for configuring product lines. \textcopyright{} 2019 The Authors. Published by Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{chen2019:subspace,
  title = {Subspace Weighting Co-Clustering of Gene Expression Data},
  author = {Chen, Xiaojun and Huang, Joshua Z. and Wu, Qingyao and Yang, Min},
  year = {2019},
  month = mar,
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
  volume = {16},
  number = {2},
  pages = {352--364},
  issn = {1557-9964},
  doi = {10.1109/TCBB.2017.2705686},
  abstract = {Microarray technology enables the collection of vast amounts of gene expression data from biological experiments. Clustering algorithms have been successfully applied to exploring the gene expression data. Since a set of genes may be only correlated to a subset of samples, it is useful to use co-clustering to recover co-clusters in the gene expression data. In this paper, we propose a novel algorithm, called Subspace Weighting Co-Clustering (SWCC), for high dimensional gene expression data. In SWCC, a gene subspace weight matrix is introduced to identify the contribution of gene objects in distinguishing different sample clusters. We design a new co-clustering objective function to recover the co-clusters in the gene expression data, in which the subspace weight matrix is introduced. An iterative algorithm is developed to solve the objective function, in which the subspace weight matrix is automatically computed during the iterative co-clustering process. Our empirical study shows encouraging results of the proposed algorithm in comparison with six state-of-the-art clustering algorithms on ten gene expression data sets. We also propose to use SWCC for gene clustering and selection. The experimental results show that the selected genes can improve the classification performance of Random Forests.},
  keywords = {Algorithm design and analysis,Approximation algorithms,Clustering algorithms,co-clustering,Entropy,Gene expression,Gene expression data,gene selection,Linear programming,Partitioning algorithms,subspace clustering},
  file = {/Users/guru/Zotero/storage/5C7WSNW2/Chen et al_2019_Subspace weighting co-clustering of gene expression data_IEEEACM Transactions on Computational Biology and Bioinformatics.pdf}
}

@article{chen2022:context,
  title = {Context and Auto-Interaction Are All You Need: {{Towards}} Context Embedding Based {{QoS}} Prediction via Automatic Feature Interaction for High Quality Cloud {{API}} Delivery},
  shorttitle = {Context and Auto-Interaction Are All You Need},
  author = {Chen, Zhen and Pan, Maosheng and He, Pengfei and Qi, Wenchao and Liu, Linlin and Shen, Limin and You, Dianlong},
  year = {2022},
  month = mar,
  journal = {Future Generation Computer Systems},
  volume = {128},
  pages = {265--281},
  issn = {0167-739X},
  doi = {10.1016/j.future.2021.10.014},
  urldate = {2023-01-15},
  abstract = {Cloud application programming interface (API) is a software intermediary that enables data exchange, business logic or functionality delivery between applications, infrastructures and IoT devices for supporting service oriented architecture. Currently, the number of cloud API in the Web is increasing and the number of cloud API with similar functionality is very large. Since quality of service (QoS) can well differentiate the performance of similar cloud API, QoS prediction has become the critical base for fast, personalized and high-quality cloud API selection and recommendation. However, contexts are used indirectly through context aware neighbors in most existing researches and the combined feature interactions are treated equally in previous factorization machine based methods, which together hinder the accurate QoS prediction. To address the above concerns, we first conduct data analysis on real-world QoS datasets and provide conclusive evidence to verify the necessity of incorporating contextual information and differentiating feature interaction. Then, we propose a context-aware QoS prediction approach via automatic feature interaction named CAFI. Contextual information of both user and cloud API sides are directly fed into CAFI through feature embedding. Moreover, the importance weights of feature interactions are learned by a generalized regularized dual averaging optimizer, so as to reward effective feature interaction and penalize noise feature interaction automatically. Lastly, final QoS prediction is obtained in an ensemble way by balancing the results of linear regression, automatic feature interaction and non-linear interaction. Extensive experiments on two public real-world QoS datasets demonstrate that CAFI can significantly improve QoS prediction accuracy. And the proposed CAFI approach is promising to encourage service providers provide high quality APIs and enable developer get desired cloud APIs quickly, thereby promoting the development of API economy.},
  langid = {english},
  keywords = {Automatic feature interaction,Cloud API,Contextual information,QoS prediction},
  file = {/Users/guru/Zotero/storage/DQFRLYVV/S0167739X21004040.html}
}

@inproceedings{chiu2015:using,
  title = {Using a Visible {{BI}} to Construct Lean Manufacturing within Big Data},
  booktitle = {2015 26th Annual {{SEMI}} Advanced Semiconductor Manufacturing Conference ({{ASMC}})},
  author = {Chiu, S.H. and Lin, H.K. and Hsu, Jung Pin and Chiu, Che Yu},
  year = {2015},
  month = may,
  pages = {241--244},
  issn = {2376-6697},
  doi = {10.1109/ASMC.2015.7164480},
  abstract = {There are several factors to influence effectiveness of manufacturing. People productivity is one of very important factor that helps to construct lean manufacturing that provides profit of company. We separate operators' work hour into effective productivity and non-effective productivity to point out opportunity to improve productivity without heavy invest. Through constructed visible people loading with BI (Business Intelligent) software analysis system using existing KPI (Key performance Index), we can see whole picture of actual factory runs. Benchmark, resource leverage and re-organize job become possible and precise by visible chart within those huge data. Several truly opportunities and effective activities will be discussed as below that exposure by visible BI; most of them was executed and help us to improve at least 30\% people productivity without extra heavy invest.},
  keywords = {Bismuth,Companies,Loading,Manufacturing,Monitoring,Productivity},
  file = {/Users/guru/Zotero/storage/WW3E55X6/Chiu et al_2015_Using a visible BI to construct lean manufacturing within big data_2015 26th annual SEMI advanced semiconductor manufacturing conference (ASMC).pdf}
}

@inproceedings{Chung2022506,
  type = {Conference Paper},
  title = {Fast {{DNN-based}} Mechatronics Prototyping Platform on Robotic Arm Control},
  author = {Chung, Yu-Chien and Lian, Hao-Hsiang and Xiao, Yong-Lun and Huang, Chih-Tsun and Liou, Jing-Jia},
  year = {2022},
  series = {Proceeding - {{IEEE International Conference}} on {{Artificial Intelligence Circuits}} and {{Systems}}, {{AICAS}} 2022},
  pages = {506},
  doi = {10.1109/AICAS54282.2022.9869932},
  abstract = {In industrial applications, a robotic controller re-quires a low-latency computation process for real-time con-straints. In the meantime, more controllers are designed with DNN-based reinforcement learning, which needs increasing computation power. In this demo, we developed a fast prototyping infrastructure in AI -based mechatronics. Our software/hardware co-optimization incorporates a cyber-physical system (CPS), a host computer, and a DNN-based accelerator on an FPGA. The holistic accelerator is built upon the ESP SoC (System-on-Chip) platform with the high-level synthesis (HLS) technique and an improved interface. Our demonstration on an intelligent robotic arm showcases 101 times speedup over a CPU-based software implementation. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Coates2019,
  type = {Article},
  title = {An Instrument to Evaluate the Maturity of Bias Governance Capability in Artificial Intelligence Projects},
  author = {Coates, D.L. and Martin, A.},
  year = {2019},
  journal = {IBM Journal of Research and Development},
  volume = {63},
  number = {4-5},
  doi = {10.1147/JRD.2019.2915062},
  abstract = {Artificial intelligence (AI) promises unprecedented contributions to both business and society, attracting a surge of interest from many organizations. However, there is evidence that bias is already prevalent in AI datasets and algorithms, which, albeit unintended, is considered to be unethical, suboptimal, unsustainable, and challenging to manage. It is believed that the governance of data and algorithmic bias must be deeply embedded in the values, mindsets, and procedures of AI software development teams, but currently there is a paucity of actionable mechanisms to help. In this paper, we describe a maturity framework based on ethical principles and best practices, which can be used to evaluate an organization's capability to govern bias. We also design, construct, validate, and test an original instrument for operationalizing the framework, which considers both technical and organizational aspects. The instrument has been developed and validated through a two-phase study involving field experts and academics. The framework and instrument are presented for ongoing evolution and utilization. \textcopyright{} 1957-2012 IBM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{combi2022:manifesto,
  title = {A Manifesto on Explainability for Artificial Intelligence in Medicine},
  author = {Combi, Carlo and Amico, Beatrice and Bellazzi, Riccardo and Holzinger, Andreas and Moore, Jason H. and Zitnik, Marinka and Holmes, John H.},
  year = {2022},
  month = nov,
  journal = {Artificial Intelligence in Medicine},
  volume = {133},
  pages = {102423},
  issn = {0933-3657},
  doi = {10.1016/j.artmed.2022.102423},
  urldate = {2023-01-15},
  abstract = {The rapid increase of interest in, and use of, artificial intelligence (AI) in computer applications has raised a parallel concern about its ability (or lack thereof) to provide understandable, or explainable, output to users. This concern is especially legitimate in biomedical contexts, where patient safety is of paramount importance. This position paper brings together seven researchers working in the field with different roles and perspectives, to explore in depth the concept of explainable AI, or XAI, offering a functional definition and conceptual framework or model that can be used when considering XAI. This is followed by a series of desiderata for attaining explainability in AI, each of which touches upon a key domain in biomedicine.},
  langid = {english},
  keywords = {Artificial intelligence,Explainability,Explainable artificial intelligence,Interpretability,Interpretable artificial intelligence},
  file = {/Users/guru/Zotero/storage/7WRWBLTK/S0933365722001750.html}
}

@article{correachica2020:security,
  title = {Security in {{SDN}}: {{A}} Comprehensive Survey},
  shorttitle = {Security in {{SDN}}},
  author = {Correa Chica, Juan Camilo and Imbachi, Jenny Cuatindioy and Botero Vega, Juan Felipe},
  year = {2020},
  month = jun,
  journal = {Journal of Network and Computer Applications},
  volume = {159},
  pages = {102595},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2020.102595},
  urldate = {2023-01-15},
  abstract = {Software Defined Networking (SDN) is a revolutionary paradigm that is maturing along with other network technologies in the next-gen trend. The separation of control and data planes in SDN enables the emergence of novel network features like centralized flow management and network programmability that encourage the introduction of new and enhanced network functions in order to improve prominent network deployment aspects such as flexibility, scalability, network-wide visibility and cost-effectiveness. Although SDN exhibits a rapid evolution that is shaping this technology as a key enabler for future implementations in heterogeneous network scenarios, namely, datacenters, ISPs, corporate, academic and home; the technology is far from being considered secure and dependable to this day which inhibits its agile adoption. In recent years, the scientific community has been attracted to explore the field of SDN security to close the gap to SDN adoption. A twofold research context has been identified: on the one hand, leveraging SDN features to enhance security; while on the other hand one can find the pursue of a secure SDN system architecture. This article includes a description of security threats that menace SDN and a list of attacks that take advantage of vulnerabilities and misconfigurations in SDN constitutive elements. Accordingly, a discussion emphasizing the duality SDN-for-security and SDN-security is also presented. A comprehensive review of state-of-the art is accompanied by a categorization of the current research literature in a taxonomy that highlights the main characteristics and contributions of each proposal. Finally, the identified urgent needs and less explored topics are used to outline the opportunities and future challenges in the field of SDN security.},
  langid = {english},
  keywords = {Attack detection,Forensics,Network applications,Network monitoring,Network security,Openflow,Programmable networks,Security threats,Software defined networking,Threats mitigation,Traffic inspection,Virtualized network functions,Vulnerabilities}
}

@inproceedings{Cossu20221231,
  type = {Conference Paper},
  title = {A Blockchain-Based Data Notarization System for Smart Mobility Services},
  author = {Cossu, Raimondo and Lunesu, Maria Ilaria and Uras, Marco and Floris, Alessandro},
  year = {2022},
  series = {Proceedings - 2022 {{IEEE International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}}, {{SANER}} 2022},
  pages = {1231--1238},
  doi = {10.1109/SANER53432.2022.00146},
  abstract = {Nowadays, Internet of Things (IoT) applications are widespread in different scenarios, such as industry, mobility, healthcare, and agriculture. A relevant share of the credit is due to the Blockchain technology, which provides important features to IoT services, such as decentralized validation of transactions as well as immutability and traceability of the transactions. In this paper, we propose a Blockchain-based data notarization system for mobility services. First, we present an IoT-based crowd monitoring system aimed at counting the number of people in a specific area and providing information regarding people mobility (i.e., how people move within the city) and dwell time (i.e., the time people stayed at specific places). Then, we discuss our proposed data notarization system focused on ensuring data integrity and immutability of the mobility data collected by the crowd monitoring system, regardless of the used Blockchain. Finally, we provide experimental results regarding people mobility data collected during a literary event as well as an implementation of the proposed data notarization system using the EthernaZero blockchain. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Costa2018474,
  type = {Conference Paper},
  title = {A Search-Based Software Engineering Approach to Support Multiple Team Formation for Scrum Projects},
  author = {Costa, Alexandre and Ramos, Felipe and Perkusich, Mirko and Freire, Arthur and Almeida, Hyggo and Perkusich, Angelo},
  year = {2018},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2018-July},
  pages = {474--479},
  doi = {10.18293/SEKE2018-108},
  abstract = {Search-Based software engineering (SBSE) deals with metaheuristic search-based optimization techniques to provide solutions for complex problems. A popular problem in literature is the team formation problem (TFP), which consists of finding the best allocation of human resources to a software development project. This problem is recognized as NP-hard and it is more complex in companies that carry out multiple projects. This paper presents an effective and automated approach to allocate multiple developers into multiple teams to maximize the technical compatibility between them. The approach consists of an SBSE method that uses Genetic Algorithm to simultaneously build multiple teams, using data from tag-based profiles. We conducted an empirical evaluation using data from eight realworld software projects of a Brazilian company. The results indicate that tag-based profiles is a promising information source to represent technical knowledge, since the suggested teams were considered to have the proper skills to the attend the technical demand of the projects. The approach was able to reach high levels of satisfaction, delivering teams in an effective and automated way. Although, further investigation needs to be conducted to reach stronger conclusions. \textcopyright{} 2018 Universitat zu Koln. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{costa2022:genetic,
  title = {A Genetic Algorithm-Based Approach to Support Forming Multiple Scrum Project Teams},
  author = {Costa, Alexandre and Ramos, Felipe and Perkusich, Mirko and Neto, Ademar De Sousa and Silva, Luiz and Cunha, Felipe and Rique, Thiago and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {10},
  pages = {68981--68994},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3186347},
  abstract = {Forming effective teams is an essential but challenging task, especially for organizations that carry out multiple projects simultaneously, a problem known as the Multiple Team Formation (MTF) problem. The literature presents several solutions for the MTF problem, mostly modeling it as a search problem. However, the existing solutions are not suitable for Scrum projects. We addressed this gap by developing an approach composed of two main steps. First, we designed a Structured Task Model to support creating developers' profiles given their performance on past Scrum projects. Then, given a set of target projects' technology requirements and the available developers' profiles, we developed a Genetic Algorithm to form the teams for a set of target projects. We evaluated the proposed approach by comparing the teams formed by our approach with the ones formed by project managers from one organization. Our approach achieved 85\% of precision when compared with the teams provided by the project managers who worked on the same target projects. We also recorded an acceptance rate of up to 75\%. The significant value of precision achieved suggests that our approach can provide teams close to the project managers' expectations. In addition, our Structured Task Model offers a promising way to build technical profiles semi-automatically for Scrum developers. In future work, we intend to investigate how to complement the developers' profiles by using other types of attributes and knowledge sources.},
  keywords = {Costs,genetic algorithm,Genetic algorithms,Intelligent software engineering,multiple team formation problem,Resource management,Search problems,search-based software engineering,Software algorithms,Software engineering,Task analysis}
}

@book{creswell2016qualitative,
  title = {Qualitative Inquiry and Research Design: {{Choosing}} among Five Approaches},
  author = {Creswell, John W and Poth, Cheryl N},
  year = {2016},
  publisher = {{Sage publications}}
}

@article{Cui2021,
  type = {Article},
  title = {Analysis of Service-Oriented Architecture and Scrum Software Development Approach for {{IIoT}}},
  author = {Cui, Yanqing and Zada, Islam and Shahzad, Sara and Nazir, Shah and Khan, Shafi Ullah and Hussain, Naveed and Asshad, Muhammad},
  year = {2021},
  journal = {Scientific Programming},
  volume = {2021},
  doi = {10.1155/2021/6611407},
  abstract = {Flexibility and change adoption are key attributes for service-oriented architecture (SOA) and agile software development processes. Although the notion of agility is quite visible on both sides, still the integration of the two diverse concepts (architectural framework and development process) should be well thought of before employing them for a software development project. For this purpose, this study is designed to analyze the two diverse software architectural framework and development approaches, that is, SOA and Scrum process model, respectively, and their integrated environment in software project development setup perspective for Industrial Internet of Things (IIoT). This study also analyzes commonalities among Scrum process model and SOA architectural framework to identify compatibility between Scrum and SOA so that the Scrum process can be constructively used for SOA based projects. This study also examines the proper design and setup of Scrum process suitable for large-scale SOA based projects. For this purpose, an SOA based research and development project is selected as a case study using Scrum as the software development process. The project development and deployment perspective include eight core modules that constitute the overall project framework. \textcopyright{} 2021 Yanqing Cui et al.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Cunha2022118,
  type = {Conference Paper},
  title = {{{TeamPlus}}: {{A}} Decision Support System for Software Team Formation},
  author = {Cunha, Felipe and Perkusich, Mirko and Almeida, Hyggo and Gorg{\^o}nio, Kyller and Perkusich, Angelo},
  year = {2022},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {118--123},
  doi = {10.1145/3555228.3555275},
  abstract = {The literature presents several search and optimization-based solutions for the software team formation problem. However, the existing solutions do not reflect the real needs of practitioners. In general, they do not systematically address the problem, provide adequate visual resources, or integrate with data from enterprise systems. Therefore, the existing solutions are unaware of the actual dynamics of projects and miss important historical information from team members. This paper introduces TeamPlus, a Decision Support System capable of communicating with the people management environment and applying quantitative measures to data to build profiles, suggest teams based on entry criteria and allow the manager to manipulate the suggestions and adjust the preferences before the best decision is made. We developed TeamPlus through a qualitative study with industry practitioners and validated it, given its practical utility and effectiveness. This paper presents an overview of its main features, a comparison with other existing solutions, and preliminary evaluation results. Link to video: https://youtu.be/Uhf\textsubscript{a}YCjh9o License type: Proprietary software (no public license) \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{czarnecki2019:software,
  title = {Software Engineering for Automated Vehicles: {{Addressing}} the Needs of Cars That Run on Software and Data},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st International Conference on Software Engineering: {{Companion}} Proceedings ({{ICSE-Companion}})},
  author = {Czarnecki, Krzysztof},
  year = {2019},
  month = may,
  pages = {6--8},
  issn = {2574-1934},
  doi = {10.1109/ICSE-Companion.2019.00024},
  abstract = {Automated vehicles are AI-based safety-critical robots that fulfill transportation needs while interacting with the general public in traffic. Software engineering for automated vehicles requires a DevOps-style process with special considerations for functions based on machine learning and incremental safety assurance at vehicle and fleet level. This technical briefing reviews current challenges, industry practices, and opportunities for future research in software engineering for automated vehicles.},
  keywords = {Computer architecture,Machine learning,Roads,Safety,Software,Software engineering,Testing},
  file = {/Users/guru/Zotero/storage/N56TE4Q3/Czarnecki_2019_Software engineering for automated vehicles_2019 IEEEACM 41st international conference on software engineering Companion proceedings (ICSE-Companion).pdf}
}

@inproceedings{czarnecki2019:softwarea,
  title = {Software Engineering for Automated Vehicles: Addressing the Needs of Cars That Run on Software and Data},
  shorttitle = {Software Engineering for Automated Vehicles},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Software Engineering}}: {{Companion Proceedings}}},
  author = {Czarnecki, Krzysztof},
  year = {2019},
  month = may,
  series = {{{ICSE}} '19},
  pages = {6--8},
  publisher = {{IEEE Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1109/ICSE-Companion.2019.00024},
  urldate = {2023-01-15},
  abstract = {Automated vehicles are AI-based safety-critical robots that fulfill transportation needs while interacting with the general public in traffic. Software engineering for automated vehicles requires a DevOps-style process with special considerations for functions based on machine learning and incremental safety assurance at vehicle and fleet level. This technical briefing reviews current challenges, industry practices, and opportunities for future research in software engineering for automated vehicles.},
  keywords = {extra_paper,primary},
  file = {/Users/guru/Zotero/storage/AVBJ4L2A/Czarnecki_2019_Software engineering for automated vehicles_Proceedings of the 41st International Conference on Software Engineering Companion Proceedings.pdf}
}

@article{D'Souza2018945,
  type = {Article},
  title = {Enabling the Generation of Web Applications from Mockups},
  author = {D'Souza, Chris and Deufemia, Vincenzo and Ginige, Athula and Polese, Giuseppe},
  year = {2018},
  journal = {Software - Practice and Experience},
  volume = {48},
  number = {4},
  pages = {945--973},
  doi = {10.1002/spe.2559},
  abstract = {Mockups are widely used to elicit and validate user requirements in web applications, and several intuitive tools have been developed in recent years, actively involving the end user in the requirements solicitation process. However, most current web development approaches and tools discard mockups after the information-gathering process, abandoning the opportunity to exploit underlying information in them for autogenerating functional web applications. To overcome this limitation, we have devised a method for deriving the database schema and the logic of the web application from the information contained within mockups. In particular, the method gathers clues on how to organize the data and the control flow of the web application by analyzing the structure and relationships of the widgets in the mockup. Based on the proposed method, we have implemented a tool supporting the generation of web applications abiding by the model-view-controller architectural pattern. The tool has been evaluated by involving several end users in the development of web applications for different domains. Copyright \textcopyright{} 2018 John Wiley \& Sons, Ltd.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{dam2019:effective,
  title = {Towards Effective {{AI-Powered}} Agile Project Management},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st International Conference on Software Engineering: {{New}} Ideas and Emerging Results ({{ICSE-NIER}})},
  author = {Dam, Hoa Khanh and Tran, Truyen and Grundy, John and Ghose, Aditya and Kamei, Yasutaka},
  year = {2019},
  month = may,
  pages = {41--44},
  doi = {10.1109/ICSE-NIER.2019.00019},
  abstract = {The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry.},
  keywords = {Artificial intelligence,Artificial Intelligence,Engines,Planning,Project management,Software,Software Engineering,Task analysis,Tools},
  file = {/Users/guru/Zotero/storage/4KRIJZEE/Dam et al_2019_Towards effective AI-Powered agile project management_2019 IEEEACM 41st international conference on software engineering New ideas and emerging results (ICSE-NIER).pdf}
}

@inproceedings{Dam201941,
  type = {Conference Paper},
  title = {Towards Effective {{AI-powered}} Agile Project Management},
  author = {Dam, Hoa Khanh and Tran, Truyen and Grundy, John and Ghose, Aditya and Kamei, Yasutaka},
  year = {2019},
  series = {Proceedings - 2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{New Ideas}} and {{Emerging Results}}, {{ICSE-NIER}} 2019},
  pages = {41--44},
  doi = {10.1109/ICSE-NIER.2019.00019},
  abstract = {The rise of Artificial intelligence (AI) has the potential to significantly transform the practice of project management. Project management has a large socio-technical element with many uncertainties arising from variability in human aspects, e.g. customers' needs, developers' performance and team dynamics. AI can assist project managers and team members by automating repetitive, high-volume tasks to enable project analytics for estimation and risk prediction, providing actionable recommendations, and even making decisions. AI is potentially a game changer for project management in helping to accelerate productivity and increase project success rates. In this paper, we propose a framework where AI technologies can be leveraged to offer support for managing agile projects, which have become increasingly popular in the industry. \textcopyright{} 2019 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Dan2020,
  type = {Conference Paper},
  title = {An {{NLP}} Approach to Estimating Effort in a Work Environment},
  author = {Dan, Iftinca and Catalin, Rusu and Oliver, Oswald},
  year = {2020},
  series = {2020 28th {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}}, {{SoftCOM}} 2020},
  doi = {10.23919/SoftCOM50211.2020.9238219},
  abstract = {Effort estimation in Software development is becoming increasingly hard to do correctly, this is in part due to the growing complexity of software projects, but also due to the higher amount of projects in total. Specialists in multiple fields are required to work together to obtain a realistic estimate, but even then, there is a good amount of risk involved when planning based on the estimate. This leads to cost increases both for the actual estimation process and the losses taken due to wrong estimations. There are already a few project estimation systems out there which take into account presumed system size, development cycles (design, develop, test) or other project related variables. We want to introduce a more granular estimation system, which uses the text descriptions of various tasks but also takes into account available metadata like seniority of the employee which will be working on said tasks, client and project/project type when estimating. The system is built on a deep neural network. The results we are getting are promising so far and we are working on establishing the human baseline accuracy while the tool is available for company employees to use. \textcopyright{} 2020 University of Split, FESB.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Dantas20181811,
  type = {Conference Paper},
  title = {Effort Estimation in Agile Software Development: {{An}} Updated Review},
  author = {Dantas, Emanuel and Perkusich, Mirko and Dilorenzo, Ednaldo and Santos, Danilo F. S. and Almeida, Hyggo and Perkusich, Angelo},
  year = {2018},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  volume = {28},
  number = {11-12},
  pages = {1811--1831},
  doi = {10.1142/S0218194018400302},
  abstract = {One of the main issues of an agile software project is how to accurately estimate development effort. In 2014, a Systematic Literature Review (SLR) regarding this subject was published. The authors concluded that there were several gaps in the literature, such as the low level of accuracy of the techniques and little consensus on appropriate cost drivers. The goal of our work is to provide an updated review of the state of the art based on this reference SLR work. We applied a Forward Snowballing approach, in which our seed set included the former SLR and its selected papers. We identified a strong indication of solutions based on Artificial Intelligence and Machine Learning methods for effort estimation in Agile Software Development (ASD). We also identified that there is a gap in terms of agreement on suitable cost drivers. Thus, we applied Thematic Analysis in the selected papers and identified a representative set of 10 cost drivers for effort estimation. This updated review of the state of the art resulted in 24 new relevant papers selected. \textcopyright{} 2018 World Scientific Publishing Company.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Dantas2018496,
  type = {Conference Paper},
  title = {Effort Estimation in Agile Software Development: {{An}} Updated Review},
  author = {Dantas, Emanuel and Perkusich, Mirko and Dilorenzo, Ednaldo and Santos, Danilo F.S. and Almeida, Hyggo and Perkusich, Angelo},
  year = {2018},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2018-July},
  pages = {496--501},
  doi = {10.18293/SEKE2018-003},
  abstract = {One of the main issues of an agile software project is how to accurately estimate development effort. In 2014, it was published a Systematic Literature Review (SLR) regarding this subject. The authors of this SLR analyzed works from 2001 to 2013 and reached the number of 25 relevant papers. Therefore, the goal of our work is to provide an updated review of the state of the art based on this reference SLR work. We applied a Forward Snowballing approach, in which our seed set are the former SLR and its selected papers. We identified changes in this new review comparing it with the reference SLR: XP methodology was mentioned in just a few works; Use Case Points (UCP) method and Case Points as size metric were not found. We also observed a strong indication of solutions based on Artificial Intelligence and Machine Learning methods for effort estimation in Agile Software Development (ASD). Finally, we identified that in the reference SLR there is a gap in terms of agreement on suitable cost drivers. Thus, in our updated review, we applied Thematic Analysis in the selected papers and identified a representative set of 10 cost drivers for effort estimation. \textcopyright{} 2018 Universitat zu Koln. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Dantas201982,
  type = {Conference Paper},
  title = {An Effort Estimation Support Tool for Agile Software Development: {{An}} Empirical Evaluation},
  author = {Dantas, Emanuel and Costa, Alexandre and Vinicius, Marcus and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo},
  year = {2019},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2019-July},
  pages = {82--87},
  doi = {10.18293/SEKE2019-141},
  abstract = {Accurate effort estimation is an important part of the software process. In Agile Software Development, the techniques for predicting effort are mostly based on expert judgment, but there are approaches based on Machine Learning. The theme continues to be challenging and a subject of further studies given the difficulty of finding accurate solutions to the problem. This paper proposes and evaluates a tool based on the decision tree method for effort estimation in agile projects. We evaluated our tool given its accuracy and ease of use collecting data from four projects. To evaluate the accuracy, we compared the values of Magnitude of Relative Error from the teams' estimations with the values provided by the tool. To evaluate the ease of use, we used the Technology Acceptance Mode. The initial results show that the tool can be reliably used with minimal training. In terms of accuracy, the tool achieved lower error compared to the estimates provided by the teams (mean: 19.05\% vs 33.32\%), and the evaluation means in TAM were higher than 4.0 in ten of the eleven variables analyzed on a Likert scale. From this work, we conclude that estimation by decision tree is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques. \textcopyright{} 2019 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Dantas20221527,
  type = {Conference Paper},
  title = {Investigating Technological Risks and Mitigation Strategies in Software Projects},
  author = {Dantas, Emanuel and Neto, Ademar Sousa and Valadares, Dalton and Perkusich, Mirko and Ramos, Felipe and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  series = {Proceedings of the {{ACM Symposium}} on {{Applied Computing}}},
  pages = {1527--1535},
  doi = {10.1145/3477314.3507062},
  abstract = {Risks are present in any software project. In particular, technological risks are complex, volatile, and difficult to predict accurately. Despite this, the literature is scarce about such risks. This paper address this gap by identifying technological risk factors and strategies to mitigate them in software projects. We conducted and analyzed semi-structured interviews with 25 experts from ten organizations that execute software projects. Participants work with emerging technologies in academia-industry collaboration projects. Our analysis, using grounded theory, led to nine technological risk factors, classified into Research risk or Product risk. Further, for each technological risk, we identified the main mitigation strategy adopted by the practitioners. The results of our study can be used as a catalog to assist organizations in managing technological risks on software projects. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{dearle2007software,
  title = {Software Deployment, Past, Present and Future},
  booktitle = {Future of Software Engineering ({{FOSE}}'07)},
  author = {Dearle, Alan},
  year = {2007},
  pages = {269--284},
  organization = {{IEEE}}
}

@article{delaët2018:make,
  title = {Make\&activate-before-Break for Seamless {{SDN}} Route Updates},
  author = {Dela{\"e}t, Sylvie and Dolev, Shlomi and Khankin, Daniel and {Tzur-David}, Shimrit},
  year = {2018},
  month = dec,
  journal = {Computer Networks},
  volume = {147},
  pages = {81--97},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2018.10.005},
  urldate = {2023-01-15},
  abstract = {Software-Defined Networking (SDN) decouples the control and data planes, enabling limitless possibilities for implementing services and applications on top of the network abstraction layer. The centralized controller provides a real-time view of the entire underlying network infrastructure, and therefore, management of the agile network becomes more simplified. This flexibility requires online routing updates. However, during these updates consistency has to be preserved, i.e., no packet losses or unrecognized duplicates should occur. Moreover, routing updates should be done on the fly in an application-seamless fashion such that no significant irregular delays or ``communication hiccups'' in packet arrivals are introduced due to the (frequent) updates. In this paper, we are the first to present methods for seamlessly preserving consistency during on-the-fly routing updates. We utilize the make-before-break paradigm, in fact, the make\&activate-before-break (MABB) paradigm. We propose two methods for implementing such paradigm. In the first method, the new route is created and activated by duplicating packets along the corresponding portions of both new and old routes, without exceeding bandwidth demands on network links. Only when the controller verifies the correct establishment and operation of the specific portion of the new route, the corresponding portion of the old route is removed. This allows the sender to continue sending packets at an unchanged rate during the entire update process, a rate that is identical to the rate prior and after the update. In the second method, we propose a technique that utilizes the controller for verifying the correctness of the new portion of a route before its activation and the safeness of dismantling the current portion of a route, while keeping the connection operational.},
  langid = {english},
  keywords = {Make-before-break,Seamless route updates,Software-Defined Networking},
  file = {/Users/guru/Zotero/storage/KIEU45PT/Delaët et al_2018_Make&activate-before-break for seamless SDN route updates_Computer Networks.pdf;/Users/guru/Zotero/storage/RBJC4ULZ/S1389128618310326.html}
}

@article{demirci2019:optimal,
  title = {Optimal Placement of Virtual Network Functions in Software Defined Networks: {{A}} Survey},
  shorttitle = {Optimal Placement of Virtual Network Functions in Software Defined Networks},
  author = {Demirci, Sedef and Sagiroglu, Seref},
  year = {2019},
  month = dec,
  journal = {Journal of Network and Computer Applications},
  volume = {147},
  pages = {102424},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2019.102424},
  urldate = {2023-01-15},
  abstract = {Software Defined Networking (SDN) and Network Function Virtualization (NFV) are the latest promising technologies introduced with the goal of making networks more flexible, controllable, cost-efficient and innovative. These technologies are recognized as building blocks for the future Internet. Through the use of NFV, network functions are virtualized and made hardware-independent thus enabling faster innovations and developments. These virtualized functions such as network address translator, firewall, deep packet inspection etc. can reside on general-purpose servers in the SDN architecture. The necessity of achieving different operational objectives such as minimizing latency, network load, cost, and energy consumption makes determining the deployment locations of these functions an important research challenge. This problem is commonly known as the ``Virtual Network Function (VNF) Placement Problem''. This paper presents a detailed survey of recent research in the VNF Placement area. A novel thematic taxonomy of current solutions is provided based on the parameters derived from the literature. State-of-the-art studies are categorized and reviewed based upon the developed taxonomy. The common aspects and differences among existing solutions are highlighted through the parameters in the taxonomy. Finally, open research challenges and emerging trends are outlined and discussed for further studies.},
  langid = {english},
  keywords = {Network function virtualization,Optimization,Software defined networking,Taxonomy,Virtual network function placement,VNF},
  file = {/Users/guru/Zotero/storage/HW5QE57S/Demirci_Sagiroglu_2019_Optimal placement of virtual network functions in software defined networks_Journal of Network and Computer Applications.pdf;/Users/guru/Zotero/storage/FKM975GM/S1084804519302760.html}
}

@article{desousa2022:artificial,
  title = {Artificial Intelligence and Speedy Trial in the Judiciary: {{Myth}}, Reality or Need? {{A}} Case Study in the {{Brazilian Supreme Court}} ({{STF}})},
  shorttitle = {Artificial Intelligence and Speedy Trial in the Judiciary},
  author = {{de Sousa}, Weslei Gomes and Fidelis, Rafael Antunes and {de Souza Bermejo}, Paulo Henrique and {da Silva Gon{\c c}alo}, Ana Gersica and {de Souza Melo}, Bruno},
  year = {2022},
  month = jan,
  journal = {Government Information Quarterly},
  volume = {39},
  number = {1},
  pages = {101660},
  issn = {0740-624X},
  doi = {10.1016/j.giq.2021.101660},
  urldate = {2023-01-15},
  abstract = {Justice has dealt with procedural delays for decades, contributing to the rise of Artificial Intelligence (AI) to face the problem. Therefore, it is essential to understand and analyze the implications of AI on speedy trial and how AI can accelerate judicial processes. To this end, we conducted a case study at the Supreme Court of Brazil (STF), collecting documents and conducting interviews for content analysis. The results consolidated a framework model that combines resources and impacts of AI on the velocity of legal decisions demonstrating how and what solutions contribute for judgment, pre-trial, and legal support. Therefore, it is believed that the gain in procedural speed with AI is not just a myth. It can become a reality, but with limitations, because there is a long way between the use of this technology in merely operational tasks and its use in complex activities such as evaluating processes in their entirety. For these future challenges, we highlight research proposals for more effective advances.},
  langid = {english},
  keywords = {AI,Artificial intelligence,Artificial neural network,Justice delay,Natural language processing,Speedy trial},
  file = {/Users/guru/Zotero/storage/JDGA5MRA/S0740624X21000964.html}
}

@article{Dey2021,
  type = {Article},
  title = {Multilayered Review of Safety Approaches for Machine Learning-Based Systems in the Days of {{AI}}},
  author = {Dey, Sangeeta and Lee, Seok-Won},
  year = {2021},
  journal = {Journal of Systems and Software},
  volume = {176},
  doi = {10.1016/j.jss.2021.110941},
  abstract = {The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety. \textcopyright{} 2021 Elsevier Inc.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/EY79PR98/1-s2.0-S0164121221000388-main.pdf}
}

@article{dey2021:multilayered,
  title = {Multilayered Review of Safety Approaches for Machine Learning-Based Systems in the Days of {{AI}}},
  author = {Dey, Sangeeta and Lee, Seok-Won},
  year = {2021},
  month = jun,
  journal = {Journal of Systems and Software},
  volume = {176},
  pages = {110941},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.110941},
  urldate = {2023-01-15},
  abstract = {The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety.},
  langid = {english},
  keywords = {Autonomous systems,Intelligent software systems,Machine learning,Safety analysis,Software engineering},
  file = {/Users/guru/Zotero/storage/AHMI9CTU/S0164121221000388.html}
}

@article{díaz-de-arcaya2023:orfeon,
  title = {Orfeon: {{An AIOps}} Framework for the Goal-Driven Operationalization of Distributed Analytical Pipelines},
  shorttitle = {Orfeon},
  author = {{D{\'i}az-de-Arcaya}, Josu and {Torre-Bastida}, Ana I. and Mi{\~n}{\'o}n, Ra{\'u}l and Almeida, Aitor},
  year = {2023},
  month = mar,
  journal = {Future Generation Computer Systems},
  volume = {140},
  pages = {18--35},
  issn = {0167-739X},
  doi = {10.1016/j.future.2022.10.008},
  urldate = {2023-01-15},
  abstract = {The use of Artificial Intelligence solutions keeps raising in the business domain. However, this adoption has not brought the expected results to companies so far. There are several reasons that make Artificial Intelligence solutions particularly complicated to adopt by businesses, such as the knowledge gap between the data science and operations teams. In this paper, we tackle the operationalization of distributed analytical pipelines in heterogeneous production environments, which span across different computational layers. In particular, we present a system called Orfeon, which can leverage different objectives and yields an optimized deployment for these pipelines. In addition, we offer the mathematical formulation of the problem alongside the objectives in hand (i.e. resilience, performance, and cost). Next, we propose a scenario utilizing cloud and edge infrastructural devices, in which we demonstrate how the system can optimize these objectives, without incurring scalability issues in terms of time nor memory. Finally, we compare the usefulness of Orfeon with a variety of tools in the field of machine learning operationalization and conclude that it is able to outperform these tools under the analyzed criteria, making it an appropriate system for the operationalization of machine learning pipelines.},
  langid = {english},
  keywords = {AIOps,Analytical pipelines,Edge computing,Machine learning operationalization,MLOps},
  file = {/Users/guru/Zotero/storage/FK84IBS8/S0167739X22003223.html}
}

@inproceedings{Dilhara2022736,
  type = {Conference Paper},
  title = {Discovering Repetitive Code Changes in Python {{ML}} Systems},
  author = {Dilhara, Malinda and Ketkar, Ameya and Sannidhi, Nikhith and Dig, Danny},
  year = {2022},
  series = {Proceedings - {{International Conference}} on {{Software Engineering}}},
  volume = {2022-May},
  pages = {736--748},
  doi = {10.1145/3510003.3510225},
  abstract = {Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use best coding practices. To fill the knowledge gap and advance the science and tooling in ML software evolution, we conducted the first and most fine-grained study on code change patterns in a diverse corpus of 1000 top-rated ML systems comprising 58 million SLOC. To conduct this study we reuse, adapt, and improve upon the state-of-the-art repetitive change mining techniques. Our novel tool, R-CPATMINER, mines over 4M commits and constructs 350K fine-grained change graphs and detects 28K change patterns. Using thematic analysis, we identified 22 pattern groups and we reveal 4 major trends of how ML developers change their code. We surveyed 650 ML developers to further shed light on these patterns and their applications, and we received a 15\% response rate. We present actionable, empirically-justified implications for four audiences: (i) researchers, (ii) tool builders, (iii) ML library vendors, and (iv) developers and educators. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{dilorenzo2020:enabling,
  title = {Enabling the Reuse of Software Development Assets through a Taxonomy for User Stories},
  author = {Dilorenzo, Ednaldo and Dantas, Emanuel and Perkusich, Mirko and Ramos, Felipe and Costa, Alexandre and Albuquerque, Danyllo and Almeida, Hyggo and Perkusich, Angelo},
  year = {2020},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {8},
  pages = {107285--107300},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2996951},
  abstract = {Context - Agile Software Development (ASD) and Reuse-Driven Software Engineering (RDSE) are well-accepted strategies to improve the efficiency of software processes. A challenge to integrate both approaches is that ASD relies mostly on tacit knowledge, hampering the reuse of software development assets. An opportunity to enable RDSE for ASD is by improving the traceability between user stories (USs), the most used notation to register product requirements in ASD. Having enough link semantics between USs could enable defining similarity between them and, consequently, promote RDSE for ASD. However, this is an open challenge. Objective - To propose a taxonomy for adding link semantics between USs, focusing on easing the task of identifying similar ones. Such links, with support of traceability tools, enable the reuse of USs and their related assets. Method: We constructed a taxonomy for types of US focusing on Web Information Systems. The taxonomy is used to classify the US, given two facets: module and operation. Such information is used to infer the similarity between USs using link rules. We developed the taxonomy based on an empirical analysis of five product backlogs, containing a total of 118 USs. Afterward, we validated the taxonomy in terms of its potential to enable the reuse of US-related assets. First, we executed an offline validation by applying it to classify 530 USs from 26 already ended projects. Finally, we applied the taxonomy in a case study with two ongoing projects (59 USs). Results: The proposed taxonomy for USs is composed of two sub-facets, namely, module and operation, which have, respectively, three and 18 terms. In terms of coverage, for the offline study and case study, we classified 90.17\% of the USs with the proposed taxonomy. For the case study, we classified all the USs analyzed. Conclusion: We concluded that it is possible to use our approach to compare USs and, consequently, retrieve their related assets. Our results regarding its practical utility have shown that users considered the taxonomy a useful approach to ease the process of assessing the similarity between user stories.},
  keywords = {Agile software development,Companies,information retrieval,Semantics,Software,Software engineering,software reuse,Task analysis,Taxonomy,technology acceptance model,Tools,user stories},
  file = {/Users/guru/Zotero/storage/G28RGLAU/Dilorenzo et al_2020_Enabling the reuse of software development assets through a taxonomy for user_IEEE access practical innovations, open solutions.pdf}
}

@inproceedings{Dong2022570,
  type = {Conference Paper},
  title = {Semi-Supervised Pre-Processing for Learning-Based Traceability Framework on Real-World Software Projects},
  author = {Dong, Liming and Zhang, He and Liu, Wei and Weng, Zhiluo and Kuang, Hongyu},
  year = {2022},
  series = {{{ESEC}}/{{FSE}} 2022 - {{Proceedings}} of the 30th {{ACM Joint Meeting European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  pages = {570--582},
  doi = {10.1145/3540250.3549151},
  abstract = {The traceability of software artifacts has been recognized as an important factor to support various activities in software development processes. However, traceability can be difficult and time-consuming to create and maintain manually, thereby automated approaches have gained much attention. Unfortunately, existing automated approaches for traceability suffer from practical issues. This paper aims to gain an understanding of the potential challenges for the underperforming of the state-of-the-art, ML-based trace link classifiers applied in real-world projects. By investigating different industrial datasets, we found that two critical (and classic) challenges, i.e. data imbalance and sparse problems, lie in real-world projects' traceability automation. To overcome these challenges, we developed a framework called SPLINT to incorporate hybrid textual similarity measures and semi-supervised learning strategies as enhancements to the learning-based traceability approaches. We carried out experiments with six open-source platforms and ten industry datasets. The results confirm that SPLINT is able to operate at higher performance on two communities' datasets. Specifically, the industrial datasets, which significantly suffer from data imbalance and sparsity problems, show an increase in F2-score over 14\% and AUC over 8\% on average. The adjusted class-balancing and self-training policies used in SPLINT (CBST-Adjust) also work effectively for the selection of pseudo-labels on minor classes from unlabeled trace sets, demonstrating SPLINT's practicability. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{donnelly2022:drift,
  title = {The Drift of Industrial Control Systems to Pseudo Security},
  author = {Donnelly, Peter and Abuhmida, Mabrouka and Tubb, Christopher},
  year = {2022},
  month = sep,
  journal = {International Journal of Critical Infrastructure Protection},
  volume = {38},
  pages = {100535},
  issn = {1874-5482},
  doi = {10.1016/j.ijcip.2022.100535},
  urldate = {2023-01-15},
  abstract = {With the promise of a synergistic impact on the Efficiency-Thoroughness Trade-Off, Marketing is increasingly promoting types of Industrial Control Systems (ICS) that by some means combine the two usually segregated core ICS functions into one. A Basic Process Control System (BPCS) is combined with a Safety-Instrumented System (SIS) in a physically integrated form factor using shared resources of some kind. This paper suggests such a strategic choice of technology can result in functional safety (FS) hazards or security vulnerabilities, giving rise to resilience concerns. It takes a sceptical view of such an approach and instead proposes strict segregation of such functions and resources. In the context of critical national infrastructure (CNI), where potentially high consequence events (HCE) may arise from unplanned incidents, the outcome of this paper is to warn against the use of such architecture - even beyond that arena. Both ancient and modern, yet similarly strategic, historical decisions are used as metaphors to illustrate how sometimes insufficiently scrutinised technologies can be later regretted. Practical technical, organisational, and cultural measures are offered to steer against the headwind of commercial pressures in promoting integrated FS and security of the BPCS-SIS environment. A contribution is made of evidence-based, business intelligence gathering measures for BPCS-SIS vendor selection together with a proposal for an alternative, adopted application of proven Uncertainty Assessment Reporting techniques for industrial certification bodies and business stakeholders alike.},
  langid = {english},
  keywords = {Complexity,Industrial control systems,Integration,Interdependence,Safety,Security}
}

@article{Eappen2020,
  type = {Article},
  title = {A Survey on Soft Computing Techniques for Spectrum Sensing in a Cognitive Radio Network},
  author = {Eappen, Geoffrey and Shankar, T.},
  year = {2020},
  journal = {SN Computer Science},
  volume = {1},
  number = {6},
  doi = {10.1007/s42979-020-00372-z},
  abstract = {The need for faster wireless connectivity is increasing rapidly in all the sectors of the technologies. Whether it is a patient monitoring system, military application, entertainment services, streaming services, or global stock markets, there is a tremendous increase in the need for enhanced wireless telecommunication services. The wireless telecommunication consumers rely on bulk data, and massive growth in the number of users has resulted in the spectrum congestion. To avoid such spectrum congestion and to satisfy the data hunger of the wireless telecommunication users, the possible solution is Cognitive Radio Network (CRN). A CRN, therefore, plays a significant role in the field of wireless communication, and an efficient spectrum sensing enhances the effectiveness of the CRN. In this paper, complete research carried out so far in the field of spectrum sensing for CRN is discussed. Different soft computing techniques (GA, PSO, ABC, ACO, FFA, FSS, Cuckoo Search, ANN, FIS, GFIS) are surveyed in this paper, along with a detailed comparative analysis between conventional and soft computing techniques for spectrum sensing. In addition to that, the challenges faced in the implementation of CRN and its requirements is also addressed. Different spectrum sensing elements and requirements are presented and road map of spectrum sensing with soft computing techniques towards 5G is discussed. Furthermore, the paper also suggests the future prospects, research challenges and open issues associated with soft computing techniques for spectrum sensing in CRN. \textcopyright{} 2020, Springer Nature Singapore Pte Ltd.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Ecar2021,
  type = {Conference Paper},
  title = {Work like Ants! {{Atta}} 2.0: {{Dealing}} with Self-Organized Teams},
  author = {Ecar, Miguel and Da Silva, Jo{\~a}o Pablo Silva},
  year = {2021},
  series = {{{ACM International Conference Proceeding Series}}},
  doi = {10.1145/3466933.3466963},
  abstract = {Traditional software engineering approaches were developed based on the old manufacturing process, in terms of roles, responsibilities, tasks, and hierarchy. Modern approaches, such as, Agile approaches, are based on a fusion of these traditional approaches and the Agile Manifesto, which changes the perspective of main software process objectives. These approaches are designed to encourage self-management, which may be challenging even in big or very small software companies. This paper proposes a software engineering complementary framework called Atta 2.0 (Atta 2), which is an approach inspired by a nature organization and aims to deal with the difficulties of self-organization issues. We performed a case study on a start-up company, that follows Scrum guidelines, has a very small team, and maintains several projects at the same time. As the outcome, we have success running the Atta 2 and obtained gains in terms of self-organization, structure, and visibility. Based on this we advocate that Atta 2 improved Sprint planning and execution, tasks creation and distribution, and slightly the quality of delivered artifacts. We also focus that Atta 2 helped to integrate new team members and to extract the best from the new ones and old team members. We advocate that Atta 2, may have value to the community, once it is a complementary option to aggregates value to well-known agile or traditional approaches. Thus, we concluded that Atta 2 can be used jointly with other management approaches, in order to promote the best-aggregated value. \textcopyright{} 2021 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Eidenbenz2021362,
  type = {Conference Paper},
  title = {Boosting Exploratory Testing of Industrial Automation Systems with {{AI}}},
  author = {Eidenbenz, Raphael and Franke, Carsten and Sivanthi, Thanikesavan and Schoenborn, Sandro},
  year = {2021},
  series = {Proceedings - 2021 {{IEEE}} 14th {{International Conference}} on {{Software Testing}}, {{Verification}} and {{Validation}}, {{ICST}} 2021},
  pages = {362--371},
  doi = {10.1109/ICST49551.2021.00048},
  abstract = {Testing of large and complex industrial control systems is challenging as the space of possible input and environmental parameters is large. Searching the entire space for potential failures is practically infeasible. This paper introduces an industrial control system robustness testing problem and evaluates artificial intelligence (AI) based strategies to efficiently explore the space and to identify parameter sets that can cause the system to fail. The proposed solution approach uses regression techniques to speed up the search and clustering methods to identify parameter sets that represent distinct system failures. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@book{ertel2018introduction,
  title = {Introduction to Artificial Intelligence},
  author = {Ertel, Wolfgang},
  year = {2018},
  publisher = {{Springer}}
}

@inproceedings{estdale2018applying,
  title = {Applying the {{ISO}}/{{IEC}} 25010 Quality Models to Software Product},
  booktitle = {European Conference on Software Process Improvement},
  author = {Estdale, John and Georgiadou, Elli},
  year = {2018},
  pages = {492--503},
  organization = {{Springer}}
}

@article{Fadhil20201709,
  type = {Article},
  title = {Artificial Intelligence for Software Engineering: {{An}} Initial Review on Software Bug Detection and Prediction},
  author = {Fadhil, Julanar Ahmed and Wei, Koh Tieng and Na, Kew Si},
  year = {2020},
  journal = {Journal of Computer Science},
  volume = {16},
  number = {12},
  pages = {1709--1717},
  doi = {10.3844/jcssp.2020.1709.1717},
  abstract = {The need for speed and quality in delivering all software engineering artifacts has inevitably remained the biggest challenge in today's software development environment. While everyone caters to complex software engineering processes, new releases are expected by the market on almost a daily basis. Thus, several Artificial Intelligence (AI) techniques have been introduced that are intensively used in the modern software engineering industry to fulfill market needs. This paper presents the initial results of our review work on software bug detection and prediction studies using AI techniques. Our focus is to (i) identify factors affecting the effectiveness of current software bug detection and prediction techniques and (ii) identify the effectiveness of AI techniques in improving current software bug detection and prediction techniques. The evidence showed that the software engineering domain has utilized artificial intelligence approaches and techniques to facilitate the complex tasks of software bug detection and bug prediction. It mainly demonstrates the significance of merging artificial intelligence with the software engineering domain in terms of reduced overhead and efficient results to enhance the quality of software products. \textcopyright{} 2020 Julanar Ahmed Fadhil, Koh Tieng Wei and Kew Si Na.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Fatima2022,
  type = {Article},
  title = {Integration of Multi Access Edge Computing with Unmanned Aerial Vehicles: {{Current}} Techniques, Open Issues and Research Directions},
  author = {Fatima, Nida and Saxena, Paresh and Gupta, Manik},
  year = {2022},
  journal = {Physical Communication},
  volume = {52},
  doi = {10.1016/j.phycom.2022.101641},
  abstract = {During the last decade, research and development in the field of multi access edge computing (MEC) has rapidly risen to prominence. One of the factors propelling MEC's evolution is the ability to deploy edge servers capable of providing both communication and computational services in close proximity to the mobile user terminal. MEC has been regarded as a potentially transformative technique for fifth-generation (5G) and beyond 5G (B5G) wireless communication systems, as well as a possible complement to traditional cloud computing. Additionally, unmanned aerial vehicles (UAVs) integrated with MEC will play a critical role by introducing an additional mobility based computational layer to provide more secure, efficient and faster services. UAV enabled MEC offers seamless connectivity, fulfilling the promise of 5G's ubiquitous connectivity. Due to the enormous interest in UAV enabled MEC, there has been a tremendous increase in the number of published research articles in this domain; however, the research area still lacks a systematic study and categorization. We present a systematic literature review (SLR) on UAV enabled MEC, examining and analyzing data on the current state of the art using preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. To streamline our assessment, this study analyzes several research papers carefully selected through a multi-stage process satisfying the eligibility criteria defined in the paper. One of the SLR's primary contributions is to broadly classify the research in the UAV enabled MEC domain into different categories including energy efficiency, resource allocation, security, architecture, and latency. We have identified key findings, technology, and pros and cons for the selected articles under each category. Additionally, we discuss the key open issues related to scalability and fairness, resource allocation and offloading optimization, service delivery with a focus on quality of experience (QoE) and quality of service (QoS), and standardization. Finally, we discuss several future research directions that would address the aforementioned issues and emerging use cases for UAV enabled MEC. \textcopyright{} 2022 Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{fayoumi2016:conceptual,
  title = {Conceptual Modeling for the Design of Intelligent and Emergent Information Systems},
  author = {Fayoumi, Amjad and Loucopoulos, Pericles},
  year = {2016},
  month = oct,
  journal = {Expert Systems with Applications},
  volume = {59},
  pages = {174--194},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2016.04.019},
  urldate = {2023-01-15},
  abstract = {A key requirement to today's fast changing economic environment is the ability of organizations to adapt dynamically in an effective and efficient manner. Information and Communication Technologies play a crucially important role in addressing such adaptation requirements. The notion of `intelligent software' has emerged as a means by which enterprises can respond to changes in a reactive manner but also to explore, in a pro-active manner, possibilities for new business models. The development of such software systems demands analysis, design and implementation paradigms that recognize the need for `co-development' of these systems with enterprise goals, processes and capabilities. The work presented in this paper is motivated by this need and to this end it proposes a paradigm that recognizes co-development as a knowledge-based activity. The proposed solution is based on a multi-perspective modeling approach that involves (i) modeling key aspects of the enterprise, (ii) reasoning about design choices and (iii) supporting strategic decision-making through simulations. The utility of the approach is demonstrated though a case study in the field of marketing for a start-up company.},
  langid = {english},
  keywords = {Business architecture,Business best practices,Conceptual modeling,Design rationale,Enterprise architecture,System dynamics},
  file = {/Users/guru/Zotero/storage/B62F43HC/S0957417416301853.html}
}

@article{fayoumi2021:integrated,
  title = {An Integrated Socio-Technical Enterprise Modelling: {{A}} Scenario of Healthcare System Analysis and Design},
  shorttitle = {An Integrated Socio-Technical Enterprise Modelling},
  author = {Fayoumi, Amjad and Williams, Richard},
  year = {2021},
  month = sep,
  journal = {Journal of Industrial Information Integration},
  volume = {23},
  pages = {100221},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2021.100221},
  urldate = {2023-01-15},
  abstract = {One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.},
  langid = {english},
  keywords = {Conceptual Modelling,Enterprise Integrated Model,Enterprise Modelling,Healthcare System,Socio-technical Systems}
}

@article{feldmann2013:modeldriven,
  title = {Model-{{Driven Engineering}} and {{Semantic Technologies}} for the {{Design}} of {{Cyber-Physical Systems}}},
  author = {Feldmann, S. and R{\"o}sch, S. and Sch{\"u}tz, D. and {Vogel-Heuser}, B.},
  year = {2013},
  month = may,
  journal = {IFAC Proceedings Volumes},
  series = {11th {{IFAC Workshop}} on {{Intelligent Manufacturing Systems}}},
  volume = {46},
  number = {7},
  pages = {210--215},
  issn = {1474-6670},
  doi = {10.3182/20130522-3-BR-4036.00050},
  urldate = {2023-01-15},
  abstract = {Rapid changes in product and system requirements force today's companies to cope with unforeseen adaptations of industrial plants. To enable the implementation of intelligent, self-adapting systems, manufacturing systems must be composed to Cyber-Physical Systems (CPS) out of Cyber-Physical Modules (CPMs). However, cross-disciplinary design of CPS and the multitude of persons involved in this process increases complexity in (re-)engineering drastically. Identification and modeling of typical CPM types in a CPM library provides a first step towards enabling reuse of constructed and verified CPMs and, thus, a knowledge base making automatic composition of CPS out of CPMs possible. In this paper, a concept for retrieving existing automation software functionality from a given hardware description by means of model-driven engineering and semantic technologies is presented.},
  langid = {english},
  keywords = {cyber-physical systems,extra_paper,knowledge-based systems,manufacturing systems,primary},
  file = {/Users/guru/Zotero/storage/JIVPAN2I/Feldmann et al. - 2013 - Model-Driven Engineering and Semantic Technologies.pdf}
}

@article{ferreira2023:lessons,
  title = {Lessons Learned to Improve the {{UX}} Practices in Agile Projects Involving Data Science and Process Automation},
  author = {Ferreira, Bruna and Marques, Silvio and Kalinowski, Marcos and Lopes, H{\'e}lio and Barbosa, Simone D. J.},
  year = {2023},
  month = mar,
  journal = {Information and Software Technology},
  volume = {155},
  pages = {107106},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.107106},
  urldate = {2023-01-15},
  abstract = {Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams' perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users' routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions.},
  langid = {english},
  keywords = {Agile,Data science,extra_paper,Lean inception,primary,User experience,User involvement,User participation,User-centered design},
  file = {/Users/guru/Zotero/storage/ZUTJY9W5/1-s2.0-S0950584922002154-main.pdf;/Users/guru/Zotero/storage/ACSIY62S/S0950584922002154.html}
}

@article{Firdose20184735,
  type = {Article},
  title = {{{PORM}}: {{Predictive}} Optimization of Risk Management to Control Uncertainty Problems in Software Engineering},
  author = {Firdose, Salma and Rao, L. Manjunath},
  year = {2018},
  journal = {International Journal of Electrical and Computer Engineering},
  volume = {8},
  number = {6},
  pages = {4735--4744},
  doi = {10.11591/ijece.v8i6.pp4735-4744},
  abstract = {Irrespective of different research-based approaches toward risk management, developing a precise model towards risk management is found to be a computationally challenging task owing to critical and vague definition of the origination of the problems. This research work introduces a model called as PROM i.e. Predictive Optimization of Risk Management with the perspective of software engineering. The significant contribution of PORM is to offer a reliable computation of risk analysis by considering generalized practical scenario of software development practices in Information Technology (IT) industry. The proposed PORM system is also designed and equipped with better risk factor assessment with an aid of machine learning approach without having more involvement of iteration. The study outcome shows that PORM system offers computationally cost effective analysis of risk factor as assessed with respect to different quality standards of object oriented system involved in every software projects. \textcopyright{} 2018 Institute of Advanced Engineering and Science.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Fok2016,
  type = {Article},
  title = {{{ControlIt}}! - {{A}} Software Framework for Whole-Body Operational Space Control},
  author = {Fok, Chien-Liang and Johnson, Gwendolyn and Yamokoski, John D. and Mok, Aloysius and Sentis, Luis},
  year = {2016},
  journal = {International Journal of Humanoid Robotics},
  volume = {13},
  number = {1},
  doi = {10.1142/S0219843615500401},
  abstract = {Whole Body Operational Space Control (WBOSC) enables floating-base highly redundant robots to achieve unified motion/force control of one or more operational space objectives while adhering to physical constraints. It is a pioneering algorithm in the field of human-centered Whole-Body Control (WBC). Although there are extensive studies on the algorithms and theory behind WBOSC, limited studies exist on the software architecture and APIs that enable WBOSC to perform and be integrated into a larger system. In this paper, we address this by presenting ControlIt!, a new open-source software framework for WBOSC. Unlike previous implementations, ControlIt! is multi-threaded to increase maximum servo frequencies using standard PC hardware. A new parameter binding mechanism enables tight integration between ControlIt! and external processes via an extensible set of transport protocols. To support a new robot, only two plugins and a URDF model is needed - the rest of ControlIt! remains unchanged. New WBC primitives can be added by writing Task or Constraint plugins. ControlIt!'s capabilities are demonstrated on Dreamer, a 16-DOF torque controlled humanoid upper body robot containing both series elastic and co-actuated joints, and using it to perform a product disassembly task. Using this testbed, we show that ControlIt! can achieve average servo latencies of about 0.5ms when configured with two Cartesian position tasks, two orientation tasks, and a lower priority posture task. This is 10 times faster than the 5ms that was achieved using UTA-WBC, the prototype implementation of WBOSC that is both application and platform-specific. Variations in the product's position is handled by updating the goal of the Cartesian position task. ControlIt!'s source code is released under LGPL and we hope it will be adopted and maintained by the WBC community for the long term as a platform for WBC development and integration. \textcopyright{} 2016 World Scientific Publishing Company.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{foote2022history,
  title = {The History of Machine Learning and Its Convergent Trajectory towards {{AI}}},
  author = {Foote, Keith D},
  year = {2022},
  journal = {Machine Learning and the City: Applications in Architecture and Urban Design},
  pages = {129--142},
  publisher = {{Wiley Online Library}}
}

@article{fourati2021:comprehensive,
  title = {Comprehensive Survey on Self-Organizing Cellular Network Approaches Applied to {{5G}} Networks},
  author = {Fourati, Hasna and Maaloul, Rihab and Chaari, Lamia and Jmaiel, Mohamed},
  year = {2021},
  month = nov,
  journal = {Computer Networks},
  volume = {199},
  pages = {108435},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2021.108435},
  urldate = {2023-01-15},
  abstract = {Self-Organizing Network (SON) stands for a key concept characterizing the behavior of the future mobile networks. The evolution of telecom infrastructures towards 5G transforms the network management from the traditional and static processes to automatic and dynamic ones. SON was proposed to offer agile on-demand services to the users through providing self-adaptation capabilities to mobile networks on different categories. This paper presents a detailed and exhaustive survey on SON evolution from 4G towards 5G networks. The central focus of this survey is upon providing a deep understanding of SON mechanisms along with the architectural changes associated with 5G networks. Within this framework, the approaches and trends in self-organizing cellular networks are discussed. Additionally, the main functionalities of SON, namely self-configuration, self-optimization and self-healing are displayed. Our work serves as an enlightening guideline for future research works on SON as far as cellular networks domain is concerned.},
  langid = {english},
  keywords = {5G,Backhaul optimization,Caching optimization,Coverage and capacity optimization,Machine learning,Mobility management,Resource optimization,Self-configuration,Self-healing,Self-optimization,SON,SON architectures},
  file = {/Users/guru/Zotero/storage/7ZJTPSKY/S1389128621003960.html}
}

@article{francese2015:using,
  title = {Using {{Project-Based-Learning}} in a Mobile Application Development Course\textemdash{{An}} Experience Report},
  author = {Francese, Rita and Gravino, Carmine and Risi, Michele and Scanniello, Giuseppe and Tortora, Genoveffa},
  year = {2015},
  month = dec,
  journal = {Journal of Visual Languages \& Computing},
  series = {Special {{Issue}} on {{DMS2015}}},
  volume = {31},
  pages = {196--205},
  issn = {1045-926X},
  doi = {10.1016/j.jvlc.2015.10.019},
  urldate = {2023-01-15},
  abstract = {In this paper, we report the experience gained in a Mobile Application Development course. We involved students in Computer Science at the University of Salerno, who in teams had to conduct a project. The goal of this project was to design and develop applications (or simply app) for Android-based devices. The adopted teaching approach was based on Project-Based-Learning and enhanced collaboration and competition. Collaboration took place among members of the same team (intra-team), while competition among different teams of students (extra-team). To allow intra-team collaboration, students used GitHub as Computer-Supported-Collaborative-Learning tool. It provided support for implicit and explicit communication among members in each team and for distributed revision control and management of software artifacts (e.g., source code and requirements models). Developed apps underwent a final public competition prized by IT managers of national and international software companies. This is how we implemented extra-team competition. IT managers expressed a positive judgment on both students׳ competition and developed apps. Also, students provided very good feedback on used teaching approach and support GitHub provided.},
  langid = {english},
  keywords = {Android,Collaborative learning,competitive learning,Mobile Application Development,Project-Based-Learning}
}

@inproceedings{Freire201840,
  type = {Conference Paper},
  title = {Investigating Gaps on {{Agile Improvement Solutions}} and Their Successful Adoption in Industry Projects-{{A}} Systematic Literature Review},
  author = {Freire, Arthur and Meireles, And{\'r}e and Guimar{\~a}es, Gleyser and Perkusich, Mirko and Da Silva, Raissa and Gorgonio, Kyller and Perkusich, Angelo and Almeida, Hyggo},
  year = {2018},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2018-July},
  pages = {40--45},
  doi = {10.18293/SEKE2018-185},
  abstract = {Background: The focus of Agile software development (ASD) is different than plan-driven development, requiring new software process improvement (SPI) paradigms. Objective: To identify and synthesize the possible gaps of Agile improvement solutions (AIS) given their focus on people factors, report of successful adoption in industry projects and availability of tool support. Method: We applied a Systematic Literature Review of studies published up to (and including) 2017 through backward and forward snowballing given a start set. Results: In total, we evaluated 55 papers, of which 44 included AIS and the main findings are: 1) 26 consider teamwork factors; 2) 21 were applied on industry; 3) 10 out of these 21 presented evidence of increase in company performance; and 4) 19 of the solutions are for the purpose of adoption, 18 for assessment and 8 are maturity models. Conclusion: The main implication for this research is a need for more and better empirical studies documenting and evaluating AIS. For the industry, the review provides a map of current AIS approaches and can be used as a starting point to adopt agile SPI. \textcopyright{} 2018 Universitat zu Koln. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Freire2021223,
  type = {Conference Paper},
  title = {Towards a Comprehensive Understanding of Agile Teamwork: {{A}} Literature-Based Thematic Network},
  author = {Freire, Arthur and Neto, Manuel and Perkusich, Mirko and Costa, Alexandre and Gorg{\^o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2021},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2021-July},
  pages = {223--228},
  doi = {10.18293/SEKE2021-106},
  abstract = {Agile Software Development (ASD) has become the mainstream software development method of choice. Its core fundamentals are based on Teamwork factors and the higher value it gives to individuals and their interactions over processes and tools. Teamwork and human factors have been addressed as essential topics in the literature, and researchers have stated the importance of measuring it to increase the chances of success of ASD projects. However, there is no common understanding regarding the factors that should be considered for defining an ASD Teamwork construct. Driven by this problem, this paper presents a thematic network that defines the themes (i.e., factors) that should be considered when addressing ASD Teamwork. The ASD Teamwork thematic network is the result of a process that consisted of (i) defining the studies used as a data source through a literature review; (ii) extracting data from these studies; (iii) translating this data into codes; (iv) translating the codes into themes; (v) creating the model of higher-order themes; and, (vi) assessing the trustworthiness of the synthesis. The resulting thematic network comprises four higher-level themes: Cohesion, Orientation, Shared Leadership, and Autonomy. We believe that the constructed thematic network can be generalized to ASD and used as the basis by researchers who intend to explore ASD Teamwork. Further, practitioners can use our results to understand agile teams' dynamics better and improve their efficiency. \textcopyright{} 2021 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Freire2022645,
  type = {Article},
  title = {A Literature-Based Thematic Network to Provide a Comprehensive Understanding of Agile Teamwork (106)},
  author = {Freire, Arthur and Neto, Manuel and Perkusich, Mirko and Costa, Alexandre and Gorg{\'o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  volume = {32},
  number = {5},
  pages = {645--659},
  doi = {10.1142/S0218194022500176},
  abstract = {Agile Software Development (ASD) has become the mainstream software development method of choice. Its core fundamentals are based on Teamwork factors and the higher value of individuals and their interactions over processes and tools. However, there is no common understanding regarding the factors that should be considered for defining an ASD Teamwork construct. Driven by this problem, we present a thematic network that synthesizes the information presented in the literature, and eases knowledge sharing by defining a terminology. The thematic network is the result of the following process: (i) studies definition to be used as data source through a literature review; (ii) data extraction from these studies; (iii) data translation into codes; (iv) codes translation into themes; (v) creation of higher-order themes model; and (vi) assessment of synthesis trustworthiness. The resulting thematic network comprises four higher-order themes: Cohesion, Orientation, Shared Leadership, and Autonomy. We also evaluate the applicability of the identified themes in ASD Teamwork constructs in the literature. We concluded that the constructed thematic network can be generalized to ASD, and used as basis by researchers who intend to explore ASD Teamwork. Further, practitioners can use our results to understand agile teams' dynamics better and improve their efficiency. \textcopyright{} 2022 World Scientific Publishing Company.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Fu2022,
  type = {Article},
  title = {{{GPT2SP}}: {{A}} Transformer-Based Agile Story Point Estimation Approach},
  author = {Fu, Michael and Tantithamthavorn, Chakkrit},
  year = {2022},
  journal = {IEEE Transactions on Software Engineering},
  doi = {10.1109/TSE.2022.3158252},
  abstract = {Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34\%-57\% more accurate than existing baseline approaches for within-project estimations; (2) 39\%-49\% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6\%-47\%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach. IEEE},
  publication_stage = {Article in press},
  source = {Scopus}
}

@article{fu2022:gpt2sp,
  title = {{{GPT2SP}}: {{A}} Transformer-Based Agile Story Point Estimation Approach},
  author = {Fu, Michael and Tantithamthavorn, Chakkrit},
  year = {2022},
  journal = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  issn = {1939-3520},
  doi = {10.1109/TSE.2022.3158252},
  abstract = {Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34\%-57\% more accurate than existing baseline approaches for within-project estimations; (2) 39\%-49\% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6\%-47\%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach.},
  keywords = {Agile Story Point Estimation,AI for SE,Artificial intelligence,Computer architecture,Estimation,Explainable AI,Planning,Task analysis,Training,Transformers},
  file = {/Users/guru/Zotero/storage/Y9IAXFM3/Fu and Tantithamthavorn - 2022 - GPT2SP A transformer-based agile story point esti.pdf}
}

@article{Fu20221825,
  type = {Article},
  title = {Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum},
  author = {Fu, Kaihua and Zhang, Wei and Chen, Quan and Zeng, Deze and Guo, Minyi},
  year = {2022},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {8},
  pages = {1825--1840},
  doi = {10.1109/TPDS.2021.3128037},
  abstract = {User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9\% and the network bandwidth usage by 53.4\%, while achieving the required 99\%-ile latency. \textcopyright{} 1990-2012 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Fulop2013253,
  type = {Conference Paper},
  title = {Semantic Representation for Emotional-Behavioral Systems},
  author = {Fulop, Istvan Marcell and Csapo, Adam and Baranyi, Peter},
  year = {2013},
  series = {{{SoMeT}} 2013 - 12th {{IEEE International Conference}} on {{Intelligent Software Methodologies}}, {{Tools}} and {{Techniques}}, {{Proceedings}}},
  pages = {253--256},
  doi = {10.1109/SoMeT.2013.6645667},
  abstract = {The paradigm of intelligent space integrates the knowledge of several scientific fields. The intelligent space approach can be successfully applied for home care of handicapped and elderly people. The use of semantic information proved to be an efficient method for the knowledge integration of different domains. In this paper, a semantic intelligent space framework is presented which can handle the capabilities and needs of the single intelligent entities in order to realize cooperation for solving complex tasks. This semantic framework is extended with the representation of the further domain of emotional-behavioral systems. It is described, what kind of benefit can be realized based on the results of this domain, then the semantic representation of the domain is explained. It is decribed as well, how the original framework can be extended in order to cope with the representation. \textcopyright{} 2013 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{gaikwad2019:voiceactivated,
  title = {Voice-Activated Solutions for Agile Retrospective Sessions},
  author = {Gaikwad, Purwa Kishor and Jayakumar, Chris Theodore and Tilve, Eashan and Bohra, Niraj and Yu, Wenfei and Spichkova, Maria},
  year = {2019},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 23rd {{International Conference KES2019}}},
  volume = {159},
  pages = {2414--2423},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2019.09.416},
  urldate = {2023-01-15},
  abstract = {Retrospective (retro) sessions are an important part of Agile/Scrum process for software development. In theory, conducting retro sessions each sprint should improve team dynamics and productivity. In praxis, retro sessions also have some disadvantages/hurdles that are hard to overcome: they are non-anonymous and time consuming. The goal of our project, conducted in collaboration between Shine Solutions and RMIT University, is to analyse the existing hurdles from industry-prospective and to provide an AI-based solution to overcome them. The two main outcomes of the project are (1) a web-based solution to support retro-sessions, and (2) a qualitative analysis of two speech recognition tools, Google Home and Amazon Alexa, to be connected with the elaborated solution to populate the retro board and help in time boxing a retro by using voice activated commands.},
  langid = {english},
  keywords = {Agile,Collaboration,Project Management,Software Engineering},
  file = {/Users/guru/Zotero/storage/B4QWRFVV/Gaikwad et al_2019_Voice-activated solutions for agile retrospective sessions_Procedia Computer Science.pdf;/Users/guru/Zotero/storage/C8XY63BM/S1877050919316205.html}
}

@inproceedings{Gaikwad20192414,
  type = {Conference Paper},
  title = {Voice-Activated Solutions for Agile Retrospective Sessions},
  author = {Gaikwad, Purwa Kishor and Jayakumar, Chris Theodore and Tilve, Eashan and Bohra, Niraj and Yu, Wenfei and Spichkova, Maria},
  year = {2019},
  series = {Procedia {{Computer Science}}},
  volume = {159},
  pages = {2414--2423},
  doi = {10.1016/j.procs.2019.09.416},
  abstract = {Retrospective (retro) sessions are an important part of Agile/Scrum process for software development. In theory, conducting retro sessions each sprint should improve team dynamics and productivity. In praxis, retro sessions also have some disadvantages/hurdles that are hard to overcome: they are non-anonymous and time consuming. The goal of our project, conducted in collaboration between Shine Solutions and RMIT University, is to analyse the existing hurdles from industry-prospective and to provide an AI-based solution to overcome them. The two main outcomes of the project are (1) a web-based solution to support retro-sessions, and (2) a qualitative analysis of two speech recognition tools, Google Home and Amazon Alexa, to be connected with the elaborated solution to populate the retro board and help in time boxing a retro by using voice activated commands. \textcopyright{} 2019 The Author(s). Published by Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{garcés2021:three,
  title = {Three Decades of Software Reference Architectures: {{A}} Systematic Mapping Study},
  shorttitle = {Three Decades of Software Reference Architectures},
  author = {Garc{\'e}s, Lina and {Mart{\'i}nez-Fern{\'a}ndez}, Silverio and Oliveira, Lucas and Valle, Pedro and Ayala, Claudia and Franch, Xavier and Nakagawa, Elisa Yumi},
  year = {2021},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {179},
  pages = {111004},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111004},
  urldate = {2023-01-15},
  abstract = {Software reference architectures have played an essential role in software systems development due to the possibility of knowledge reuse. Although increasingly adopted by industry, these architectures are not yet completely understood. This work presents a panorama on existing software reference architectures, characterizing them according to their context, goals, perspectives, application domains, design approaches, and maturity, as well as the industry involvement for their construction. For this, we planned and conducted a systematic mapping study. During last decade, the number of reference architectures in very diverse application domains has increased, resulting from efforts of industry, academia, and through their collaborations. Academic reference architectures are oriented to facilitate the reuse of architectural and domain knowledge. The industry has focused on architectures for standardization with certain maturity level. However, the great amount of architectures studied in this work have been designed without following a systematic process, and they lack the maturity to be used in real software projects. Further investigations can be oriented to gathering empirical evidences, from different sources than academic data libraries, that allow to understand how references architectures have been constructed, utilized, and maintained during the whole software life-cycle.},
  langid = {english},
  keywords = {Reference architecture,Secondary study,Software architecture,Systematic mapping},
  file = {/Users/guru/Zotero/storage/3SU8FVUR/Garcés et al_2021_Three decades of software reference architectures_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/QTCATSEE/S0164121221001011.html}
}

@article{garcia2021:hindsight,
  title = {Hindsight Logging for Model Training},
  author = {Garcia, Rolando and Liu, Eric and Sreekanti, Vikram and Yan, Bobby and Dandamudi, Anusha and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Sen, Koushik},
  year = {2021},
  month = feb,
  journal = {Proceedings of the VLDB Endowment},
  volume = {14},
  number = {4},
  pages = {682--693},
  issn = {2150-8097},
  doi = {10.14778/3436905.3436925},
  urldate = {2023-01-15},
  abstract = {In modern Machine Learning, model training is an iterative, experimental process that can consume enormous computation resources and developer time. To aid in that process, experienced model developers log and visualize program variables during training runs. Exhaustive logging of all variables is infeasible, so developers are left to choose between slowing down training via extensive conservative logging, or letting training run fast via minimalist optimistic logging that may omit key information. As a compromise, optimistic logging can be accompanied by program checkpoints; this allows developers to add log statements post-hoc, and "replay" desired log statements from checkpoint---a process we refer to as hindsight logging. Unfortunately, hindsight logging raises tricky problems in data management and software engineering. Done poorly, hindsight logging can waste resources and generate technical debt embodied in multiple variants of training code. In this paper, we present methodologies for efficient and effective logging practices for model training, with a focus on techniques for hindsight logging. Our goal is for experienced model developers to learn and adopt these practices. To make this easier, we provide an open-source suite of tools for Fast Low-Overhead Recovery (flor) that embodies our design across three tasks: (i) efficient background logging in Python, (ii) adaptive periodic checkpointing, and (iii) an instrumentation library that codifies hindsight logging for efficient and automatic record-replay of model-training. Model developers can use each flor tool separately as they see fit, or they can use flor in hands-free mode, entrusting it to instrument their code end-to-end for efficient record-replay. Our solutions leverage techniques from physiological transaction logs and recovery in database systems. Evaluations on modern ML benchmarks demonstrate that flor can produce fast checkpointing with small user-specifiable overheads (e.g. 7\%), and still provide hindsight log replay times orders of magnitude faster than restarting training from scratch.},
  file = {/Users/guru/Zotero/storage/BBYQJNWG/Garcia et al_2021_Hindsight logging for model training_Proceedings of the VLDB Endowment.pdf}
}

@article{gasser2017layered,
  title = {A Layered Model for {{AI}} Governance},
  author = {Gasser, Urs and Almeida, Virgilio AF},
  year = {2017},
  journal = {IEEE Internet Computing},
  volume = {21},
  number = {6},
  pages = {58--62},
  publisher = {{IEEE}}
}

@article{ghobakhloo2020:industry,
  title = {Industry 4.0, Digitization, and Opportunities for Sustainability},
  author = {Ghobakhloo, Morteza},
  year = {2020},
  month = apr,
  journal = {Journal of Cleaner Production},
  volume = {252},
  pages = {119869},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2019.119869},
  urldate = {2023-01-15},
  abstract = {The fourth industrial revolution and the underlying digital transformation, known as Industry 4.0, is progressing exponentially. The digital revolution is reshaping the way individuals live and work fundamentally, and the public remains optimistic regarding the opportunities Industry 4.0 may offer for sustainability. The present study contributes to the sustainability literature by systematically identifying the sustainability functions of Industry 4.0. In doing so, the study first reviews the fundamental design principles and technology trends of Industry 4.0 and introduces the architectural design of Industry 4.0. The study further draws on the interpretive structural modelling technique to model the contextual relationships among the Industry 4.0 sustainability functions. Results indicate that sophisticated precedence relationships exist among various sustainability functions of Industry 4.0. `Matrice d'Impacts Crois\'es Multiplication Appliqu\'ee \`aun Classement' (MICMAC) analysis reveals that economic sustainability functions such as production efficiency and business model innovation tend to be the more immediate outcome of Industry 4.0, which pays the way for development of more remote socioenvironmental sustainability functions of Industry 4.0 such as energy sustainability, harmful emission reduction, and social welfare improvement. This study can serve Industry 4.0 stakeholders \textendash{} leaders in the public and private sectors, industrialists, and academicians \textendash{} to better understand the opportunities that the digital revolution may offer for sustainability, and work together more closely to ensure that Industry 4.0 delivers the intended sustainability functions around the world as effectively, equally, and fairly as possible.},
  langid = {english},
  keywords = {Digitization,Environmentalism,Industrial internet,Industry 4.0,Smart manufacturing,Sustainability},
  file = {/Users/guru/Zotero/storage/4YBNBGN7/S0959652619347390.html}
}

@article{gill2019:transformative,
  title = {Transformative Effects of {{IoT}}, {{Blockchain}} and {{Artificial Intelligence}} on Cloud Computing: {{Evolution}}, Vision, Trends and Open Challenges},
  shorttitle = {Transformative Effects of {{IoT}}, {{Blockchain}} and {{Artificial Intelligence}} on Cloud Computing},
  author = {Gill, Sukhpal Singh and Tuli, Shreshth and Xu, Minxian and Singh, Inderpreet and Singh, Karan Vijay and Lindsay, Dominic and Tuli, Shikhar and Smirnova, Daria and Singh, Manmeet and Jain, Udit and Pervaiz, Haris and Sehgal, Bhanu and Kaila, Sukhwinder Singh and Misra, Sanjay and Aslanpour, Mohammad Sadegh and Mehta, Harshit and Stankovski, Vlado and Garraghan, Peter},
  year = {2019},
  month = dec,
  journal = {Internet of Things},
  volume = {8},
  pages = {100118},
  issn = {2542-6605},
  doi = {10.1016/j.iot.2019.100118},
  urldate = {2023-01-15},
  abstract = {Cloud computing plays a critical role in modern society and enables a range of applications from infrastructure to social media. Such system must cope with varying load and evolving usage reflecting societies' interaction and dependency on automated computing systems whilst satisfying Quality of Service (QoS) guarantees. Enabling these systems are a cohort of conceptual technologies, synthesized to meet demand of evolving computing applications. In order to understand current and future challenges of such system, there is a need to identify key technologies enabling future applications. In this study, we aim to explore how three emerging paradigms (Blockchain, IoT and Artificial Intelligence) will influence future cloud computing systems. Further, we identify several technologies driving these paradigms and invite international experts to discuss the current status and future directions of cloud computing. Finally, we proposed a conceptual model for cloud futurology to explore the influence of emerging paradigms and technologies on evolution of cloud computing.},
  langid = {english},
  keywords = {Artificial Intelligence,Blockchain,Cloud applications,Cloud computing,Cloud paradigms and technologies,IoT,Quality of Service},
  file = {/Users/guru/Zotero/storage/XW4RJNT8/S2542660519302331.html}
}

@article{gill2022:ai,
  title = {{{AI}} for next Generation Computing: {{Emerging}} Trends and Future Directions},
  shorttitle = {{{AI}} for next Generation Computing},
  author = {Gill, Sukhpal Singh and Xu, Minxian and Ottaviani, Carlo and Patros, Panos and Bahsoon, Rami and Shaghaghi, Arash and Golec, Muhammed and Stankovski, Vlado and Wu, Huaming and Abraham, Ajith and Singh, Manmeet and Mehta, Harshit and Ghosh, Soumya K. and Baker, Thar and Parlikad, Ajith Kumar and Lutfiyya, Hanan and Kanhere, Salil S. and Sakellariou, Rizos and Dustdar, Schahram and Rana, Omer and Brandic, Ivona and Uhlig, Steve},
  year = {2022},
  month = aug,
  journal = {Internet of Things},
  volume = {19},
  pages = {100514},
  issn = {2542-6605},
  doi = {10.1016/j.iot.2022.100514},
  urldate = {2023-01-15},
  abstract = {Autonomic computing investigates how systems can achieve (user) specified ``control'' outcomes on their own, without the intervention of a human operator. Autonomic computing fundamentals have been substantially influenced by those of control theory for closed and open-loop systems. In practice, complex systems may exhibit a number of concurrent and inter-dependent control loops. Despite research into autonomic models for managing computer resources, ranging from individual resources (e.g., web servers) to a resource ensemble (e.g., multiple resources within a data centre), research into integrating Artificial Intelligence (AI) and Machine Learning (ML) to improve resource autonomy and performance at scale continues to be a fundamental challenge. The integration of AI/ML to achieve such autonomic and self-management of systems can be achieved at different levels of granularity, from full to human-in-the-loop automation. In this article, leading academics, researchers, practitioners, engineers, and scientists in the fields of cloud computing, AI/ML, and quantum computing join to discuss current research and potential future directions for these fields. Further, we discuss challenges and opportunities for leveraging AI and ML in next generation computing for emerging computing paradigms, including cloud, fog, edge, serverless and quantum computing environments.},
  langid = {english},
  keywords = {Artificial intelligence,Cloud computing,Edge computing,Fog computing,Machine learning,Next generation computing,Quantum computing,Serverless computing},
  file = {/Users/guru/Zotero/storage/Y53Y2UC7/S254266052200018X.html}
}

@article{Giordano2022,
  type = {Article},
  title = {On the Use of Artificial Intelligence to Deal with Privacy in {{IoT}} Systems: {{A}} Systematic Literature Review},
  author = {Giordano, Giammaria and Palomba, Fabio and Ferrucci, Filomena},
  year = {2022},
  journal = {Journal of Systems and Software},
  volume = {193},
  doi = {10.1016/j.jss.2022.111475},
  abstract = {The Internet of Things (IoT) refers to a network of Internet-enabled devices that can make different operations, like sensing, communicating, and reacting to changes arising in the surrounding environment. Nowadays, the number of IoT devices is already higher than the world population. These devices operate by exchanging data between them, sometimes through an intermediate cloud infrastructure, and may be used to enable a wide variety of novel services that can potentially improve the quality of life of billions of people. Nonetheless, all that glitters is not gold: the increasing adoption of IoT comes with several privacy concerns due to the lack or loss of control over the sensitive data exchanged by these devices. This represents a key challenge for software engineering researchers attempting to address those privacy concerns by proposing (semi-)automated solutions to identify sources of privacy leaks. In this respect, a notable trend is represented by the adoption of smart solutions, that is, the definition of techniques based on artificial intelligence (AI) algorithms. This paper proposes a systematic literature review of the research in smart detection of privacy concerns in IoT devices. Following well-established guidelines, we identify 152 primary studies that we analyze under three main perspectives: (1) What are the privacy concerns addressed with AI-enabled techniques; (2) What are the algorithms employed and how they have been configured/validated; and (3) Which are the domains targeted by these techniques. The key results of the study identified six main tasks targeted through the use of artificial intelligence, like Malware Detection or Network Analysis. Support Vector Machine is the technique most frequently used in literature, however in many cases researchers do not explicitly indicate the domain where to use artificial intelligence algorithms. We conclude the paper by distilling several lessons learned and implications for software engineering researchers. \textcopyright{} 2022},
  publication_stage = {Final},
  source = {Scopus}
}

@article{giray2021:software,
  title = {A Software Engineering Perspective on Engineering Machine Learning Systems: {{State}} of the Art and Challenges},
  shorttitle = {A Software Engineering Perspective on Engineering Machine Learning Systems},
  author = {Giray, G{\"o}rkem},
  year = {2021},
  month = oct,
  journal = {Journal of Systems and Software},
  volume = {180},
  pages = {111031},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111031},
  urldate = {2023-01-15},
  abstract = {Context: Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems. Method: I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions. Conclusion: The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.},
  langid = {english},
  keywords = {Deep learning,Machine learning,Software development,Software engineering,Software process,Systematic literature review},
  file = {/Users/guru/Zotero/storage/8I9UJS3G/S016412122100128X.html}
}

@article{giret2016:engineering,
  title = {An Engineering Framework for {{Service-Oriented Intelligent Manufacturing Systems}}},
  author = {Giret, Adriana and Garcia, Emilia and Botti, Vicente},
  year = {2016},
  month = sep,
  journal = {Computers in Industry},
  series = {Emerging {{ICT}} Concepts for Smart, Safe and Sustainable Industrial Systems},
  volume = {81},
  pages = {116--127},
  issn = {0166-3615},
  doi = {10.1016/j.compind.2016.02.002},
  urldate = {2023-01-15},
  abstract = {Nowadays fully integrated enterprises are being replaced by business networks in which each participant provides others with specialized services. As a result, the Service Oriented Manufacturing Systems emerges. These systems are complex and hard to engineer. The main source of complexity is the number of different technologies, standards, functions, protocols, and execution environments that must be integrated in order to realize them. This paper proposes a framework and associated engineering approach for assisting the system developers of Service Oriented Manufacturing Systems. The approach combines multi-agent system with Service Oriented Architectures for the development of intelligent automation control and execution of manufacturing systems.},
  langid = {english},
  keywords = {Multi-agent system,Service Oriented Architectures,Service Oriented Intelligent Manufacturing Systems,Software engineering method},
  file = {/Users/guru/Zotero/storage/D7UL4FZB/S0166361516300045.html}
}

@inproceedings{Gomes2020311,
  type = {Conference Paper},
  title = {Evaluating the Relationship of Personality and Teamwork Quality in the Context of Agile Software Development},
  author = {Gomes, Alexandre and Silva, Manuel and Valadares, Dalton C{\'e}zane Gomes and Perkusich, Mirko and Albuquerque, Danyllo and Almeida, Hyggo and Perkusich, Angelo},
  year = {2020},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {PartF162440},
  pages = {311--316},
  doi = {10.18293/SEKE2020-158},
  abstract = {The software industry is increasingly adopting agile software development (ASD). A characteristic of ASD is of focusing on people over processes. Given this, the literature presents models to evaluate teamwork quality for agile teams. Another perspective is to predict the team's behavior, given the members' personality. This study aims to evaluate the effect of the personality of a team on its teamwork quality. For this purpose, we executed an empirical study collecting data from 38 subjects from five software teams, using a psychometric and a teamwork quality instrument presented in the literature. We triangulated the data from both instruments to check their agreement through correlation analysis. As a result, the soft skills expected given the psychometric instrument were observed given the metrics presented in the teamwork quality instrument, evidencing the impact of the team's personality on its efficiency. Moreover, we observed that the personality of the project manager has a direct impact on the behavior of the team. The results presented herein show that personality instruments might be used to predict the team's behavior having several applications such as assisting in forming teams. \textcopyright{} 2020 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{gopalakrishna2022:if,
  title = {``{{If}} Security Is Required'': {{Engineering}} and Security Practices for Machine Learning-Based {{IoT}} Devices},
  booktitle = {2022 {{IEEE}}/{{ACM}} 4th International Workshop on Software Engineering Research and Practices for the {{IoT}} ({{SERP4IoT}})},
  author = {Gopalakrishna, Nikhil Krishna and Anandayuvaraj, Dharun and Detti, Annan and Bland, Forrest Lee and Rahaman, Sazzadur and Davis, James C.},
  year = {2022},
  month = may,
  pages = {1--8},
  doi = {10.1145/3528227.3528565},
  abstract = {The latest generation of IoT systems incorporate machine learning (ML) technologies on edge devices. This introduces new engineering challenges to bring ML onto resource-constrained hardware, and complications for ensuring system security and privacy. Existing research prescribes iterative processes for machine learning enabled IoT products to ease development and increase product success. However, these processes mostly focus on existing practices used in other generic software development areas and are not specialized for the purpose of machine learning or IoT devices. This research seeks to characterize engineering processes and security practices for ML-enabled IoT systems through the lens of the engineering lifecycle. We collected data from practitioners through a survey (N=25) and interviews (N=4). We found that security processes and engineering methods vary by company. Respondents emphasized the engineering cost of security analysis and threat modeling, and trade-offs with business needs. Engineers reduce their security investment if it is not an explicit requirement. The threats of IP theft and reverse engineering were a consistent concern among practitioners when deploying ML for IoT devices. Based on our findings, we recommend further research into understanding engineering cost, compliance, and security trade-offs.},
  keywords = {Computer security,Costs,Cyber-Physical Systems,Embedded Systems,Internet of Things,Interviews,Machine learning,Machine Learning,Prototypes,Reverse engineering,Security and Privacy,Software Engineering}
}

@article{Granlund2021,
  type = {Article},
  title = {Towards Regulatory-Compliant {{MLOps}}: {{Oravizio}}'s Journey from a Machine Learning Experiment to a Deployed Certified Medical Product},
  author = {Granlund, Tuomas and Stirbu, Vlad and Mikkonen, Tommi},
  year = {2021},
  journal = {SN Computer Science},
  volume = {2},
  number = {5},
  doi = {10.1007/s42979-021-00726-1},
  abstract = {Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context. \textcopyright{} 2021, The Author(s).},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/HKDEPMTL/s42979-021-00726-1.pdf}
}

@incollection{grosan2011rule,
  title = {Rule-Based Expert Systems},
  booktitle = {Intelligent Systems},
  author = {Grosan, Crina and Abraham, Ajith},
  year = {2011},
  pages = {149--185},
  publisher = {{Springer}}
}

@article{Guan2022,
  type = {Article},
  title = {Mobile Learning Platform in Cloud Computing with Information Security and Android System},
  author = {Guan, Dejun and Su, Jian},
  year = {2022},
  journal = {Security and Communication Networks},
  volume = {2022},
  doi = {10.1155/2022/5491411},
  abstract = {In order to meet the real-time communication between teachers and students and improve students' interest and efficiency in learning, this paper designs a mobile learning communication system based on information security and Android system with cloud computing. By using the method of combining database records and web logs to mine the user's browsing records and behaviors, these implicit user behaviors are transformed into explicit user evaluations of the project. Then, the cosine similarity calculation method is used to calculate users and the similarity between the users. The users are clustered by the K-means clustering method, so that the users are automatically divided into several user clusters according to their behaviors. Finally, the user's nearest neighbor score is used to predict the pair. Based on the above method, a mobile learning communication system based on Android is realized, and the system basically meets the functional needs of users. The development of this system not only promotes the mutual communication between students but also facilitates the students' learning, which has a certain promoting effect on the improvement of their academic performance. Copyright \textcopyright{} 2022 Dejun Guan.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{gudivada2017data,
  title = {Data Quality Considerations for Big Data and Machine Learning: {{Going}} beyond Data Cleaning and Transformations},
  author = {Gudivada, Venkat and Apon, Amy and Ding, Junhua},
  year = {2017},
  journal = {International Journal on Advances in Software},
  volume = {10},
  number = {1},
  pages = {1--20}
}

@inproceedings{Gui20201701,
  type = {Conference Paper},
  title = {{{APTrace}}: {{A}} Responsive System for Agile Enterprise Level Causality Analysis},
  author = {Gui, Jiaping and Li, Ding and Chen, Zhengzhang and Rhee, Junghwan and Xiao, Xusheng and Zhang, Mu and Jee, Kangkook and Li, Zhichun and Chen, Haifeng},
  year = {2020},
  series = {Proceedings - {{International Conference}} on {{Data Engineering}}},
  volume = {2020-April},
  pages = {1701--1712},
  doi = {10.1109/ICDE48307.2020.00151},
  abstract = {While backtracking analysis has been successful in assisting the investigation of complex security attacks, it faces a critical dependency explosion problem. To address this problem, security analysts currently need to tune backtracking analysis manually with different case-specific heuristics. However, existing systems fail to fulfill two important system requirements to achieve effective backtracking analysis. First, there need flexible abstractions to express various types of heuristics. Second, the system needs to be responsive in providing updates so that the progress of backtracking analysis can be frequently inspected, which typically involves multiple rounds of manual tuning. In this paper, we propose a novel system, APTrace, to meet both of the above requirements. As we demonstrate in the evaluation, security analysts can effectively express heuristics to reduce more than 99.5\% of irrelevant events in the backtracking analysis of real-world attack cases. To improve the responsiveness of backtracking analysis, we present a novel execution-window partitioning algorithm that significantly reduces the waiting time between two consecutive updates (especially, 57 times reduction for the top 1\% waiting time). \textcopyright{} 2020 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Guimarães2021229,
  type = {Conference Paper},
  title = {A Comparative Study of Psychometric Instruments in Software Engineering},
  author = {Guimar{\~a}es, G. and Perkusich, M. and Albuquerque, D. and Guimar{\~a}es, E.N. and Almeida, H. and Santos, D. and Perkusich, A.},
  year = {2021},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2021-July},
  pages = {229--234},
  doi = {10.18293/SEKE2021-108},
  abstract = {Over the years, researchers have explored the influence of human factors in software engineering, showing that the team members' personalities might affect teamwork. However, it is challenging to measure software engineers' personalities due to the number of available psychometric instruments and the possibility of using different scales and classifications. Our study compares the personality traits measured by three psychometric instruments used in Software Engineering: Big Five Inventory (BFI), 16 Personality Factors (16PF), and Context Cards (CC). For this purpose, we executed an empirical study in which we collected data from 29 software developers for each of the evaluated instruments. As a result, we identified a moderate correlation between BFI and 16PF, confirming the current state-of-the-art. For the remaining combinations, there was a weak correlation. As implications for this research, there is a need to empirically evaluate BFI and CC (context-specific survey) in terms of construct validity since they have moderate to low correlation. \textcopyright{} 2021 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{guo2022:cooperative,
  title = {Cooperative {{Communication Resource Allocation Strategies}} for {{5G}} and {{Beyond Networks}}: {{A Review}} of {{Architecture}}, {{Challenges}} and {{Opportunities}}},
  shorttitle = {Cooperative {{Communication Resource Allocation Strategies}} for {{5G}} and {{Beyond Networks}}},
  author = {Guo, Wanying and Qureshi, Nawab Muhammad Faseeh and Siddiqui, Isma Farah and Shin, Dong Ryeol},
  year = {2022},
  month = nov,
  journal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {34},
  number = {10, Part A},
  pages = {8054--8078},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2022.07.019},
  urldate = {2023-01-15},
  abstract = {The fifth-generation mobile network (5G) supports Internet of Things (IoT) devices and processes large-scale data volumes through mobile devices. With this facility, we find a novel concept of Cooperative communication that manages massive channels accessibility, heterogeneous networks, complex interference environments and high energy consumption mediums through high signal coverage and capacity among mobile devices. The core of cooperative communication system relies on resource allocation techniques that achieves robust interference management, resource scheduling and user matching. To this extend, we find several strategies that discuss cooperative communication allocation techniques from various technological aspects. This review paper compiles all such strategies and discusses cooperative communication resource allocation techniques in a broader scope. The review is designed in such a systematic way that, at first, we classify cooperative communication process according to the number of relay nodes, signal forwarding mode, and transceiver diversity gain. After that, we discuss the core technologies of cooperative communication that includes channel multiplexing, relay selection, power allocation. Followed by that, we discuss the network model of the 5G cooperative communication system having spectrum sharing, new antenna technology, and NOMA along with the several application case studies. In addition to that, we also brief about the resource allocation algorithms of the 5G cooperative communication system from both the certain and uncertain channel states. Finally, we conclude the review discussing the current applied architecture of 5G cooperative communication resource allocation along with challenges, opportunities and open problems.},
  langid = {english},
  keywords = {5G cooperative communication,Relay section,Resource allocation}
}

@inproceedings{Gupta20206,
  type = {Conference Paper},
  title = {Challenges in Scaling {{AI-powered}} Distributed Software Product: {{A}} Case Study of a Healthcare Organization},
  author = {Gupta, Rajeev K. and Balaji, B. and Mekanathan, V. and Ferose Khan, J.},
  year = {2020},
  series = {Proceedings - 2020 {{ACM}}/{{IEEE}} 15th {{International Conference}} on {{Global Software Engineering}}, {{ICGSE}} 2020},
  pages = {6--10},
  doi = {10.1145/3372787.3389300},
  abstract = {Artificial intelligence (AI) is transforming care delivery and expanding precision medicine. This paper presents experiences of a 110-person, spread across three countries, involves multiple business units and external suppliers that successfully achieved multiple milestones. The product is an organization visionary software system, a mission-critical software system that conforms to stringent healthcare regulatory standards. We are practicing three styles of coordination practices that brings a solution to communication challenges. We are also describing our experiences of SAFe practices, Spotify like team culture, and 'Psychological Safety' that that helps in time-critical situations. The authors bring our experiences as a Program Manager, Project Manager, Quality Manager, and chief Architect who has been an integral part of the journey and establishing these practices over since the incubation stage of the referred product. These practices have helped to an extent where we have achieved regulatory acceptance and milestones successfully within aggressive time and taking steady steps towards where other business units are adopting our practices for managing multiple healthcare software systems. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/NEWW3Z2L/3372787.3389300.pdf}
}

@article{gutierrez2021:systematic,
  title = {Systematic Fuzz Testing Techniques on a Nanosatellite Flight Software for Agile Mission Development},
  author = {Gutierrez, Tamara and Bergel, Alexandre and Gonzalez, Carlos E. and Rojas, Camilo J. and Diaz, Marcos A.},
  year = {2021},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {9},
  pages = {114008--114021},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3104283},
  abstract = {The success of CubeSat space missions depends on the ability to perform properly in a harsh environment. A key component in space missions is the flight software, which manages all of the processes executed by the satellite on its onboard computer. Literature shows that CubeSat missions suffer high infant mortality, and many spacecraft failures are related to flight software errors, some of them resulting in complete mission loss. Extensive operation testing is the primary technique used by CubeSats developers to ensure flight software quality and avoid such failures. The ``New Space'' requirements pressure to add ``agility'' to the software development, which could limit the capacity to test. While advanced and beneficial software testing techniques are found in the software engineering field, CubeSat software solutions mostly rely on unit testing, software in the loop simulation, and hardware in the loop simulation. In this work, fuzz testing techniques were developed, implemented, and evaluated as a manner to expedite operational testing of CubeSats while maintaining their completeness. The impact of the tools was evaluated by using the three new 3U CubeSats under development at the University of Chile. We identified twelve bugs not covered by classic testing strategies in less than three days. These failures were reported, fixed, and characterized by the developers in eight sprint sessions. Our results indicate that fuzz testing improved the completeness of flight software testing through automation and with almost no development interruption. Although our approach has been tested on the SUCHAI flight software, it applies to systems that follow a similar architecture.},
  keywords = {CubeSat,embedded software,flight software,fuzz testing,Fuzzing,Hardware,nanosatellites,open source,primary,Software,software quality,Space missions,Space vehicles,testing,Testing},
  file = {/Users/guru/Zotero/storage/6RBQYDHE/Gutierrez et al_2021_Systematic fuzz testing techniques on a nanosatellite flight software for agile_IEEE access practical innovations, open solutions.pdf}
}

@incollection{habash2022:building,
  title = {4 - {{Building}} as a Smart System},
  booktitle = {Sustainability and {{Health}} in {{Intelligent Buildings}}},
  author = {Habash, Riadh},
  editor = {Habash, Riadh},
  year = {2022},
  month = jan,
  series = {Woodhead {{Publishing Series}} in {{Civil}} and {{Structural Engineering}}},
  pages = {95--128},
  publisher = {{Woodhead Publishing}},
  doi = {10.1016/B978-0-323-98826-1.00004-1},
  urldate = {2023-01-15},
  abstract = {The growing calls for connectivity, capability, and agility as determinants of sustainability in the built environment entail a growing need to improve the smart infrastructure particularly in terms of the electrical grid and communication-enabled solutions. Innovation through design is making the electricity industry experiences the same sort of rapid development as the information and communication technology industry has undergone during the past 30 years. Today, electrical utilities have the opportunity of seizing the potential of advanced communication, computing, and control technologies where smart technologies are the existing hub of innovation. To realize this fact, it is essential to integrate smartness and intelligence at all levels of the built environment infrastructure. This chapter explains how utilities and consumers are enabled to pool power from multiple plants and distributed sources to achieve high flexibility and scale by taking advantage of the smart grid capabilities. By integrating energy efficiency, distributed energy generation technologies, and demand flexibility into buildings, the state of the art in grid-interactive efficient buildings can be advanced. Such integration is feasible with the deployment of smart electrical load management strategies supported by sensor and actuator wireless networks that offer potential on the path toward Tactile Internet, Internet of Energy, smart urban mobility, net-zero energy buildings, and ultimately long-term sustainability and health.},
  isbn = {978-0-323-98826-1},
  langid = {english},
  keywords = {Demand response,Grid-interactive efficient buildings,Internet of Energy,Internet-of-Things,Net-zero energy buildings,Smart grid,Smart mobility,Tactile Internet,Virtual power plants,Wireless sensor and actuator networks}
}

@article{Habibullah2023,
  type = {Article},
  title = {Non-Functional Requirements for Machine Learning: Understanding Current Use and Challenges among Practitioners},
  author = {Habibullah, Khan Mohammad and Gay, Gregory and Horkoff, Jennifer},
  year = {2023},
  journal = {Requirements Engineering},
  doi = {10.1007/s00766-022-00395-3},
  abstract = {Systems that rely on Machine Learning (ML systems) have differing demands on quality\textemdash known as non-functional requirements (NFRs)\textemdash from traditional systems. NFRs for ML systems may differ in their definition, measurement, scope, and comparative importance. Despite the importance of NFRs in ensuring the quality ML systems, our understanding of all of these aspects is lacking compared to our understanding of NFRs in traditional domains. We have conducted interviews and a survey to understand how NFRs for ML systems are perceived among practitioners from both industry and academia. We have identified the degree of importance that practitioners place on different NFRs, including cases where practitioners are in agreement or have differences of opinion. We explore how NFRs are defined and measured over different aspects of a ML system (i.e., model, data, or whole system). We also identify challenges associated with NFR definition and measurement. Finally, we explore differences in perspective between practitioners in industry, academia, or a blended context. This knowledge illustrates how NFRs for ML systems are treated in current practice, and helps to guide future RE for ML efforts. \textcopyright{} 2023, The Author(s).},
  publication_stage = {Article in press},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/RRMQG2DA/s00766-022-00395-3.pdf;/Users/guru/Zotero/storage/SLYDZF9Z/Habibullah et al. - 2023 - Non-functional requirements for machine learning .pdf}
}

@inproceedings{Halme2022,
  type = {Conference Paper},
  title = {Ethical User Stories: {{Industrial}} Study},
  author = {Halme, Erika and Agbese, Mamia and Antikainen, Jani and Alanen, Hanna-Kaisa and Jantunen, Marianna and Khan, Arif Ali and Kemell, Kai-Kristian and Vakkuri, Ville and Abrahamsson, Pekka},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3122},
  abstract = {In Port terminals a progressive change is underway in digitalizing traditional systems to SMART systems with the aid of AI. This study follows one of such progressions, the SMARTER project. SMARTER is a sub research and development project of the Sea for Value program of DIMECC company, Finland to create replicable models for digitalization for future terminals which involves the use of AI enabled tools. AI and Autonomous Systems (AS) are the direction that software systems are taking today. But due to ethical challenges involved in the use of AI systems and increased emphasis on ethical practices in the use and design of AI systems, our study provides an ethical angle, Ethical User Stories (EUS). We use an ethically aligned design tool the ECCOLA method to transfer ethical requirements into EUS for non-functional requirements for an aspect of the logistics system, passenger flow. Over the span of six months, 125 EUS using the ECCOLA method were collected through a series of workshops for the passenger flow use case and the findings are revealed in this paper. This project is in the field of maritime industry and concentrates on digitalization of port terminals and this particular paper focuses on the passenger flow. Results are positive towards the practice of Ethical User Stories. \textcopyright{} 2022 Copyright for this paper by its authors},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{halme2022:ethical,
  title = {Ethical {{Tools}}, {{Methods}} and {{Principles}} in {{Software Engineering}} and {{Development}}: {{Case Ethical User Stories}}},
  shorttitle = {Ethical {{Tools}}, {{Methods}} and {{Principles}} in {{Software Engineering}} and {{Development}}},
  booktitle = {Product-{{Focused Software Process Improvement}}},
  author = {Halme, Erika},
  editor = {Taibi, Davide and Kuhrmann, Marco and Mikkonen, Tommi and Kl{\"u}nder, Jil and Abrahamsson, Pekka},
  year = {2022},
  volume = {13709},
  pages = {631--637},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-21388-5_48},
  urldate = {2023-02-27},
  abstract = {The great leap with the development of Artificial Intelligence (AI) and Machine Learning (ML) technology has increased the range of different requirements for software quality, especially in terms of ethics. To implement high-level requirements, like ethical principles, into the workflow of software engineering, new requirements engineer tools are to be developed. Ethical User Stories (EUS) offers a simple way of implementing ethics in software development. This research has investigated the idea of using familiar requirements engineering artifacts, User Stories, to implement ethical principles, into the workflow of software engineering and operationalizing the studied phenomena of EUS. The preliminary results, found through two ongoing empirical studies with a data collection of 600+ EUS, show that EUS is a pressure-free, human-centric and accessible approach to Ethically Aligned Design (EAD) that intertwines with quality characteristics and relieves the developer from the heavy burden of ethical consideration to a smooth workflow of software engineering. An effective EUS is consistent throughout the user story and shares the idea that user-driven ethical motivation generates system functionality or benefits non-functional software design for quality assurance.},
  isbn = {978-3-031-21387-8 978-3-031-21388-5},
  langid = {english},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/SM8C9NKN/Halme - 2022 - Ethical Tools, Methods and Principles in Software .pdf}
}

@incollection{hamilton2021:15,
  title = {15 - {{Envisioning Education}} 4.0\textemdash{{A}} Scenario Planning Approach to Predicting the Future},
  booktitle = {Future {{Directions}} in {{Digital Information}}},
  author = {Hamilton, Martin},
  editor = {Baker, David and Ellis, Lucy},
  year = {2021},
  month = jan,
  series = {Chandos {{Digital Information Review}}},
  pages = {267--283},
  publisher = {{Chandos Publishing}},
  doi = {10.1016/B978-0-12-822144-0.00015-X},
  urldate = {2023-01-15},
  abstract = {What could the future of education look like for libraries, those who run them, and their patrons? In this chapter, we will look at some key trends and how they could play out using a scenario planning approach, and model the potential impact on key actors and our institutions themselves. We will introduce the scenario planning methodology originally developed by Shell in the 1970s and subsequently adopted by groups as diverse as the fashion industry (Forum for the Future, 2019), the International Olympic Committee (IOC, 2020), and the UK's Government Office for Science (GO-Science, 2017). We will specifically address the extent to which the delivery of education could be transformed by Fourth Industrial Revolution technologies\textemdash and what the resulting `Education 4.0' paradigm might look like. Will most universities and colleges as we know them to cease to exist, with everyone having the chance to earn an online qualification from one of the world's most prestigious institutions or study for a widely respected industry-led certification? Or will formal education become even more vital because of the advanced skills required for careers in areas such as biotechnology and artificial intelligence?},
  isbn = {978-0-12-822144-0},
  langid = {english},
  keywords = {Education 4.0,Industry 4.0,Innovation,Scenario planning,Technological change},
  file = {/Users/guru/Zotero/storage/5TY5DX9U/B978012822144000015X.html}
}

@article{han2022:data,
  title = {Towards a Data Science Platform for Improving {{SME}} Collaboration through {{Industry}} 4.0 Technologies},
  author = {Han, Hui and Trimi, Silvana},
  year = {2022},
  month = jan,
  journal = {Technological Forecasting and Social Change},
  volume = {174},
  pages = {121242},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2021.121242},
  urldate = {2023-01-15},
  abstract = {Industry 4.0 (I4.0) is about realizing digital transformation by linking machines to plants, fleets, and humans through sensors and control elements in order to create smart networks, smart factories, smart manufacturing, and smart value chains. By leveraging I4.0 technologies, a small and medium enterprise (SME) can increase its organizational agility, adaptability, and resilience to cope with today's competitive environment by becoming a valuable and innovative partner in the power dynamics with its large buyer counterparts. However, SMEs face technology, trust, and big data challenges when they adopt I4.0 technologies. This study provides new solutions for SMEs to overcome these three challenges in implementing I4.0. Specifically, the paper proposes the following: (1) a roadmap for the application of I4.0 technologies to enhance the collaboration capabilities of SMEs; (2) a structure for I4.0 standardization to develop and sustain trust among partners; and (3) an improved data science platform for systematizing big data to extract critical information for collaboration solutions for SMEs. Additionally, the solutions are evaluated based on an application case of a Greek SME, demonstrating their potentials for practical implementation.},
  langid = {english},
  keywords = {Cloud computing,Collaboration,Data science,Industry 4.0,SME,Trust},
  file = {/Users/guru/Zotero/storage/53I48VNL/S0040162521006752.html}
}

@article{Hanifi2022,
  type = {Article},
  title = {Artificial Intelligence Methods for Improving the Inventive Design Process, Application in Lattice Structure Case Study},
  author = {Hanifi, Masih and Chibane, Hicham and Houssin, Remy and Cavallucci, Denis and Ghannad, Naser},
  year = {2022},
  journal = {Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM},
  volume = {36},
  doi = {10.1017/S0890060422000051},
  abstract = {Nowadays, firms are constantly looking for methodological approaches that help them to decrease the time needed for the innovation process. Among these approaches, it is worth mentioning the TRIZ-based frameworks such as the Inventive Design Methodology (IDM), where the Problem Graph method is used to formulate a problem. However, the application of IDM is time-consuming due to the construction of a complete map to clarify a problem situation. Therefore, the Inverse Problem Graph (IPG) method has been introduced within the IDM framework to enhance its agility. Nevertheless, the manual gathering of essential information, including parameters and concepts, requires effort and time. This paper integrates the neural network doc2vec and machine learning algorithms as Artificial Intelligence methods into a graphical method inspired by the IPG process. This integration can facilitate and accelerate the development of inventive solutions by extracting parameters and concepts in the inventive design process. The method has been applied to develop a new lattice structure solution in the material field. Copyright \textcopyright{} The Author(s), 2022. Published by Cambridge University Press.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Hannousse2021126,
  type = {Article},
  title = {Searching Relevant Papers for Software Engineering Secondary Studies: {{Semantic Scholar}} Coverage and Identification Role},
  author = {Hannousse, Abdelhakim},
  year = {2021},
  journal = {IET Software},
  volume = {15},
  number = {1},
  pages = {126--146},
  doi = {10.1049/sfw2.12011},
  abstract = {Searching relevant papers is a fundamental task for the elaboration of secondary studies. This task is known to be tedious and time-consuming when it is made manually, especially with the presence of several academic repositories. Recently, Semantic Scholar has emerged as a new artificial intelligence-based search engine enabling a set of valuable features. The present study investigates the role of Semantic Scholar in retrieving relevant papers for performing secondary studies in software engineering. For this sake, an examination is performed to check the ability of Semantic Scholar to locate included papers in recent and well-established secondary studies. Afterwards, a hybrid and automatic search strategy is introduced making use of Semantic Scholar as a sole search engine and it incorporates: automatic search, snowballing, and use of Computer Science Ontology (CSO) and Software Engineering Body of Knowledge (SWEBOK) for refining queries. The proposed strategy is validated by replicating the search of high-quality secondary studies in the software engineering field. To guarantee objectivity, a systematic search is conducted of recent secondary studies published in the field since 2015. For the coverage test, Semantic Scholar is examined to locate primary papers of selected secondary studies and identify missing venues. The proposed search strategy is used to check the ability to retrieve primary papers of each secondary study. The systematic search yielded 20 high-quality secondary studies with 1337 distinct primary papers. The coverage test revealed that Semantic Scholar covers 98.88\% of the papers. The proposed search strategy enabled the full replication of 13 studies and more than 90\% for the 7 remaining studies. \textcopyright{} 2021 The Authors. IET Software published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Harel20199770,
  type = {Conference Paper},
  title = {Labor Division with Movable Walls: {{Composing}} Executable Specifications with Machine Learning and Search (Blue Sky Idea)},
  author = {Harel, David and Marron, Assaf and Rosenfeld, Ariel and Vardi, Moshe and Weiss, Gera},
  year = {2019},
  series = {33rd {{AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2019, 31st {{Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2019 and the 9th {{AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2019},
  pages = {9770--9774},
  abstract = {Artificial intelligence (AI) techniques, including, e.g., machine learning, multi-agent collaboration, planning, and heuristic search, are emerging as ever-stronger tools for solving hard problems in real-world applications. Executable specification techniques (ES), including, e.g., Statecharts and scenario-based programming, is a promising development approach, offering intuitiveness, ease of enhancement, compositionality, and amenability to formal analysis. We propose an approach for integrating AI and ES techniques in developing complex intelligent systems, which can greatly simplify agile/spiral development and maintenance processes. The approach calls for automated detection of whether certain goals and sub-goals are met; a clear division between sub-goals solved with AI and those solved with ES; compositional and incremental addition of AI-based or ES-based components, each focusing on a particular gap between a current capability and a well-stated goal; and, iterative refinement of sub-goals solved with AI into smaller sub-sub-goals where some are solved with ES, and some with AI. We describe the principles of the approach and its advantages, as well as key challenges and suggestions for how to tackle them. \textcopyright{} 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{harman2012role,
  title = {The Role of Artificial Intelligence in Software Engineering},
  booktitle = {2012 First International Workshop on Realizing {{AI}} Synergies in Software Engineering ({{RAISE}})},
  author = {Harman, Mark},
  year = {2012},
  pages = {1--6},
  organization = {{IEEE}}
}

@inproceedings{Hartikainen2022,
  type = {Conference Paper},
  title = {Human-Centered {{AI}} Design in Reality: {{A}} Study of Developer Companies' Practices {{A}} Study of Developer Companies' Practices},
  author = {Hartikainen, Maria and V{\"a}{\"a}n{\"a}nen, Kaisa and Lehti{\"o}, Anu and {Ala-Luopa}, Saara and Olsson, Thomas},
  year = {2022},
  series = {{{ACM International Conference Proceeding Series}}},
  doi = {10.1145/3546155.3546677},
  abstract = {Human-Centered AI (HCAI) advocates the development of AI applications that are trustworthy, usable, and based on human needs. While the conceptual foundations of HCAI are extensively discussed in recent literature, the industry practices and methods appear to lag behind. To advance HCAI method development, current practices of AI developer companies need to be understood. To understand how HCAI principles manifest in the current practices of AI development, we conducted an interview study of practitioners from 12 AI developer companies in Finland, focusing on the early stages of AI application development. Our thematic analysis reveals current development practices and identifies four main challenges: (i) detachment of HCAI work from technical development, (ii) clients' central role as the source of user requirements, (iii) uncertain nature of AI, and (iv) lack of value-based understanding of AI. The findings inform the development of HCAI methods and implementation of HCAI principles in AI application development. \textcopyright{} 2022 Owner/Author.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/BY9PNGBW/Hartikainen et al. - 2022 - Human-centered AI design in reality A study of de.pdf;/Users/guru/Zotero/storage/MP6MQUZ8/3546155.3546677.pdf}
}

@article{Hasterok2022777,
  type = {Article},
  title = {{{PAISE}}\textregistered - Das Vorgehensmodell F\"ur {{KI-Engineering}}; [{{PAISE}}\textregistered - Process Model for {{AI}} Systems Engineering]},
  author = {Hasterok, Constanze and Stompe, Janina},
  year = {2022},
  journal = {At-Automatisierungstechnik},
  volume = {70},
  number = {9},
  pages = {777--786},
  doi = {10.1515/auto-2022-0020},
  abstract = {The application of artificial-intelligence-(AI)-based methods within the context of complex systems poses new challenges within the product life cycle. The process model for AI systems engineering, PAISE\textregistered, addresses these challenges by combining approaches from the disciplines of systems engineering, software development and data science. The general approach builds on a component-wise development of the overall system including an AI component. This allows domain specific development processes to be parallelized. At the same time, component dependencies are tested within interdisciplinary checkpoints, thus resulting in a refinement of component specifications. \textcopyright{} 2022 the author(s), published by De Gruyter.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{hauschild2022:guideline,
  title = {Guideline for Software Life Cycle in Health Informatics},
  author = {Hauschild, Anne-Christin and Martin, Roman and Holst, Sabrina Celine and Wienbeck, Joachim and Heider, Dominik},
  year = {2022},
  month = dec,
  journal = {iScience},
  volume = {25},
  number = {12},
  pages = {105534},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2022.105534},
  urldate = {2023-01-15},
  abstract = {The long-lasting trend of medical informatics is to adapt novel technologies in the medical context. In particular, incorporating artificial intelligence to support clinical decision-making can significantly improve monitoring, diagnostics, and prognostics for the patient's and medic's sake. However, obstacles hinder a timely technology transfer from research to the clinic. Due to the pressure for novelty in the research context, projects rarely implement quality standards. Here, we propose a guideline for academic software life cycle processes tailored to the needs and capabilities of research organizations. While the complete implementation of a software life cycle according to commercial standards is not feasible in scientific work, we propose a subset of elements that we are convinced will provide a significant benefit while keeping the effort within a feasible range. Ultimately, the emerging quality checks for academic software development can pave the way for an accelerated deployment of academic advances in clinical practice.},
  langid = {english},
  keywords = {Bioinformatics,Health informatics,Software engineering},
  file = {/Users/guru/Zotero/storage/QVXGWF77/Hauschild et al_2022_Guideline for software life cycle in health informatics_iScience.pdf;/Users/guru/Zotero/storage/UF5YK39H/S2589004222018065.html}
}

@inproceedings{Hayrapetian2018,
  type = {Conference Paper},
  title = {Empirically Analyzing and Evaluating Security Features in Software Requirements},
  author = {Hayrapetian, Allenoush and Raje, Rajeev},
  year = {2018},
  series = {{{ACM International Conference Proceeding Series}}},
  doi = {10.1145/3172871.3172879},
  abstract = {Software requirements, for complex projects, often contain specifications of non-functional attributes (e.g., security-related features). The process of analyzing such requirements for compliance is laborious and error prone. Due to the inherent free-flowing nature of software requirements, it is appealing to apply Natural Language Processing (NLP) and Machine Learning (ML)-based techniques for analyzing these documents. In this paper, we propose a semi-automatic methodology that assesses the security requirements of software systems with respect to completeness and ambiguity, creating a bridge between the requirements documents and being in compliance with standards. Security standards, such as ISO and OWASP, are compared against software project documents for textual entailment relationships. These entailment results along with the document annotations are used to train a Neural Network model to predict whether a given statement in the document is found within the security standard or not. Hence, this approach aims to identify the appropriate structures that underlie software requirements documents. Once such structures are formalized and empirically validated, they will provide guidelines to software organizations for generating comprehensive and unambiguous requirements specification documents as related to security-oriented features. \textcopyright{} 2018 Association for Computing Machinery.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Hehenberger2014188,
  type = {Article},
  title = {Perspectives on Hierarchical Modeling in Mechatronic Design},
  author = {Hehenberger, P.},
  year = {2014},
  journal = {Advanced Engineering Informatics},
  volume = {28},
  number = {3},
  pages = {188--197},
  doi = {10.1016/j.aei.2014.06.005},
  abstract = {Hierarchical modeling helps to describe product models and data from different viewpoints that, representing the different disciplines involved in the design process of mechatronic systems. This paper gives an overview of hierarchical modeling techniques. This includes the investigation of systems, which requires handling different issues that address very specific views of the system (system aspects) and come from various disciplines. Also the model granularity which describes the extent to which an object or model is broken down into smaller elements it an important aspect. The different phases of the product life cycle require models with different objectives and levels of detail. Some models are needed mainly in specific phases of the product life cycle, which are discussed in detail in the paper. Especially in the conceptual design phase some design-characteristic aspects such as hierarchy of parameters, modularity of the design should be analyzed, because in this phase the largest part of the later resulting product costs is predetermined or even fixed. As a consequence, the scope for design is limited to merely small changes in the subsequent design phases. Therefore the interaction between the design phases and the related models plays an important role the development process of mechatronic systems. \textcopyright{} 2014 Elsevier Ltd. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Herwanto2022,
  type = {Conference Paper},
  title = {An Intelligent Systems Approach for Supporting Privacy Awareness in Agile Software Development},
  author = {Herwanto, Guntur Budi},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3122},
  abstract = {Privacy by design principles is an established standard guiding the design and development of privacy-aware systems. Privacy engineering acts as a role to close the gap between the privacy policy and the realization of the system or technology that will be developed. Many privacy engineering methodologies depend heavily on a waterfall-style approach that can be very time-consuming and is not tailored to the speed of agile process, which the majority of the industry is currently taking. In this research, we aim to address those challenges by an intelligent system approach in the form of a natural language processing and recommendation system. As a scientific basis, we use experimental design research to evaluate our intelligent systems that will be integrated in privacy requirements and design context. With this research, we intend to contribute to the advancement of privacy engineering in an agile environment by providing a system that allows better integration of privacy protection with currently used development processes, such as Scrum. \textcopyright{} 2022 Copyright for this paper by its authors},
  publication_stage = {Final},
  source = {Scopus}
}

@article{hewa2021:survey,
  title = {Survey on Blockchain Based Smart Contracts: {{Applications}}, Opportunities and Challenges},
  shorttitle = {Survey on Blockchain Based Smart Contracts},
  author = {Hewa, Tharaka and Ylianttila, Mika and Liyanage, Madhusanka},
  year = {2021},
  month = mar,
  journal = {Journal of Network and Computer Applications},
  volume = {177},
  pages = {102857},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2020.102857},
  urldate = {2023-01-15},
  abstract = {Blockchain is one of the disruptive technical innovation in the recent computing paradigm. Many applications already notoriously hard and complex are fortunate to ameliorate the service with the blessings of blockchain and smart contracts. The decentralized and autonomous execution with in-built transparency of blockchain based smart contracts revolutionize most of the applications with optimum and effective functionality. The paper explores the significant applications which already benefited from the smart contracts. We also highlight the future potential of the blockchain based smart contracts in these applications perspective.},
  langid = {english},
  keywords = {Applications,Blockchain,Corda,DLT,Ethereum,Hyperledger Fabric,Smart contracts,Stellar},
  file = {/Users/guru/Zotero/storage/T5RXEE7H/Hewa et al. - 2021 - Survey on blockchain based smart contracts Applic.pdf;/Users/guru/Zotero/storage/VZTEBGS5/S1084804520303234.html}
}

@article{heymann2022:guideline,
  title = {Guideline for {{Deployment}} of {{Machine Learning Models}} for {{Predictive Quality}} in {{Production}}},
  author = {Heymann, Henrik and Kies, Alexander D. and Frye, Maik and Schmitt, Robert H. and Boza, Andr{\'e}s},
  year = {2022},
  month = jan,
  journal = {Procedia CIRP},
  series = {Leading Manufacturing Systems Transformation \textendash{} {{Proceedings}} of the 55th {{CIRP Conference}} on {{Manufacturing Systems}} 2022},
  volume = {107},
  pages = {815--820},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2022.05.068},
  urldate = {2023-01-15},
  abstract = {Predicting product quality represents a common area of application of machine learning (ML) in manufacturing. However, manifold challenges occur during the integration of ML models into production processes. Therefore, this paper aims to provide a guideline for the deployment of ML models in production environments. Relevant decisions and steps for deploying models in predictive quality use cases are demonstrated. The results for each component of the proposed guideline - deployment design, productionizing \& testing, monitoring, and retraining - have been validated with industry experts including exemplary implementations.},
  langid = {english},
  keywords = {Artificial Intelligence,Deployment,Machine Learning,Manufacturing,Predictive Quality,Production},
  file = {/Users/guru/Zotero/storage/PI3GXPU6/Heymann et al_2022_Guideline for Deployment of Machine Learning Models for Predictive Quality in_Procedia CIRP.pdf;/Users/guru/Zotero/storage/4XTBGP5G/S2212827122003523.html}
}

@article{Hoda20223808,
  type = {Article},
  title = {Socio-Technical Grounded Theory for Software Engineering},
  author = {Hoda, Rashina},
  year = {2022},
  journal = {IEEE Transactions on Software Engineering},
  volume = {48},
  number = {10},
  pages = {3808--3832},
  doi = {10.1109/TSE.2021.3106280},
  abstract = {Grounded Theory (GT), a sociological research method designed to study social phenomena, is increasingly being used to investigate the human and social aspects of software engineering (SE). However, being written by and for sociologists, GT is often challenging for a majority of SE researchers to understand and apply. Additionally, SE researchers attempting ad hoc adaptations of traditional GT guidelines for modern socio-technical (ST) contexts often struggle in the absence of clear and relevant guidelines to do so, resulting in poor quality studies. To overcome these research community challenges and leverage modern research opportunities, this paper presents Socio-Technical Grounded Theory (STGT) designed to ease application and achieve quality outcomes. It defines what exactly is meant by an ST research context and presents the STGT guidelines that expand GT's philosophical foundations, provide increased clarity and flexibility in its methodological steps and procedures, define possible scope and contexts of application, encourage frequent reporting of a variety of interim, preliminary, and mature outcomes, and introduce nuanced evaluation guidelines for different outcomes. It is hoped that the SE research community and related ST disciplines such as computer science, data science, artificial intelligence, information systems, human computer/robot/AI interaction, human-centered emerging technologies (and increasingly other disciplines being transformed by rapid digitalisation and AI-based augmentation), will benefit from applying STGT to conduct quality research studies and systematically produce rich findings and mature theories with confidence. \textcopyright{} 1976-2012 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{hoffmann2022:development,
  title = {Development of a Framework for the Holistic Generation of {{ML-based}} Business Models in Manufacturing},
  author = {Hoffmann, Felix and Lang, Enno and Metternich, Joachim},
  year = {2022},
  month = jan,
  journal = {Procedia CIRP},
  series = {Leading Manufacturing Systems Transformation \textendash{} {{Proceedings}} of the 55th {{CIRP Conference}} on {{Manufacturing Systems}} 2022},
  volume = {107},
  pages = {209--214},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2022.04.035},
  urldate = {2023-01-15},
  abstract = {Analyzing data with the help of Machine Learning (ML) promises to raise significant potentials in all relevant target dimensions and different application fields of industrial production. Through the increasing availability of data in the context of digitalization as well as continuously more powerful and cost-effective possibilities for data processing, the amount of economically viable scenarios for implementing ML-based business models (BMs) in production rises. Despite the emerging data-related possibilities, especially small and medium sized enterprises (SMEs) struggle with identifying reasonable use cases for ML in their own company. This can be ascribed to a lack of knowledge about the necessary elements for ML applications' sustainable implementation and operation. Therefore, this paper aims to develop a framework for the holistic generation of ML-based BMs in manufacturing. At first, characteristics as well as general and specific requirements for ML-based BMs in manufacturing are elaborated. Subsequently, a morphology for the systematic development of ML-based BMs is generated using the insights gained. In a concluding step, the application of the developed concept is validated based on a selected use case.},
  langid = {english},
  keywords = {artificial intelligence,business model,development model,machine tools,manufacturing,production},
  file = {/Users/guru/Zotero/storage/M8W4524Q/Hoffmann et al_2022_Development of a framework for the holistic generation of ML-based business_Procedia CIRP.pdf;/Users/guru/Zotero/storage/I2GKMEAW/S2212827122002517.html}
}

@article{holzinger2022:information,
  title = {Information Fusion as an Integrative Cross-Cutting Enabler to Achieve Robust, Explainable, and Trustworthy Medical Artificial Intelligence},
  author = {Holzinger, Andreas and Dehmer, Matthias and {Emmert-Streib}, Frank and Cucchiara, Rita and Augenstein, Isabelle and Ser, Javier Del and Samek, Wojciech and Jurisica, Igor and {D{\'i}az-Rodr{\'i}guez}, Natalia},
  year = {2022},
  month = mar,
  journal = {Information Fusion},
  volume = {79},
  pages = {263--278},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.10.007},
  urldate = {2023-01-15},
  abstract = {Medical artificial intelligence (AI) systems have been remarkably successful, even outperforming human performance at certain tasks. There is no doubt that AI is important to improve human health in many ways and will disrupt various medical workflows in the future. Using AI to solve problems in medicine beyond the lab, in routine environments, we need to do more than to just improve the performance of existing AI methods. Robust AI solutions must be able to cope with imprecision, missing and incorrect information, and explain both the result and the process of how it was obtained to a medical expert. Using conceptual knowledge as a guiding model of reality can help to develop more robust, explainable, and less biased machine learning models that can ideally learn from less data. Achieving these goals will require an orchestrated effort that combines three complementary Frontier Research Areas: (1) Complex Networks and their Inference, (2) Graph causal models and counterfactuals, and (3) Verification and Explainability methods. The goal of this paper is to describe these three areas from a unified view and to motivate how information fusion in a comprehensive and integrative manner can not only help bring these three areas together, but also have a transformative role by bridging the gap between research and practical applications in the context of future trustworthy medical AI. This makes it imperative to include ethical and legal aspects as a cross-cutting discipline, because all future solutions must not only be ethically responsible, but also legally compliant.},
  langid = {english},
  keywords = {Artificial intelligence,Explainability,Explainable AI,Graph-based machine learning,Information fusion,Medical AI,Neural-symbolic learning and reasoning,Robustness,Trust},
  file = {/Users/guru/Zotero/storage/84SEFEAK/Holzinger et al_2022_Information fusion as an integrative cross-cutting enabler to achieve robust,_Information Fusion.pdf;/Users/guru/Zotero/storage/79UQKHYD/S1566253521002050.html}
}

@article{Horcas2023,
  type = {Article},
  title = {A Modular Metamodel and Refactoring Rules to Achieve Software Product Line Interoperability},
  author = {Horcas, Jose-Miguel and Pinto, M{\'o}nica and Fuentes, Lidia},
  year = {2023},
  journal = {Journal of Systems and Software},
  volume = {197},
  doi = {10.1016/j.jss.2022.111579},
  abstract = {Emergent application domains, such as cyber\textendash physical systems, edge computing or industry 4.0. present a high variability in software and hardware infrastructures. However, no single variability modeling language supports all language extensions required by these application domains (i.e., attributes, group cardinalities, clonables, complex constraints). This limitation is an open challenge that should be tackled by the software engineering field, and specifically by the software product line (SPL) community. A possible solution could be to define a completely new language, but this has a high cost in terms of adoption time and development of new tools. A more viable alternative is the definition of refactoring and specialization rules that allow interoperability between existing variability languages. However, with this approach, these rules cannot be reused across languages because each language uses a different set of modeling concepts and a different concrete syntax. Our approach relies on a modular and extensible metamodel that defines a common abstract syntax for existing variability modeling extensions. We map existing feature modeling languages in the SPL community to our common abstract syntax. Using our abstract syntax, we define refactoring rules at the language construct level that help to achieve interoperability between variability modeling languages. \textcopyright{} 2022 The Author(s)},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Hossen2022240,
  type = {Conference Paper},
  title = {Practical Efficient Microservice Autoscaling with {{QoS}} Assurance},
  author = {Hossen, Md Rajib and Islam, Mohammad A. and Ahmed, Kishwar},
  year = {2022},
  series = {{{HPDC}} 2022 - {{Proceedings}} of the 31st {{International Symposium}} on {{High-Performance Parallel}} and {{Distributed Computing}}},
  pages = {240--252},
  doi = {10.1145/3502181.3531460},
  abstract = {Cloud applications are increasingly moving away from monolithic services to agile microservices-based deployments. However, efficient resource management for microservices poses a significant hurdle due to the sheer number of loosely coupled and interacting components. The interdependencies between various microservices make existing cloud resource autoscaling techniques ineffective. Meanwhile, machine learning (ML) based approaches that try to capture the complex relationships in microservices require extensive training data and cause intentional SLO violations. Moreover, these ML-heavy approaches are slow in adapting to dynamically changing microservice operating environments. In this paper, we propose PEMA (Practical Efficient Microservice Autoscaling), a lightweight microservice resource manager that finds efficient resource allocation through opportunistic resource reduction. PEMA's lightweight design enables novel workload-aware and adaptive resource management. Using three prototype microservice implementations, we show that PEMA can find efficient resource allocation and save up to 33\% resources compared to the commercial rule-based resource allocations. \textcopyright{} 2022 Owner/Author.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Huang2020118,
  type = {Article},
  title = {Verifying {{SysML}} Activity Diagrams Using Formal Transformation to {{Petri}} Nets},
  author = {Huang, Edward and McGinnis, Leon F. and Mitchell, Steven W.},
  year = {2020},
  journal = {Systems Engineering},
  volume = {23},
  number = {1},
  pages = {118--135},
  doi = {10.1002/sys.21524},
  abstract = {The development of contemporary systems is an extremely complex process. One approach to modeling system behavior uses activity diagrams from Unified Modeling Language (UML)/System Modeling Language (SysML), providing a standard object-oriented graphical notation and enhancing reusability. However, UML/SysML activity diagrams do not directly support the kind of analysis needed to verify the system behavior, such as might be available with a Petri net (PN) model. We show that a behavior model represented by a set of fUML-compliant modeling elements in UML/SysML activity diagrams can be transformed into an equivalent PN, so that the analysis capability of PN can be applied. We define a formal mathematical notation for a set of modeling elements in activity diagrams, show the mapping rules between PN and activity diagrams, and propose a formal transformation algorithm. Two example system behavior models represented by UML/SysML activity diagrams are used for illustration. \textcopyright{} 2019 Wiley Periodicals, Inc.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{humphrey1988software,
  title = {The Software Engineering Process: Definition and Scope},
  booktitle = {Proceedings of the 4th International Software Process Workshop on {{Representing}} and Enacting the Software Process},
  author = {Humphrey, Watts S},
  year = {1988},
  pages = {82--83}
}

@book{humphrey1995discipline,
  title = {A Discipline for Software Engineering},
  author = {Humphrey, Watts S},
  year = {1995},
  publisher = {{Pearson Education India}}
}

@article{hussain2019:integration,
  title = {Integration of {{VANET}} and {{5G Security}}: {{A}} Review of Design and Implementation Issues},
  shorttitle = {Integration of {{VANET}} and {{5G Security}}},
  author = {Hussain, Rasheed and Hussain, Fatima and Zeadally, Sherali},
  year = {2019},
  month = dec,
  journal = {Future Generation Computer Systems},
  volume = {101},
  pages = {843--864},
  issn = {0167-739X},
  doi = {10.1016/j.future.2019.07.006},
  urldate = {2023-01-15},
  abstract = {The commercial adaptation of Vehicular Ad hoc NETwork (VANET) to achieve secure Intelligent Transportation System (ITS) heavily depends on the security guarantees for the end-users and consumers. Current VANET security standards address most of the security challenges faced by the vehicular networks. However, with the emergence of 5th Generation (5G) networks, and the demand for a range of new applications and services through vehicular networks, it is imperative to integrate 5G and vehicular networks. To achieve a seamless integration, various design and implementation issues related to 5G and VANETs must be addressed. We focus on the security issues that need to be considered in order to enable the secure integration of 5G and VANETs. More precisely, we conduct in-depth study of the current security issues, solutions, and standards used in vehicular networks and then we identify the security gaps in the existing VANET security solutions. We investigate the security features of 5G networks and discuss how they can be leveraged in vehicular networks to enable a seamless and efficient integration. We also propose a security architecture for vehicular networks wherein the current VANET security standards and 5G security features coexist to support secure VANET applications. Finally, we discuss some future challenges and research directions for 5G-enabled secure vehicular networks.},
  langid = {english},
  keywords = {5G VANET,Architecture,Connected car,Security,VANET applications,VANET security standards},
  file = {/Users/guru/Zotero/storage/BH6JXVIK/Hussain et al_2019_Integration of VANET and 5G Security_Future Generation Computer Systems.pdf;/Users/guru/Zotero/storage/EI3WVXSI/S0167739X19306909.html}
}

@inproceedings{Husson20213303,
  type = {Conference Paper},
  title = {Analysis and Illustration of the Practical Impact of {{Artificial Intelligence}} and {{Intelligent Personal Assistants}} on Business Processes in Small- and Medium-Sized Service Enterprises},
  author = {Husson, Daniel and Holland, Alexander and Fathi, Madjid and Arteaga Sanchez, Rocio},
  year = {2021},
  series = {Conference {{Proceedings}} - {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}},
  pages = {3303--3310},
  doi = {10.1109/SMC52423.2021.9659298},
  abstract = {Artificial intelligence (AI) and intelligent personal assistants (IPAs) are becoming more and more important. This is no longer limited to private use but also becoming increasingly important in everyday business life. The identification of optimization potentials through the use of AI and IPAs is therefore relevant from both a theoretical and a practical point of view. This paper, therefore, identifies concrete use cases for relevant processes in small- and medium sized enterprises (SME) in the service industry and enhances them with AI and IPA capabilities. Based on a prototype, the use cases were presented to 10 experts who were interviewed regarding their usefulness and influencing factors. Subsequently, the results of the interviews were categorized and validated again by a quantitative survey within the expert panel. As a result, the use cases were evaluated with regard to the specific influencing factors and the potential for optimization was determined. The use cases were evaluated based on this data. It was shown that IPA features in particular are perceived as useful. On average, AI and IPA features have a cost savings potential of over 31\%. This shows the importance of these features and the need to consider them when modeling modern business processes. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{hynninen2018software,
  title = {Software Testing: {{Survey}} of the Industry Practices},
  booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics ({{MIPRO}})},
  author = {Hynninen, Timo and Kasurinen, Jussi and Knutas, Antti and Taipale, Ossi},
  year = {2018},
  pages = {1449--1454},
  organization = {{IEEE}}
}

@misc{ibm2023:what,
  title = {What Is {{Software Testing}} and {{How Does}} It {{Work}}? | {{IBM}}},
  shorttitle = {What Is {{Software Testing}} and {{How Does}} It {{Work}}?},
  author = {IBM},
  year = {2023},
  urldate = {2023-03-28},
  abstract = {Software testing is the process of evaluating and verifying that a software product or application does what it's supposed to do.  The benefits of good testing include preventing bugs and improving performance.},
  howpublished = {https://www.ibm.com/topics/software-testing},
  langid = {american},
  file = {/Users/guru/Zotero/storage/4T2U3PKN/software-testing.html}
}

@article{Ibrahim20201054,
  type = {Article},
  title = {Open Systems Science: Digital Transformation and Developing Business Model toward Smart Farms' Platform},
  author = {Ibrahim, Rania E. and Elramly, Amr and Hassan, Hoda M.},
  year = {2020},
  journal = {International Journal of Circuits, Systems and Signal Processing},
  volume = {14},
  pages = {1054--1073},
  doi = {10.46300/9106.2020.14.134},
  abstract = {This paper describes efforts by National Authority for Remote Sensing and Space Sciences (NARSS) to help the Egyptian government to manage and monitor the national projects. We successfully developed a geospatial data sharing portal (NARSSGeoPortal) as part of the government need to build national Decision Support System (DSS). We were able to solve the software development issues as well as the satellite imagery sourcing issues, but the main challenge remains around how to collect complete and correct data from the public about their private businesses nationwide. The most challenging is how to engage the public and encourage the business owners who are the main sources of data to provide the government Geoportal with data about their businesses. It is also challenging to engage the scientists and experts from government research centers into the data sharing Geoportal. Furthermore, it is a challenge to integrate the government research centers with the public businesses' daily operation. The data sharing Geoportal is built for all national projects and government authorities, however, in this paper we focus on the Agriculture authorities and farming businesses where the challenge is how to collect correct and complete data per acre about the seeds, fertilizers, water, pest control and all other farm related data that the satellite imagery does not provide. The goal is to integrate the farms into unified national monitoring, and control system while developing advanced smart farms with the use of Internet of Things (IoT). The proposed collaboration agriculture platform fills the gap between two groups. The first group includes the government authorities, financial institutions, and research centers. The second group includes farmers, supply chain, and agriculture engineers. The platform show how employment can be generated by transforming the national ecosystem. The paper also fills a major gap in industry as well as in academia by providing the first Bluetooth Low Energy computer aided design tool that will facilitate testing, designing, deploying, managing and debugging of real Bluetooth Low Energy networks. \textcopyright{} 2020, North Atlantic University Union NAUN. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@book{ieee1983ieee,
  title = {{{IEEE}} Standard Glossary of Software Engineering Terminology},
  author = {Committee, IEEE Computer Society. Software Engineering Technical},
  year = {1983},
  volume = {729},
  publisher = {{IEEE}}
}

@inproceedings{Iftikhar2021131,
  type = {Conference Paper},
  title = {Adopting Artificial Intelligence in Danish {{SMEs}}: {{Barriers}} to Become a Data Driven Company, Its Solutions and Benefits},
  author = {Iftikhar, Nadeem and Nordbjerg, Finn Ebertsen},
  year = {2021},
  series = {Proceedings of the 2nd {{International Conference}} on {{Innovative Intelligent Industrial Production}} and {{Logistics}}, {{IN4PL}} 2021},
  pages = {131--136},
  doi = {10.5220/0010691800003062},
  abstract = {Artificial intelligence allows small and medium-sized enterprises (SMEs) in the manufacturing sector to improve performance, reduce downtime and increase productivity. SMEs in Denmark are still struggling to implement artificial intelligence based strategies since they face a range of challenges, such as business applications, data availability, organizational culture towards the acceptance of new technologies, investment in new technologies, skills gap, development process and effective strategy. In the beginning, the paper describes the challenges faced by SMEs in adopting artificial intelligence. Then, the paper suggests solutions to overcome these challenges and discusses the importance of artificial intelligence as well as the opportunities it offers to SMEs Copyright \textcopyright{} 2021 by SCITEPRESS \textendash{} Science and Technology Publications, Lda. All rights reserved},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{ijaz2019:nonfunctional,
  title = {Non-Functional Requirements Prioritization: {{A}} Systematic Literature Review},
  booktitle = {2019 45th Euromicro Conference on Software Engineering and Advanced Applications ({{SEAA}})},
  author = {Ijaz, Khush Bakht and Inayat, Irum and Allah Bukhsh, Faiza},
  year = {2019},
  month = aug,
  pages = {379--386},
  doi = {10.1109/SEAA.2019.00064},
  abstract = {Continuous delivery and rapidly changing requirements in agile environments force the developers to put non-functional requirements (NFRs) on halt till maintenance phase. However, neglecting NFRs during prioritization phase may lead to inaccurate estimations for software projects resulting in high maintenance cost and failures. The subjective and uncertain nature of non-functional requirements makes them unfit to be prioritized using conventional prioritization methods. Although the existing literature reports on inadequate consideration given to NFRs prioritization, still no comprehensive systematic effort has been done to report the limitations and evaluation mechanisms of existing NFRs prioritization approaches. Requirements engineering society lacks a broad understanding of NFRs prioritization approaches and the challenges which need to be overcome. Therefore, we aim to investigate (i) the existing NFR prioritization techniques and their validation mechanisms, (ii) the role of Artificial Intelligence (AI) in NFRs prioritization, and (iii) the limitations of existing NFRs prioritization techniques. For this, we reviewed the literature published from 2008 till present and extracted 30 studies. The results reveal twenty-five NFRs prioritization techniques out of which only three are AI based. The major limitations we have come across are that most of the NFRs prioritization techniques are not scalable to large datasets, inter-dependencies between functional requirements (FRs) and NFRs are ignored, and the uncertainties associated with NFRs are not considered at all. However, the literature suggests that AI-based techniques and Fuzzy logic may be used to solve issues such as uncertainties i.e. ambiguities, vagueness, and subjective opinions of stakeholders. This review adds to the existing body of knowledge on NFRs and motivates the practitioners to focus on the NFR prioritization by highlighting the limitations of the existing methods.},
  keywords = {Bibliographies,Data mining,non-functional requirements,Quality assessment,quality attributes,quality requirements,Search problems,Software engineering,systematic literature review,Systematics},
  file = {/Users/guru/Zotero/storage/Y9T89R28/Ijaz et al_2019_Non-functional requirements prioritization_2019 45th euromicro conference on software engineering and advanced applications (SEAA).pdf}
}

@article{iqbal2022:enhanced,
  title = {Enhanced Time-Constraint Aware Tasks Scheduling Mechanism Based on Predictive Optimization for Efficient Load Balancing in Smart Manufacturing},
  author = {Iqbal, Naeem and Khan, Anam-Nawaz and {Imran} and Rizwan, Atif and Qayyum, Faiza and Malik, Sehrish and Ahmad, Rashid and Kim, Do-Hyeun},
  year = {2022},
  month = jul,
  journal = {Journal of Manufacturing Systems},
  volume = {64},
  pages = {19--39},
  issn = {0278-6125},
  doi = {10.1016/j.jmsy.2022.05.015},
  urldate = {2023-01-15},
  abstract = {Smart manufacturing has great potential for developing customized products to meet the dynamic needs of customers and a collaborative network to enhance production efficiency. It is an emerging area with the revolution of business strategies, such as Industry 4.0 and Industrial Internet of the United States of America. IIoT and data-driven technologies, such as AI and ML, have leveraged the production environment by facilitating mass personalized customization of products and improving manufacturing processes collaboratively. However, these technologies are segregated and dispersed in the digitization of manufacturing products and automation of machines in existing manufacturing processes. Therefore, it is a leverage challenge to develop an integrated solution based on IIoT and data-driven technologies for an autonomous manufacturing environment. This research study presents an integrated solution using an enhanced TCA tasks scheduling mechanism based on a predictive optimization approach to improve smart manufacturing production efficiency. The proposed enhanced TCA tasks scheduling mechanism is an improved variant of FEF scheduling that considers accurate decision (prediction) measures and tasks' minimal (optimal) time to schedule tasks efficiently. This study aims to efficiently plan task execution sequence to increase smart manufacturing productions and efficiency of resource utilization in real-time by maximizing utilization of smart machines, minimizing tasks idle time, and autonomously controlling the smart manufacturing environment through installed sensors and actuators. Furthermore, different evaluation methods are used to analyze the significance of the proposed PO-TCA scheduling mechanism, such as response time, tasks drop rate, tasks starvation rate, and machine utilization rate. The experimental analysis shows that the proposed enhanced PO-TCA scheduling mechanism reduces dropout and starvation rates by 21~\% and 17~\%, respectively. Our proposed mechanism also improves machine utilization by an average of 18~\% compared to the baseline scheduling strategy. Moreover, experimental results signify that the proposed PO-TCA scheduling mechanism improves the utilization of smart machines and minimizes the task's idle time to achieve a trade-off between tasks response and waiting time.},
  langid = {english},
  keywords = {Industry 4.0,Optimization,Predictive optimization,Real-time Tasks Scheduling,Smart manufacturing,Tasks scheduling},
  file = {/Users/guru/Zotero/storage/XYPYCQ37/S0278612522000851.html}
}

@article{irshad2022:supporting,
  title = {Supporting Refactoring of {{BDD}} Specifications\textemdash{{An}} Empirical Study},
  author = {Irshad, Mohsin and B{\"o}rstler, J{\"u}rgen and Petersen, Kai},
  year = {2022},
  month = jan,
  journal = {Information and Software Technology},
  volume = {141},
  pages = {106717},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2021.106717},
  urldate = {2023-01-15},
  abstract = {Context: Behavior-driven development (BDD) is a variant of test-driven development where specifications are described in a structured domain-specific natural language. Although refactoring is a crucial activity of BDD, little research is available on the topic. Objective: To support practitioners in refactoring BDD specifications by (1) proposing semi-automated approaches to identify refactoring candidates; (2) defining refactoring techniques for BDD specifications; and (3) evaluating the proposed identification approaches in an industry context. Method: Using Action Research, we have developed an approach for identifying refactoring candidates in BDD specifications based on two measures of similarity and applied the approach in two projects of a large software organization. The accuracy of the measures for identifying refactoring candidates was then evaluated against an approach based on machine learning and a manual approach based on practitioner perception. Results: We proposed two measures of similarity to support the identification of refactoring candidates in a BDD specification base; (1) normalized compression similarity (NCS) and (2) similarity ratio (SR). A semi-automated approach based on NCS and SR was developed and applied to two industrial cases to identify refactoring candidates. Our results show that our approach can identify candidates for refactoring 6o times faster than a manual approach. Our results furthermore showed that our measures accurately identified refactoring candidates compared with a manual identification by software practitioners and outperformed an ML-based text classification approach. We also described four types of refactoring techniques applicable to BDD specifications; merging candidates, restructuring candidates, deleting duplicates, and renaming specification titles. Conclusion: Our results show that NCS and SR can help practitioners in accurately identifying BDD specifications that are suitable candidates for refactoring, which also decreases the time for identifying refactoring candidates.},
  langid = {english},
  keywords = {BDD,Behavior-driven development,Normalized Compression Distance (NCD),Normalized Compression Similarity (NCS),Refactoring,Reuse,Similarity ratio (SR),Specifications,Testing},
  file = {/Users/guru/Zotero/storage/THYX9I4E/Irshad et al_2022_Supporting refactoring of BDD specifications—An empirical study_Information and Software Technology.pdf;/Users/guru/Zotero/storage/QY96JYGU/S0950584921001695.html}
}

@article{islam2022:smartvalidator,
  title = {{{SmartValidator}}: {{A}} Framework for Automatic Identification and Classification of Cyber Threat Data},
  shorttitle = {{{SmartValidator}}},
  author = {Islam, Chadni and Babar, M. Ali and Croft, Roland and Janicke, Helge},
  year = {2022},
  month = jun,
  journal = {Journal of Network and Computer Applications},
  volume = {202},
  pages = {103370},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2022.103370},
  urldate = {2023-01-15},
  abstract = {A wide variety of Cyber Threat Information (CTI) is used by Security Operation Centres (SOCs) to perform validation of security incidents and alerts. Security experts manually define different types of rules and scripts based on CTI to perform validation tasks. These rules and scripts need to be updated continuously due to evolving threats, changing SOCs' requirements and dynamic nature of CTI. The manual process of updating rules and scripts delays the response to attacks. To reduce the burden of human experts and accelerate response, we propose a novel Artificial Intelligence (AI) based framework, SmartValidator. SmartValidator leverages Machine Learning (ML) techniques to enable automated validation of alerts. It consists of three layers to perform the tasks of data collection, model building and alert validation. It projects the validation task as a classification problem. Instead of building and saving models for all possible requirements, we propose to automatically construct the validation models based on SOC's requirements and CTI. We built a Proof of Concept (PoC) system with eight ML algorithms, two feature engineering techniques and 18 requirements to investigate the effectiveness and efficiency of SmartValidator. The evaluation results showed that when prediction models were built automatically for classifying cyber threat data, the F1-score of 75\% of the models were above 0.8, which indicates adequate performance of the PoC for use in a real-world organization. The results further showed that dynamic construction of prediction models required 99\% less models to be built than pre-building models for all possible requirements. Thus, SmartValidator is much more efficient to use when SOCs' requirements and threat behaviour are constantly evolving. The framework can be followed by various industries to accelerate and automate the validation of alerts and incidents based on their CTI and SOC's preferences.},
  langid = {english},
  keywords = {Alert validator,Artificial Intelligence,Cyber security,Cyber Threat Information,Machine Learning,Natural language processing,Security automation,Security Operation Centre,Threat data,Threat intelligence},
  file = {/Users/guru/Zotero/storage/HPM6P54B/S1084804522000340.html}
}

@incollection{jackson2010:modern,
  title = {3 - {{The}} Modern Business Environment},
  booktitle = {Web 2.0 {{Knowledge Technologies}} and the {{Enterprise}}},
  author = {Jackson, Paul},
  editor = {Jackson, Paul},
  year = {2010},
  month = jan,
  series = {Chandos {{Information Professional Series}}},
  pages = {55--96},
  publisher = {{Chandos Publishing}},
  doi = {10.1016/B978-1-84334-537-4.50003-X},
  urldate = {2023-01-15},
  isbn = {978-1-84334-537-4},
  langid = {english},
  file = {/Users/guru/Zotero/storage/THL2ZRRJ/B978184334537450003X.html}
}

@inproceedings{Jacobsen2020,
  type = {Conference Paper},
  title = {Perceived and Measured Task Effectiveness in Human-{{AI}} Collaboration},
  author = {Jacobsen, Rune M{\o}berg and Bysted, Lukas Bj{\o}rn Leer and Johansen, Patrick Skov and Papachristos, Eleftherios and Skov, Mikael B.},
  year = {2020},
  series = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},
  doi = {10.1145/3334480.3383104},
  abstract = {Human-AI Collaboration is emerging all around with the increasing utilisation of AI. Few prior studies have investigated the perceived effectiveness of users solving tasks with AI. To expand on these, we conducted a within-subjects repeated measures study involving 35 participants sorting household waste according to recyclability both with and without the help of an AI system. Our results show that people both sorted more effectively and perceived themselves more effective. Furthermore, we document a trend where people sorting without suggestions perceived themselves more effective than they were, while the opposite was true for people when sorting receiving suggestions. Based on our results we propose open questions for future research on perceived effectiveness when collaborating with AI systems. \textcopyright{} 2020 Owner/Author.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Jansen20214971,
  type = {Conference Paper},
  title = {Strengths and Weaknesses of Persona Creation Methods: {{Guidelines}} and Opportunities for Digital Innovations},
  author = {Jansen, Bernard J. and Jung, Soon-Gyo and Salminen, Joni and Guan, Kathleen W. and Nielsen, Lene},
  year = {2021},
  series = {Proceedings of the {{Annual Hawaii International Conference}} on {{System Sciences}}},
  volume = {2020-January},
  pages = {4971--4980},
  abstract = {Persona is a technique for enhancing user understanding and improving the user-centered design of digital products. Persona creation has traditionally been divided into Qualitative, Quantitative, and Mixed Methods approaches. However, no literature systematically contrasts the strengths and weaknesses of these approaches. We review the literature to map the strengths and weaknesses of these approaches and evaluate the potential of personas for the domain of digital innovation. We provide insights for better creation and use of personas by both researchers and practitioners, especially those that are new to personas, deploying personas in a new domain, or familiar with only one of the persona creation approaches. \textcopyright{} 2021 IEEE Computer Society. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Javaid202258,
  type = {Article},
  title = {Significance of Machine Learning in Healthcare: {{Features}}, Pillars and Applications},
  author = {Javaid, Mohd and Haleem, Abid and Pratap Singh, Ravi and Suman, Rajiv and Rab, Shanay},
  year = {2022},
  journal = {International Journal of Intelligent Networks},
  volume = {3},
  pages = {58--73},
  doi = {10.1016/j.ijin.2022.05.002},
  abstract = {Machine Learning (ML) applications are making a considerable impact on healthcare. ML is a subtype of Artificial Intelligence (AI) technology that aims to improve the speed and accuracy of physicians' work. Countries are currently dealing with an overburdened healthcare system with a shortage of skilled physicians, where AI provides a big hope. The healthcare data can be used gainfully to identify the optimal trial sample, collect more data points, assess ongoing data from trial participants, and eliminate data-based errors. ML-based techniques assist in detecting early indicators of an epidemic or pandemic. This algorithm examines satellite data, news and social media reports, and even video sources to determine whether the sickness will become out of control. Using ML for healthcare can open up a world of possibilities in this field. It frees up healthcare providers' time to focus on patient care rather than searching or entering information. This paper studies ML and its need in healthcare, and then it discusses the associated features and appropriate pillars of ML for healthcare structure. Finally, it identified and discussed the significant applications of ML for healthcare. The applications of this technology in healthcare operations can be tremendously advantageous to the organisation. ML-based tools are used to provide various treatment alternatives and individualised treatments and improve the overall efficiency of hospitals and healthcare systems while lowering the cost of care. Shortly, ML will impact both physicians and hospitals. It will be crucial in developing clinical decision support, illness detection, and personalised treatment approaches to provide the best potential outcomes. \textcopyright{} 2022 The Authors},
  publication_stage = {Final},
  source = {Scopus}
}

@article{javaid2023:integrated,
  title = {An Integrated Outlook of {{Cyber}}\textendash{{Physical Systems}} for {{Industry}} 4.0: {{Topical}} Practices, Architecture, and Applications},
  shorttitle = {An Integrated Outlook of {{Cyber}}\textendash{{Physical Systems}} for {{Industry}} 4.0},
  author = {Javaid, Mohd and Haleem, Abid and Singh, Ravi Pratap and Suman, Rajiv},
  year = {2023},
  month = jan,
  journal = {Green Technologies and Sustainability},
  volume = {1},
  pages = {100001},
  issn = {2949-7361},
  doi = {10.1016/j.grets.2022.100001},
  urldate = {2023-01-15},
  abstract = {Industry 4.0 requires a strong understanding of Cyber\textendash Physical Systems (CPS). An Industry 4.0-enabled manufacturing environment that offers real-time data gathering, transparency and analysis across all parts of a manufacturing process is known as cyber\textendash physical systems, also known as cyber manufacturing. Data analytics enables executives to make data-driven choices and boost productivity, while automation speeds up manufacturing and decreases machine downtime. The main objective of deploying Industry 4.0 solutions is to enable manufacturing organisations to increase collaboration by making the correct information accessible to the right people in real time. The aim of encouraging optimal decision-making at the appropriate moment is to improve efficiency and production further. The critical terms in Industry 4.0, such as the CPS, Internet of Things (IoT), and Digital Twin, are widely used interchangeably in conversations about smart manufacturing. These are essential to Industry 4.0 and smart manufacturing because they give users access to real-time operating data of the equipment they represent. Cyber components and physical components make up the two elements of CPS. The main aim of this paper is to brief CPS and its need for Industry 4.0. Embedded Processes and Smart 5C diagrammatically elaborate architecture of CPS for Industry 4.0. Finally, the paper identifies and discusses significant applications of CPS in Industry 4.0. For this paper, we identified and then studied relevant literature on CPS for establishing an Industry 4.0 environment. CPS is integrated into several items, including vehicles and other equipment, to carry out particular functions. CPS can be utilised in any industry, including engineering, manufacturing, transportation, and even health care, because they are all easy to use. CPS network collaborative systems are built for communication. Furthermore, cutting-edge network technologies like cloud solutions are used by CPS.},
  langid = {english},
  keywords = {Applications,Cyber–Physical Systems (CPS),Industry 4.0,Manufacturing}
}

@article{Jazaeri20213187,
  type = {Article},
  title = {Edge Computing in {{SDN-IoT}} Networks: A Systematic Review of Issues, Challenges and Solutions},
  author = {Jazaeri, Seyedeh Shabnam and Jabbehdari, Sam and Asghari, Parvaneh and Haj Seyyed Javadi, Hamid},
  year = {2021},
  journal = {Cluster Computing},
  volume = {24},
  number = {4},
  pages = {3187--3228},
  doi = {10.1007/s10586-021-03311-6},
  abstract = {Software defined networks and the Internet of Things (IoT) are two major and emerging developments in networking that have much in common and their survival depends on each other. Software Defined Networking (SDN) is one of the 5G enabling innovations that can help design complex, manageable, cost-effective and adaptable networks. On the other hand, Edge Computing (EC) will do automatic analytical computing on data from sensors, network switches, or other devices instead of waiting for data to be returned to a centralized data store. The IoT also requires a decentralized Internet, as the demand for real-time data analysis is growing and centralized processing systems are being overrun. So, it is necessary to provide a solution to encompass the advantages of EC, IoT and SDN simultaneously, as an integrated platform. In this research, after presenting a brief explanation about the key points of the SDN, IoT, EC and related concepts, and recent articles in this regard are investigated. This systematic literature review (SLR) study focuses on different frameworks and platforms that meet the mentioned requirements by considering the advantages of integrating EC, SDN, and IoT technologies. This platform provides centralized management of heterogeneous devices and architectures and supports the problem of resource limitations in IoT. The short output of this literature contains the following: (1) presenting a summary of review studies and research articles in this area which have been published from 2013 till 2021; (2) providing some key technical questions; (3) presenting some different technical classifications to categorizing the characteristics and features of EC and SDN in IoT; (4) discussing the key challenges; (5) Presenting future directions for research and open issues. \textcopyright{} 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Jiang20151091,
  type = {Article},
  title = {A Survey on Load Testing of Large-Scale Software Systems},
  author = {Jiang, Zhen Ming and Hassan, Ahmed E.},
  year = {2015},
  journal = {IEEE Transactions on Software Engineering},
  volume = {41},
  number = {11},
  pages = {1091--1118},
  doi = {10.1109/TSE.2015.2445340},
  abstract = {Many large-scale software systems must service thousands or millions of concurrent requests. These systems must be load tested to ensure that they can function correctly under load (i.e., the rate of the incoming requests). In this paper, we survey the state of load testing research and practice. We compare and contrast current techniques that are used in the three phases of a load test: (1) designing a proper load, (2) executing a load test, and (3) analyzing the results of a load test. This survey will be useful for load testing practitioners and software engineering researchers with interest in the load testing of large-scale software systems. \textcopyright{} 2015 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Jiang2021,
  type = {Article},
  title = {Investigating and Recommending Co-Changed Entities for {{JavaScript}} Programs},
  author = {Jiang, Zijian and Zhong, Hao and Meng, Na},
  year = {2021},
  journal = {Journal of Systems and Software},
  volume = {180},
  doi = {10.1016/j.jss.2021.111027},
  abstract = {JavaScript (JS) is one of the most popular programming languages due to its flexibility and versatility, but maintaining JS code is tedious and error-prone. In our research, we conducted an empirical study to characterize the relationship between co-changed software entities (e.g., functions and variables), and built a machine learning (ML)-based approach to recommend additional entity to edit given developers' code changes. Specifically, we first crawled 14,747 commits in 10 open-source projects; for each commit, we created at least one change dependency graph (CDG) to model the referencer\textendash referencee relationship between co-changed entities. Next, we extracted the common subgraphs between CDGs to locate recurring co-change patterns between entities. Finally, based on those patterns, we extracted code features from co-changed entities and trained an ML model that recommends entities-to-change given a program commit. According to our empirical investigation, (1) three recurring patterns commonly exist in all projects; (2) 80\%\textendash 90\% of co-changed function pairs either invoke the same function(s), access the same variable(s), or contain similar statement(s); (3) our ML-based approach CoRec recommended entity changes with high accuracy (73\%\textendash 78\%). CoRec complements prior work because it suggests changes based on program syntax, textual similarity, as well as software history; it achieved higher accuracy than two existing tools in our evaluation. \textcopyright{} 2021},
  publication_stage = {Final},
  source = {Scopus}
}

@article{jolak2022:conserve,
  title = {{{CONSERVE}}: {{A}} Framework for the Selection of Techniques for Monitoring Containers Security},
  shorttitle = {{{CONSERVE}}},
  author = {Jolak, Rodi and Rosenstatter, Thomas and Mohamad, Mazen and Strandberg, Kim and Sangchoolie, Behrooz and Nowdehi, Nasser and Scandariato, Riccardo},
  year = {2022},
  month = apr,
  journal = {Journal of Systems and Software},
  volume = {186},
  pages = {111158},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111158},
  urldate = {2023-01-15},
  abstract = {Context: Container-based virtualization is gaining popularity in different domains, as it supports continuous development and improves the efficiency and reliability of run-time environments. Problem: Different techniques are proposed for monitoring the security of containers. However, there are no guidelines supporting the selection of suitable techniques for the tasks at hand. Objective: We aim to support the selection and design of techniques for monitoring container-based virtualization environments. Approach : First, we review the literature and identify techniques for monitoring containerized environments. Second, we classify these techniques according to a set of categories, such as technical characteristic, applicability, effectiveness, and evaluation. We further detail the pros and cons that are associated with each of the identified techniques. Result: As a result, we present CONSERVE, a multi-dimensional decision support framework for an informed and optimal selection of a suitable set of container monitoring techniques to be implemented in different application domains. Evaluation: A mix of eighteen researchers and practitioners evaluated the ease of use, understandability, usefulness, efficiency, applicability, and completeness of the framework. The evaluation shows a high level of interest, and points out to potential benefits.},
  langid = {english},
  keywords = {Attack analysis,Container monitoring,Intrusion detection,Security,Software and systems engineering,Virtualization},
  file = {/Users/guru/Zotero/storage/MTBTZKLT/Jolak et al_2022_CONSERVE_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/X3CNXXN3/S0164121221002478.html}
}

@inproceedings{ju2020:what,
  title = {What {{Agile Processes Should We Use}} in {{Software Engineering Course Projects}}?},
  booktitle = {Proceedings of the 51st {{ACM Technical Symposium}} on {{Computer Science Education}}},
  author = {Ju, An and Hemani, Adnan and Dimitriadis, Yannis and Fox, Armando},
  year = {2020},
  month = feb,
  series = {{{SIGCSE}} '20},
  pages = {643--649},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3328778.3366864},
  urldate = {2023-01-15},
  abstract = {While project-based software engineering courses aim to provide learning opportunities grounded in professional processes, it is not always possible to replicate every process in classrooms due to course constraints. Previous studies observed how students react to various processes and gave retroactive recommendations. In this study, we instead combine a field study on professional Agile (eXtreme Programming, XP) teams and an established team process taxonomy to proactively select team processes to incorporate in a project-based software engineering course. With collected knowledge from the field study, we choose three XP processes to augment the design of a mature software engineering project course. We choose processes that are 1) considered important by professionals, and 2) complete with respect to coverage of the taxonomy's main categories. We then compare the augmented course design with the original design in a case study. Our results suggest that 1) even without extra resources, adding these new processes does not interfere with learning opportunities for XP processes previously existing in the course design; 2) student teams experience similar benefits from these new processes as professional teams do, and students appreciate the usefulness and value of the processes. In other words, our approach allows instructors to make conscious choices of XP processes that improve student learning outcomes while exposing students to a more complete set of processes and thus preparing them better for professional careers. Course designers with limited resources are encouraged to use our methodology to evaluate and improve the designs of their own project-based courses.},
  isbn = {978-1-4503-6793-6},
  keywords = {agile,project-based learning,software engineering},
  file = {/Users/guru/Zotero/storage/B43CND5B/Ju et al_2020_What Agile Processes Should We Use in Software Engineering Course Projects_Proceedings of the 51st ACM Technical Symposium on Computer Science Education.pdf}
}

@inproceedings{Jüngling2020,
  type = {Conference Paper},
  title = {Towards {{AI-based}} Solutions in the System Development Lifecycle},
  author = {J{\"u}ngling, Stephan and Peraic, Martin and Martin, Andreas},
  year = {2020},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2600},
  abstract = {Many teams across different industries and organizations explicitly apply agile methodologies such as Scrum in their system development lifecycle (SDLC). The choice of the technology stack, the programming language, or the decision whether AI solutions could be incorporated into the system design either is given by corporate guidelines or is chosen by the project team based on their individual skill set. The paper describes the business case of implementing an AI-based automatic passenger counting system for public transportation, shows preliminary results of the prototype using anonymous passenger recognition on the edge with the help of Google Coral devices. It shows how different solutions could be integrated with the help of rule base systems and how AI-based solutions could be established in the SDLC as valid and cost-saving alternatives to traditionally programmed software components. Copyright \textcopyright{} 2020 held by the author(s).},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Jüngling2022,
  type = {Conference Paper},
  title = {Using the Strategy Design Pattern for Hybrid {{AI}} System Design},
  author = {J{\"u}ngling, Stephan and Peraic, Martin and Zhu, Cheng},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3121},
  abstract = {The idea of design patterns originated in the architecture domain, subsequently shaped the standardization and communication of object-oriented system design for IT architectures, and facilitated the description of best practices in business process design. Recently, the idea of design patterns not only stipulated an initial collection and classification of machine learning patterns, but has also been used to structure and document machine learning based systems from a traditional software engineering perspective. We promote the idea of using design patterns as a general means to visualize the design of hybrid AI systems and present how the strategy design pattern in particular can be used for a passenger counting system by switching the implementation strategies from a standard YOLOv5 based object recognition with Deep Sort tracking to a customized head-based YOLOv5 detection in combination with a customized Deep Sort tracking strategy. In our example, the newly presented human head detector and tracker could significantly improve the overall accuracy of passenger counting in dense and crowded situations. Furthermore, we show, how rule-based symbolic decisions can be allocated to an abstractstrategy class, while the sub-symbolic machine learning task is delegated to the most appropriate person- or head-based ConcreteStrategy class during run-time. \textcopyright{} 2022 Copyright for this paper by its authors},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/4J2C98TI/paper5.pdf;/Users/guru/Zotero/storage/BUWP2W9N/paper5.pdf}
}

@article{karanatsiou2019:bibliometric,
  title = {A Bibliometric Assessment of Software Engineering Scholars and Institutions (2010\textendash 2017)},
  author = {Karanatsiou, Dimitra and Li, Yihao and Arvanitou, Elvira-Maria and Misirlis, Nikolaos and Wong, W. Eric},
  year = {2019},
  month = jan,
  journal = {Journal of Systems and Software},
  volume = {147},
  pages = {246--261},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2018.10.029},
  urldate = {2023-01-15},
  abstract = {This paper presents the findings of a bibliometric study, targeting an eight-year period (2010\textendash 2017), with the aim of identifying: (a) emerging research directions, (b) the top-20 institutions, and (c) top-20 early stage, consolidated, and experienced scholars in the field of software engineering. To perform this goal, we performed a bibliometric study, by applying the mapping study technique on top-quality software engineering venues, and developed a dataset of 14,456 primary studies. As the ranking metric for institutions, we used the count of papers in which authors affiliated with this institute have been identified in the obtained dataset, whereas regarding scholars we computed the corresponding rankings based on the number of published papers and the average number of citations. Finally, we identified the top-20 rising scholars in the SE research community, based on their recent publication record (between 2015 and 2017) and their research age.},
  langid = {english},
  keywords = {Publications,Software engineering,Top institutions,Top scholars},
  file = {/Users/guru/Zotero/storage/4E4MN2E8/S0164121218302334.html}
}

@article{Katz2022574,
  type = {Article},
  title = {Verification of Image-Based Neural Network Controllers Using Generative Models},
  author = {Katz, Sydney M. and Corso, Anthony L. and Strong, Christopher A. and Kochenderfer, Mykel J.},
  year = {2022},
  journal = {Journal of Aerospace Information Systems},
  volume = {19},
  number = {9},
  pages = {574--584},
  doi = {10.2514/1.I011071},
  abstract = {Although neural networks are effective tools for processing information from image-based sensors to produce control actions, their complex nature limits their use in safety-critical systems. For this reason, recent work has focused on combining techniques in formal methods and reachability analysis to obtain guarantees on the closed-loop performance of neural network controllers. However, these techniques do not scale to the high-dimensional and complicated input space of image-based neural network controllers. This work proposes a method to address these challenges by training a generative adversarial network to map states to plausible input images. Concatenating the generator network with the control network results in a network with a low-dimensional input space, which allows for the use of existing closed-loop verification tools to obtain formal guarantees on the performance of image-based controllers. This approach is applied to provide safety guarantees for an image-based neural network controller for an autonomous aircraft taxi problem. The resulting guarantees are with respect to the set of input images modeled by the generator network, and so a recall metric is provided to evaluate how well the generator captures the space of plausible images. \textcopyright{} 2022 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Kaur20193462,
  type = {Article},
  title = {Web Effort Estimation Techniques: {{A}} Systematic Literature Review},
  author = {Kaur, Manpreet and Sood, Sumesh},
  year = {2019},
  journal = {Compusoft},
  volume = {8},
  number = {11},
  pages = {3462--3471},
  doi = {10.6084/ijact.v8i11.1017},
  abstract = {Web Effort Estimation is an important estimation measure for predicting the effort required to develop a web application. The completion of web projects within stipulated time and budget is not possible without accurate effort estimation. The numerous effort estimation models are present these days and they have achieved a pinnacle of success, but the uncertainty features are daunting its progress due to deviations in the data set collected, types of projects, and data set characteristics. The literature studied for this research task elaborated that this field still lacks in a significant direction for consolidated documentation, which guides the researchers to choose a specific technique in order to predict the effort required for web application development. The wide and versatile nature of this domain daunting the researchers to mine the literature in a more appropriate way and deploy ensemble techniques of effort prediction models in order to achieve better results for web application viz., schedule delays, budget overruns. The systematic literature review (SLR) in this research task has been done to inspect the various aspects affecting the prediction accuracy of web applications and these identified characteristics lead to a better effort estimation model. The literature review is conducted on a collection of 143 papers retrieved from online journals and conference proceedings. Only 53 relevant papers are selected for broad investigation. The study reveals that the expert judgment and algorithm-based models are very popular and used frequently for effort prediction, instead the machine learning (ML) based models are rare in use but cater comparatively better prediction accuracy. The authors suggest taking cognizance of this research domain for developing ensembles of early effort prediction models to overcome delays in schedule and budget. \textcopyright{} 2019 National Institute of Science Communication and Information Resources (NISCAIR).},
  publication_stage = {Final},
  source = {Scopus}
}

@article{kaur2020:comprehensive,
  title = {A Comprehensive Survey of Service Function Chain Provisioning Approaches in {{SDN}} and {{NFV}} Architecture},
  author = {Kaur, Karamjeet and Mangat, Veenu and Kumar, Krishan},
  year = {2020},
  month = nov,
  journal = {Computer Science Review},
  volume = {38},
  pages = {100298},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100298},
  urldate = {2023-01-15},
  abstract = {Network Function Virtualization (NFV) has emerged as an innovative network architecture paradigm that uses IT virtualization technology to abstract the network node functions from hardware. The virtualized network services hosted on Virtual Machines (VMs) are called Virtual Network Functions (VNFs). The sequence of multiple VNFs required by network operators to perform traffic steering is called a Service Function Chain (SFC). Software Defined Networking (SDN) is a complementary technology which allows programmatic control of network functions and policy-based resource management. The flexibility of SDN facilitates structuring of SFCs with minimum latency. SFC provisioning using SDN and NFV will enable implementation of next generation 5G networks and make the subscriber/operator relationship more economical and flexible. In this paper, a Systematic Literature Review (SLR) is used to select the high-quality research studies related to dynamic provisioning of SFCs in SDN and NFV. A total of 70 studies available in the literature are analyzed. Thereafter, a layered taxonomy is proposed to classify the literature based on the parameters of optimization approaches for the provisioning of SFCs. Finally, the open research challenges for SFC deployment are identified and discussed. This paper is intended to serve as a ready reference for the research community to develop effective and efficient techniques for SFC provisioning in combined SDN/NFV networks by considering a combination of multiple factors viz. placement of VNFs, load balancing, and availability. It will surely aid Cloud Service Providers (CSPs), Application Service Providers (ASPs), and Internet Service Providers (ISPs) in offering reliable, scalable and high-performance services to their customers.},
  langid = {english},
  keywords = {Availability,Load balancing of VNFs,Network Function Virtualization,Service classifier,Service Function Chaining,Service Function Forwarder,Virtual Network Function,VNF placement},
  file = {/Users/guru/Zotero/storage/7L78PKXP/S1574013720303981.html}
}

@article{kaur2021:comprehensive,
  title = {A Comprehensive Survey of {{DDoS}} Defense Solutions in {{SDN}}: {{Taxonomy}}, Research Challenges, and Future Directions},
  shorttitle = {A Comprehensive Survey of {{DDoS}} Defense Solutions in {{SDN}}},
  author = {Kaur, Sukhveer and Kumar, Krishan and Aggarwal, Naveen and Singh, Gurdeep},
  year = {2021},
  month = nov,
  journal = {Computers \& Security},
  volume = {110},
  pages = {102423},
  issn = {0167-4048},
  doi = {10.1016/j.cose.2021.102423},
  urldate = {2023-01-15},
  abstract = {The recent emergence of technologies such as Network Functions Virtualization (NFV), Intent based Networking, Internet of Things (IoT), 5G, and Cloud Computing have led to the rapid growth of networks. The inflexibility and vendor-specific nature of traditional network devices are unable to fulfill the requirements of modern data centers. Software-Defined Networking (SDN) has captured data center space due to its innovative features viz. vendor neutrality, programmability, and centralized management. However, SDN is also facing various security threats due to weaknesses in its inherent architecture. This article has attempted to identify various vulnerable points in the SDN framework and has classified the SDN-aimed DDoS attacks based on their impacts. This article presents a systematic literature review on various DDoS defense mechanisms to protect the control plane, data plane, and data-control plane communication channel. In this study, a well-defined methodology is used to select the high-quality research articles of DDoS defense mechanisms in the SDN framework. Among numerous articles published in the last few years, the authors have selected 75 articles with the highest impact factor and citation. Moreover, we present the taxonomy of DDoS defense solutions that classify the reviewed articles based on the attack targets, DDoS defense approaches, testing environment, and traffic generation mechanism. Finally, we identified the research gaps and highlighted various research challenges for future research. This study is intended to serve as a ready reference for the research community to develop more efficient and reliable DDoS defense solutions in the SDN networks.},
  langid = {english},
  keywords = {Control plane,Data plane,Distributed denial of service attack,OpenFlow,Security,Software-defined networking},
  file = {/Users/guru/Zotero/storage/ILUGRIYY/S0167404821002479.html}
}

@inproceedings{Keller2019,
  type = {Conference Paper},
  title = {Affordance-Experimentation-Actualization Theory in Artificial Intelligence Research - {{A}} Predictive Maintenance Story},
  author = {Keller, Robert and Stohr, Alexander and Fridgen, Gilbert and Lockl, Jannik and Rieger, Alexander},
  year = {2019},
  series = {40th {{International Conference}} on {{Information Systems}}, {{ICIS}} 2019},
  abstract = {Artificial intelligence currently counts among the most prominent digital technologies and promises to generate significant business value in the future. Despite a growing body of knowledge, research could further benefit from incorporating technological features, human actors, and organizational goals into the examination of artificial intelligence-enabled systems. This integrative perspective is crucial for effective implementation. Our study intends to fill this gap by introducing affordance-experimentation-actualization theory to artificial intelligence research. In doing so, we conduct a case study on the implementation of predictive maintenance using affordance-experimentation-actualization theory as our theoretical lens. From our study, we find further evidence for the existence of the experimentation phase during which organizations make new technologies ready for effective use. We propose extending the experimentation phase with the activity of 'conceptual exploration' in order to make affordance-experimentation-actualization theory applicable to a broader range of technologies and the domain of AI-enabled systems in particular. \textcopyright{} 40th International Conference on Information Systems, ICIS 2019. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary}
}

@article{kersten2018:five,
  title = {Five Predictions for the Coming Decades of Software},
  author = {Kersten, Mik},
  year = {2018},
  month = sep,
  journal = {IEEE Software},
  volume = {35},
  number = {5},
  pages = {7--9},
  issn = {1937-4194},
  doi = {10.1109/MS.2018.3571232},
  abstract = {To help celebrate software engineering's 50th anniversary, department editor Mik Kersten considers how software engineering will evolve over the coming 50 years. His five predictions aren't intended to be precise; they aim to provide discussion topics for the shape of software engineering trends to come. This article is part of a theme issue on software engineering's 50th anniversary.},
  keywords = {AI,artificial intelligence,Carlota Perez,On DevOps,software development,software engineering,Software engineering,Technology forecasting},
  file = {/Users/guru/Zotero/storage/K2PTBG54/Kersten_2018_Five predictions for the coming decades of software_IEEE Software.pdf}
}

@inproceedings{kessel2018:integrating,
  title = {Integrating Reuse into the Rapid, Continuous Software Engineering Cycle through Test-Driven Search},
  booktitle = {Proceedings of the 4th {{International Workshop}} on {{Rapid Continuous Software Engineering}}},
  author = {Kessel, Marcus and Atkinson, Colin},
  year = {2018},
  month = may,
  series = {{{RCoSE}} '18},
  pages = {8--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3194760.3194761},
  urldate = {2023-01-15},
  abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
  isbn = {978-1-4503-5745-6},
  keywords = {rapid continuous code reuse,rapid continuous integration,test-driven development,test-driven reuse,test-driven search},
  file = {/Users/guru/Zotero/storage/BQPRL6ZL/Kessel_Atkinson_2018_Integrating reuse into the rapid, continuous software engineering cycle through_Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering.pdf}
}

@article{khan2018:understanding,
  title = {Understanding Autonomic Network Management: {{A}} Look into the Past, a Solution for the Future},
  shorttitle = {Understanding Autonomic Network Management},
  author = {Khan, Manzoor Ahmed and Peters, Sebastian and Sahinel, Doruk and {Pozo-Pardo}, Francisco Denis and Dang, Xuan-Thuy},
  year = {2018},
  month = jun,
  journal = {Computer Communications},
  volume = {122},
  pages = {93--117},
  issn = {0140-3664},
  doi = {10.1016/j.comcom.2018.01.014},
  urldate = {2023-01-15},
  abstract = {The evolution of mobile network technologies and their vertical integration, heterogeneity of applications, and the advent of sophisticated end-user devices have continuously been expanding the complexity of network management tasks. In addition, there is a significant urge for the dynamic reconfiguration of networks to meet operators' costs and to achieve their performance objectives. These facts substantiate the idea of pushing the classical human dependent network management approaches out of the equation to a great extent. The vast scope of network management makes it difficult to have a common understanding and definition, which is often noticeable in different research articles. The situation is further worsened by the network evolution timeline that traverses several technological shifts, such as the time when computer networks and mobile networks were far apart, to the time of fully IP-based and converged networks. Hence, one of the main aims of this paper is to provide a study of the network management evolution in general and in particular the concepts of autonomic network management, so that researchers may be equipped to understand the involved concepts. To achieve the aforementioned objective, the authors carried out an elaborate analysis of the different network management approaches, mapped them to a timeline, and discussed their features. This analysis sets the stage for an extensive discussion of the enabling concepts of autonomic network management, followed by a survey of research projects targeting the advancement of the autonomic networking vision. Having identified incomplete realizations of autonomic network management due to simplifying assumptions, this paper focused on the relevant aspects of architectural construction with the presentation of the core challenges to be addressed so as to realize a fully autonomic network management framework. These challenges led us to reconstruct the design goals that the contributions of this work were built upon. The first proposal of this paper is to deploy intelligent software agents on different hierarchical layers of the proposed mobile network architecture. The agents implement different stages of cognitive control loops and contribute to learning algorithms for various management tasks. CoDIPAS-RL learning framework is used for layer specific learning decisions. To advance the autonomic network management, the authors also propose a novel idea of self-learning that enables the meta-learning vision. This paper concludes with a discussion on the implementation of our autonomic network management framework and with a use case that shows the performance of the proposed approach.},
  langid = {english},
  keywords = {Autonomic network management,Meta learning,Self-x network management},
  file = {/Users/guru/Zotero/storage/6GL3FUMJ/S0140366417305327.html}
}

@article{khan2022:introduction,
  title = {Introduction to the Special Issue on Managing Software Processes Using Soft Computing Techniques},
  author = {Khan, Arif Ali and Abrahamsson, Pekka and Niazi, Mahmood},
  year = {2022},
  month = dec,
  journal = {Information and Software Technology},
  volume = {152},
  pages = {107055},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.107055},
  urldate = {2023-01-15},
  abstract = {The coronavirus outbreak dramatically changed the work culture in the software industry. Most software practitioners began working remotely, which significantly revolutionized the traditional software processes landscape. Software development organizations have begun thinking about automating software processes to cope with the challenges raised by remote work. This special issue presents papers describing soft computing solutions for improving traditional software processes and capabilities. This editorial introduces the accepted papers and reflects on their contributions.},
  langid = {english},
  file = {/Users/guru/Zotero/storage/N5N7AZI2/Khan et al_2022_Introduction to the special issue on managing software processes using soft_Information and Software Technology.pdf;/Users/guru/Zotero/storage/FEKICBXJ/S0950584922001665.html}
}

@article{khan2022:review,
  title = {A Review on Machine Learning and Deep Learning for Various Antenna Design Applications},
  author = {Khan, Mohammad Monirujjaman and Hossain, Sazzad and Mozumdar, Puezia and Akter, Shamima and Ashique, Ratil H.},
  year = {2022},
  month = apr,
  journal = {Heliyon},
  volume = {8},
  number = {4},
  pages = {e09317},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2022.e09317},
  urldate = {2023-01-15},
  abstract = {The next generation of wireless communication networks will rely heavily on machine learning and deep learning. In comparison to traditional ground-based systems, the development of various communication-based applications is projected to increase coverage and spectrum efficiency. Machine learning and deep learning can be used to optimize solutions in a variety of applications, including antennas. The latter have grown popular for obtaining effective solutions due to high computational processing, clean data, and large data storage capability. In this research, machine learning and deep learning for various antenna design applications have been discussed in detail. The general concept of machine learning and deep learning is introduced. However, the main focus is on various antenna applications, such as millimeter wave, body-centric, terahertz, satellite, unmanned aerial vehicle, global positioning system, and textiles. The feasibility of antenna applications with respect to conventional methods, acceleration of the antenna design process, reduced number of simulations, and better computational feasibility features are highlighted. Overall, machine learning and deep learning provide satisfactory results for antenna design.},
  langid = {english},
  keywords = {Antenna,Beam-forming,Body-centric,CDF,CNN,Deep MIMO,DNN,Frequency,GSCM,LOS,Machine learning,Meta-material identification,Millimeter wave,NLOS,PDP,Radio frequency,RFC,THz communications,THz DL CT},
  file = {/Users/guru/Zotero/storage/ZUTIMAS7/S2405844022006053.html}
}

@inproceedings{khanna2021:artificial,
  title = {Artificial Intelligence Based Risk Management Framework for Distributed Agile Software Development},
  booktitle = {2021 8th International Conference on Signal Processing and Integrated Networks ({{SPIN}})},
  author = {Khanna, Esha and Popli, Rashmi and Chauhan, Naresh},
  year = {2021},
  month = aug,
  pages = {657--660},
  issn = {2688-769X},
  doi = {10.1109/SPIN52536.2021.9566000},
  abstract = {Distributed Agile Software Development blends the principles of agile to geographically separated teams and thereby combines the quality benefits of Agile Software Development with cost benefits of Distributed Software Development. Distributed Agile Software Development brings along new risks that arise due to the difference in working principles of Agile Software Development and Distributed Software Development. Identifying and managing these risks on time improves the quality of the software product and reduces the overall development cost. This work presents a novel Artificial Intelligence based framework for managing risks in Distributed Agile Software Development. The proposed framework will help the practitioners to timely identify and deal with associated risks and thereby reduce the cost and development time.},
  keywords = {Agile software development,Artificial intelligence,Artificial Intelligence,Bibliographies,Costs,Distributed Agile Software Development,Risk management,Risk Management,Signal processing,Software}
}

@inproceedings{Khanna2021657,
  type = {Conference Paper},
  title = {Artificial Intelligence Based Risk Management Framework for Distributed Agile Software Development},
  author = {Khanna, Esha and Popli, Rashmi and Chauhan, Naresh},
  year = {2021},
  series = {Proceedings of the 8th {{International Conference}} on {{Signal Processing}} and {{Integrated Networks}}, {{SPIN}} 2021},
  pages = {657--660},
  doi = {10.1109/SPIN52536.2021.9566000},
  abstract = {Distributed Agile Software Development blends the principles of agile to geographically separated teams and thereby combines the quality benefits of Agile Software Development with cost benefits of Distributed Software Development. Distributed Agile Software Development brings along new risks that arise due to the difference in working principles of Agile Software Development and Distributed Software Development. Identifying and managing these risks on time improves the quality of the software product and reduces the overall development cost. This work presents a novel Artificial Intelligence based framework for managing risks in Distributed Agile Software Development. The proposed framework will help the practitioners to timely identify and deal with associated risks and thereby reduce the cost and development time. \textcopyright{} 2021 IEEE},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Khatoon2022,
  type = {Article},
  title = {Importance of Semantic Interoperability in Smart Agriculture Systems},
  author = {Khatoon, P Salma and Ahmed, Muqeem},
  year = {2022},
  journal = {Transactions on Emerging Telecommunications Technologies},
  volume = {33},
  number = {5},
  doi = {10.1002/ett.4448},
  abstract = {The Internet of Things (IoT) connects people with real-world objects to exchange data. It is performed using services and devices present in the user's activities based on the IoT. Based on a common vocabulary or mappings, knowledge and information can be exchanged by different agents, services, and devices in this scenario. The heterogeneous sources can be represented and integrated by an ontology. The Semantic Web offers semantic interoperability that facilitates communication between heterogeneous devices and technology platforms. The state-of-the-art of IoT semantic interoperability is assessed and reviewed in this work. The importance and challenges of interoperability are discussed in detail with an in-depth analysis of the requirements. IoT based semantic interoperability model is discussed with semantic annotations of data required for heterogeneous IoT devices. Additionally, it has been presented which Semantic Web technologies are incorporated, and the challenges have been studied in this research area. Agriculture is a domain where IoT applications have a lot of potentials. The market is filled with several devices that collect data from the farms and send it to the cloud. Semantic Web technology for applications in agriculture has been discussed in detail with data integration. Semantic resources for agriculture have been enlisted with linked data hubs and semantic data standards. The article aims to address the need and requirement of IoT with semantic interoperability in the field of agriculture. The use of IoT interoperability in agriculture can bring long term benefits to the farmers and increase productivity while reducing the overall costs incurred. \textcopyright{} 2022 John Wiley \& Sons, Ltd.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{King201981,
  type = {Conference Paper},
  title = {{{AI}} for Testing Today and Tomorrow: Industry Perspectives},
  author = {King, Tariq M. and Arbon, Jason and Santiago, Dionny and Adamo, David and Chin, Wendy and Shanmugam, Ram},
  year = {2019},
  series = {Proceedings - 2019 {{IEEE International Conference}} on {{Artificial Intelligence Testing}}, {{AITest}} 2019},
  pages = {81--88},
  doi = {10.1109/AITest.2019.000-3},
  abstract = {With modern advances in artificial intelligence (AI) and machine learning and their applications to software testing, the intersection of AI and testing is receiving close attention. The 2018 Annual Western Conference on Software Testing Analysis and Review featured a two-session panel on AI for Software Testing (AIST). The panel brought together six industry experts with experience developing AIST products, services, and research prototypes. Questions sourced from the industrial testing community were used to provoke thought, stimulate conversation, and guide panel discussions. This paper provides a review of the industry panel, which includes discussions on the visions, ideas, thoughts, strategies, directions, and lessons learned developing systems that use AI to test software, applying methods to test AI systems, and designing self-testing systems. Both the testing community survey and the expert panel yielded insightful perspectives on AIST in practice. \textcopyright{} 2019 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Kirby2021358,
  type = {Conference Paper},
  title = {Weighing the Evidence: {{On}} Relationship Types in Microservice Extraction},
  author = {Kirby, Lisa J. and Boerstra, Evelien and Anderson, Zachary J.C. and Rubin, Julia},
  year = {2021},
  series = {{{IEEE International Conference}} on {{Program Comprehension}}},
  volume = {2021-May},
  pages = {358--368},
  doi = {10.1109/ICPC52881.2021.00041},
  abstract = {The microservice-based architecture-A SOA-inspired principle of dividing systems into components that communicate with each other using language-Agnostic APIs-has gained increased popularity in industry. Yet, migrating a monolithic application to microservices is a challenging task. A number of automated microservice extraction techniques have been proposed to help developers with the migration complexity. These techniques, at large, construct a graph-based representation of an application and cluster its elements into service candidates. The techniques vary by their decomposition goals and, subsequently, types of relationships between application elements that they consider-structural, semantic term similarity, and evolutionary-with each technique utilizing a fixed subset and weighting of these relationship types.In this paper, we perform a multi-method exploratory study with 10 industrial practitioners to investigate (1) the applicability and usefulness of different relationships types during the microservice extraction process and (2) expectations practitioners have for tools utilizing such relationships. Our results show that practitioners often need a "what-if"analysis tool that simultaneously considers multiple relationship types during the extraction process and that there is no fixed way to weight these relationships. Our study also identifies organization-And application-specific considerations that lead practitioners to prefer certain relationship types over others, e.g., the age of the codebase and languages spoken in the organization. It outlines possible strategies to help developers during the extraction process, e.g., the ability to iteratively filter and customize relationships. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{kitchenham2007:guidelines,
  title = {Guidelines for Performing {{Systematic Literature Reviews}} in {{Software Engineering}}},
  author = {Kitchenham, Barbara and Charters, Stuart},
  year = {2007},
  month = jan,
  volume = {2},
  abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest.  Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
  file = {/Users/guru/Zotero/storage/FTFMVM5D/Guidelines-for-performing-Systematic-Literature-Reviews-in-Software-Engineering.pdf}
}

@article{kitchenham2010:what,
  title = {What's up with Software Metrics? \textendash{} {{A}} Preliminary Mapping Study},
  shorttitle = {What's up with Software Metrics?},
  author = {Kitchenham, Barbara},
  year = {2010},
  month = jan,
  journal = {Journal of Systems and Software},
  series = {{{SI}}: {{Top Scholars}}},
  volume = {83},
  number = {1},
  pages = {37--51},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2009.06.041},
  urldate = {2023-01-15},
  abstract = {Background Many papers are published on the topic of software metrics but it is difficult to assess the current status of metrics research. Aim This paper aims to identify trends in influential software metrics papers and assess the possibility of using secondary studies to integrate research results. Method Search facilities in the SCOPUS tool were used to identify the most cited papers in the years 2000\textendash 2005 inclusive. Less cited papers were also selected from 2005. The selected papers were classified according factors such as to main topic, goal and type (empirical or theoretical or mixed). Papers classified as ``Evaluation studies'' were assessed to investigate the extent to which results could be synthesized. Results Compared with less cited papers, the most cited papers were more frequently journal papers, and empirical validation or data analysis studies. However, there were problems with some empirical validation studies. For example, they sometimes attempted to evaluate theoretically invalid metrics and fail to appreciate the importance of the context in which data are collected. Conclusions This paper, together with other similar papers, confirms that there is a large body of research related to software metrics. However, software metrics researchers may need to refine their empirical methodology before they can answer useful empirical questions.},
  langid = {english},
  keywords = {Empirical evaluation problems,Influential papers,Literature survey,Mapping study,Secondary study,Software metrics}
}

@inproceedings{Kiv2017116,
  type = {Conference Paper},
  title = {An Intentional Perspective on Partial Agile Adoption},
  author = {Kiv, Soreangsey and Heng, Samedi and Kolp, Manuel and Wautelet, Yves},
  year = {2017},
  series = {{{ICSOFT}} 2017 - {{Proceedings}} of the 12th {{International Conference}} on {{Software Technologies}}},
  pages = {116--127},
  doi = {10.5220/0006429301160127},
  abstract = {Nowadays, the agile paradigm is one of the most important approaches used for software development besides structured and traditional life cycles. To facilitate its adoption and minimize the risks, different meta-models have been proposed trying to unify it. Yet, very few of them have focused on one fundamental question: How to partially adopt agile methods? Intuitively, choosing which practices to adopt from agile methods should be made based on their most prioritized goals in the software development process. To answer this issue, this paper proposes a model for partial agile methods adoption based on intentional (i.e., goal) perspectives. Hence, adoption can be considered as defining the goals in the model, corresponding to the intentions of the software development team. Next, by mapping with our goal-based model, suitable practices for adoption could be easily found. Moreover, the relationship between roles and their dependencies to achieve a specific goal can also be visualized. This will help the software development team to easily identify the vulnerabilities associated with each goal and, in turn, help to minimize risks. Copyright \textcopyright{} 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Kiv2022,
  type = {Article},
  title = {Using an Ontology for Systematic Practice Adoption in Agile Methods: {{Expert}} System and Practitioners-Based Validation},
  author = {Kiv, Soreangsey and Heng, Samedi and Wautelet, Yves and Poelmans, Stephan and Kolp, Manuel},
  year = {2022},
  journal = {Expert Systems with Applications},
  volume = {195},
  doi = {10.1016/j.eswa.2022.116520},
  abstract = {As many software development teams have started to adopt agile methods, a vast amount of valuable experiences have been reported on in both academic and industrial knowledge bases. This information has been used through various approaches to guide and help practitioners finding suitable practices for their software development projects. Nevertheless, not many of these approaches could gather the available experiences to make them systematically reusable and help practitioners understanding agile practices in depth. To the best of our knowledge, only one ontology has been created to solve this problem; some limitations related to its quality and usability make it nevertheless unqualified to serve the intended purpose. The aim of this paper is to build an expert system (i.e. an evidence-based tool) to ease agile practices adoption by efficiently and effectively providing information on them. Firstly, we improve the concepts and relationships in the aforementioned ontology and theoretically validate it using a large data-set of agile practices adoption experiences collected through a Systematic Literature Review (SLR). Secondly, we develop a supporting tool having a friendly Graphical User Interface (GUI) allowing to use the ontology as a concrete agile practice knowledge provider. Finally, we empirically validate the enhanced ontology and evaluate the supporting tool using a survey with agile experts. Our supporting tool can help practitioners to decide what practice to adopt, how to adopt it, how to solve practical issues, etc. The ontology and the tool materialize our contribution to the field of systematic agile practices adoption. \textcopyright{} 2022 Elsevier Ltd},
  publication_stage = {Final},
  source = {Scopus}
}

@article{klavestad2020monitoring,
  title = {Monitoring Activities of Daily Living Using {{UWB}} Radar Technology: {{A}} Contactless Approach},
  author = {Klavestad, Sindre and Assres, Gebremariam and Fagernes, Siri and Gr{\o}nli, Tor-Morten},
  year = {2020},
  journal = {IoT},
  volume = {1},
  number = {2},
  pages = {320--336},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@inproceedings{Kokotajlo202174,
  type = {Conference Paper},
  title = {Artificial Intelligence within Agile Software Development: {{Projected}} Impacts to Cyber Offense-Defense Balance},
  author = {Kokotajlo, Timothy and Long, David and Reith, Mark and Dill, Richard},
  year = {2021},
  series = {3rd {{European Conference}} on the {{Impact}} of {{Artificial Intelligence}} and {{Robotics}}, {{ECIAIR}} 2021},
  pages = {74--81},
  doi = {10.34190/EAIR.21.030},
  abstract = {The cyber offense-defense balance theory evaluates the relative effort offense and defense forces need to prevail in conflict. Cyber conflict is a large domain, and many factors influence the offense-defense balance. Recent investments into Artificial Intelligence (A.I.) and Agile development can be used to make more accurate predictions using existing cyber offense-defense models. This paper predicts future shifts in the balance of cyber warfare caused by the current and projected capabilities of AI-augmented Agile software development pipelines. Initially, increased investments and capabilities are predicted to give an advantage to aggressing forces. Over time, however, technology advances and more standardized processes will likely create a more significant advantage for defensive capabilities. \textcopyright{} 2021 3rd European Conference on the Impact of Artificial Intelligence and Robotics, ECIAIR 2021. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Komolov2022,
  type = {Article},
  title = {Towards Predicting Architectural Design Patterns: {{A}} Machine Learning Approach},
  author = {Komolov, Sirojiddin and Dlamini, Gcinizwe and Megha, Swati and Mazzara, Manuel},
  year = {2022},
  journal = {Computers},
  volume = {11},
  number = {10},
  doi = {10.3390/computers11100151},
  abstract = {Software architecture plays an important role in software development, especially in software quality and maintenance. Understanding the impact of certain architectural patterns on software quality and verification of software requirements has become increasingly difficult with the increasing complexity of codebases in recent years. Researchers over the years have proposed automated approaches based on machine learning. However, there is a lack of benchmark datasets and more accurate machine learning (ML) approaches. This paper presents an ML-based approach for software architecture detection, namely, MVP (Model\textendash View\textendash Presenter) and MVVM (Model\textendash View\textendash ViewModel). Firstly, we present a labeled dataset that consists of 5973 data points retrieved from GitHub. Nine ML methods are applied for detection of software architecture from source code metrics. Using precision, recall, accuracy, and F1 score, the outstanding ML model performance is 83\%, 83\%, 83\%, and 83\%, respectively. The ML model's performance is validated using k-fold validation (k = 5). Our approach outperforms when compared with the state-of-the-art. \textcopyright{} 2022 by the authors.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Koontz2014780,
  type = {Conference Paper},
  title = {Apache Mission Processor Software Architecture: {{Future}} Airborne Capability Environment ({{FACE}}\texttrademark ) Considerations},
  author = {Koontz, Ronald and Johnson, Dale},
  year = {2014},
  series = {Annual {{Forum Proceedings}} - {{AHS International}}},
  volume = {1},
  pages = {780--792},
  abstract = {Open Systems Architecture (OSA) applied to rotorcraft avionics has continued to mature and evolve during the past decade. The Apache attack helicopter Mission Processor software architecture, Networked Common Operating Real-time Environment (NCORE), was originally architected to reduce life-cycle costs, minimize the time to incorporate and field new capabilities, take advantage of new software technologies and improve the organization of existing functional capabilities. The Open Group's Future Airborne Capability Environment (FACE\texttrademark ) consortium is an ongoing joint industry, academia, Department of Defense initiative that is defining a new business model and reference architecture with the objective of dramatically reducing new aviation platform development costs and enabling the leveraging of common software applications across platforms: "develop once use often". FACE technical strategies include standardizing interfaces between reference architecture segments, specifying multiple profiles and employing a shared repository based data model that facilitates application reuse across aviation platforms. After providing technical context, analysis leads to the overarching conclusion that the FACE Reference Architecture and future rotorcraft programs that leverage it can benefit from NCORE OSA proven strategies, approaches and decisions. Similarly, the NCORE OSA must continue to evolve and further align with the FACE Reference Architecture to enable both new and existing application reuse and to remain current and relevant. Copyright\textcopyright{} 2014 by the American Helicopter Society International, Inc. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Kopp2022338,
  type = {Conference Paper},
  title = {An Approach and a Software Tool for Automatic Source Code Generation Driven by Business Rules},
  author = {Kopp, Andrii and Orlovskyi, Dmytro},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3171},
  pages = {338--353},
  abstract = {This paper proposes an approach to automatic source code generation driven by business rules. This approach is inspired by low-code and automatic programming to improve the software development process and accelerate product delivery through the source code generation from natural language statements. The proposed approach considers business rules as input, uses the triplestore model for knowledge representation based on business rules, utilizes association rules to suggest attribute data types, and produces an abstract data model. This abstract data model is a framework for software components generation of various purposes and syntax, such as SQL scripts for database tables creation and Java Beans for server-side implementation. A software solution based on the proposed approach translates the data model into the source code of software components: MySQL database and Java classes. But it can be extended to generate various software components based on different syntax rules. Performed experiments demonstrate that generated software components are verified and valid since they were checked using static code analysis and dynamic testing. Conclusion formulates research outcomes, obtained results, and limitations. Future work outlines the next research steps in this field. \textcopyright{} 2022 Copyright for this paper by its authors.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{košinár2013:simulations,
  title = {Simulations of {{Agile Software Processes}} for {{Healthcare Information Systems Development Based}} on {{Machine Learning Methods}}},
  author = {Ko{\v s}in{\'a}r, Michal and {\v S}trba, Radoslav},
  year = {2013},
  month = jan,
  journal = {IFAC Proceedings Volumes},
  series = {12th {{IFAC Conference}} on {{Programmable Devices}} and {{Embedded Systems}}},
  volume = {46},
  number = {28},
  pages = {175--180},
  issn = {1474-6670},
  doi = {10.3182/20130925-3-CZ-3023.00028},
  urldate = {2023-01-15},
  abstract = {Requirements on software products are becoming more and more complicated and software systems of today are characterized by increasing complexity and size. Therefore, software systems can no longer be developed feasibly without the processes supported by appropriate methods. We propose a method for configuration and modification of software processes in companies based on gathered knowledge and our approach allows to support and optimize the processes with formal methods of modelling and machine-learning based simulations.},
  langid = {english},
  keywords = {agile methods,Discrete-Event Simulation,effort estimation,formalization,healthcare information systems,modelling,multi-agent,OWL,PROLOG,SCRUM,simulation,software process,TIL},
  file = {/Users/guru/Zotero/storage/XDIRTJQI/Košinár_Štrba_2013_Simulations of Agile Software Processes for Healthcare Information Systems_IFAC Proceedings Volumes.pdf;/Users/guru/Zotero/storage/Q3N94GXP/S1474667015373201.html}
}

@inproceedings{Košinár2013175,
  type = {Conference Paper},
  title = {Simulations of Agile Software Processes for Healthcare Information Systems Development Based on Machine Learning Methods},
  author = {Ko{\v s}in{\'a}r, Michal and {\v S}trba, Radoslav},
  year = {2013},
  series = {{{IFAC Proceedings Volumes}} ({{IFAC-PapersOnline}})},
  volume = {12},
  pages = {175--180},
  doi = {10.3182/20130925-3-CZ-3023.00028},
  abstract = {Requirements on software products are becoming more and more complicated and software systems of today are characterized by increasing complexity and size. Therefore, software systems can no longer be developed feasibly without the processes supported by appropriate methods. We propose a method for configuration and modification of software processes in companies based on gathered knowledge and our approach allows to support and optimize the processes with formal methods of modelling and machine-learning based simulations. \textcopyright{} IFAC.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{kourtesis2014:semanticbased,
  title = {Semantic-Based {{QoS}} Management in Cloud Systems: {{Current}} Status and Future Challenges},
  shorttitle = {Semantic-Based {{QoS}} Management in Cloud Systems},
  author = {Kourtesis, Dimitrios and {Alvarez-Rodr{\'i}guez}, Jose Mar{\'i}a and Paraskakis, Iraklis},
  year = {2014},
  month = mar,
  journal = {Future Generation Computer Systems},
  series = {Special {{Section}}: {{The Management}} of {{Cloud Systems}}, {{Special Section}}: {{Cyber-Physical Society}} and {{Special Section}}: {{Special Issue}} on {{Exploiting Semantic Technologies}} with {{Particularization}} on {{Linked Data}} over {{Grid}} and {{Cloud Architectures}}},
  volume = {32},
  pages = {307--323},
  issn = {0167-739X},
  doi = {10.1016/j.future.2013.10.015},
  urldate = {2023-01-15},
  abstract = {Cloud Computing and Service Oriented Architectures have seen a dramatic increase of the amount of applications, services, management platforms, data, etc. gaining momentum for the necessity of new complex methods and techniques to deal with the vast heterogeneity of data sources or services. In this sense Quality of Service (QoS) seeks for providing an intelligent environment of self-management components based on domain knowledge in which cloud components can be optimized easing the transition to an advanced governance environment. On the other hand, semantics and ontologies have emerged to afford a common and standard data model that eases the interoperability, integration and monitoring of knowledge-based systems. Taking into account the necessity of an interoperable and intelligent system to manage QoS in cloud-based systems and the emerging application of semantics in different domains, this paper reviews the main approaches for semantic-based QoS management as well as the principal methods, techniques and standards for processing and exploiting diverse data providing advanced real-time monitoring services. A semantic-based framework for QoS management is also outlined taking advantage of semantic technologies and distributed datastream processing techniques. Finally a discussion of existing efforts and challenges is also provided to suggest future directions.},
  langid = {english},
  keywords = {Big data,Cloud systems,Linked data,Ontologies,Quality of service,Semantics,Sensor data,Service oriented architectures},
  file = {/Users/guru/Zotero/storage/ENAMSAZZ/Kourtesis et al_2014_Semantic-based QoS management in cloud systems_Future Generation Computer Systems.pdf;/Users/guru/Zotero/storage/KLRY8ITU/S0167739X1300232X.html}
}

@article{Kovačević2022,
  type = {Article},
  title = {Automatic Detection of {{Long Method}} and {{God Class}} Code Smells through Neural Source Code Embeddings},
  author = {Kova{\v c}evi{\'c}, Aleksandar and Slivka, Jelena and Vidakovi{\'c}, Dragan and Gruji{\'c}, Katarina-Glorija and Luburi{\'c}, Nikola and Proki{\'c}, Simona and Sladi{\'c}, Goran},
  year = {2022},
  journal = {Expert Systems with Applications},
  volume = {204},
  doi = {10.1016/j.eswa.2022.117607},
  abstract = {Code smells are structures in code that often harm its quality. Manually detecting code smells is challenging, so researchers proposed many automatic detectors. Traditional code smell detectors employ metric-based heuristics, but researchers have recently adopted a Machine-Learning (ML) based approach. This paper compares the performance of multiple ML-based code smell detection models against multiple metric-based heuristics for detection of God Class and Long Method code smells. We assess the effectiveness of different source code representations for ML: we evaluate the effectiveness of traditionally used code metrics against code embeddings (code2vec, code2seq, and CuBERT). This study is the first to evaluate the effectiveness of pre-trained neural source code embeddings for code smell detection to the best of our knowledge. This approach helped us leverage the power of transfer learning \textendash{} our study is the first to explore whether the knowledge mined from code understanding models can be transferred to code smell detection. A secondary contribution of our research is the systematic evaluation of the effectiveness of code smell detection approaches on the same large-scale, manually labeled MLCQ dataset. Almost every study that proposes a detection approach tests this approach on the dataset unique for the study. Consequently, we cannot directly compare the reported performances to derive the best-performing approach. \textcopyright{} 2022 The Author(s)},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Kozanidis202245,
  type = {Conference Paper},
  title = {Asking about Technical Debt: {{Characteristics}} and Automatic Identification of Technical {{DebtQuestions}} on Stack Overflow},
  author = {Kozanidis, Nicholas and Verdecchia, Roberto and Guzm{\'a}n, Emitz{\'a}},
  year = {2022},
  series = {International {{Symposium}} on {{Empirical Software Engineering}} and {{Measurement}}},
  pages = {45--56},
  doi = {10.1145/3544902.3546245},
  abstract = {Background: Q\&A sites allow to study how users reference and request support on technical debt. To date only few studies, focusing on narrow aspects, investigate technical debt on Stack Overflow. Aims: We aim at gaining an in-depth understanding on the characteristics of technical debt questions on Stack Overflow. In addition, we assessifidentificationstrategiesbasedonmachinelearning canbe used to automatically identify and classify technical debt questions. Method: We use automated and manual processes to identify technical debt questions on Stack Overflow. The final set of 415 questions is analyzed to study (i) technical debt types, (ii) question length, (iii) perceived urgency, (iv) sentiment, and (v) themes. Natural language processing and machine learning techniques are used to assess if questions can be identified and classified automatically. Results: Architecture debt is the most recurring debt type, followed by code and design debt. Most questions display mild urgency, with frequency of higher urgency steadily declining as urgency rises. Question length varies across debt types. Sentiment ismostly neutral. 29 recurrentthemes emerge.Machinelearning can beusedtoidentify technical debt questions and binary urgency, but not debt types. Conclusions: Different patterns emerge from the analysis of technical debt questions on Stack Overflow. The results provide further insights on the phenomenon, and support the adoption of a more comprehensive strategy to identify technical debt questions. \textcopyright{} 2022 Association for Computing Machinery.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{krasteva2020:adopting,
  title = {Adopting {{Agile Software Development}} Methodologies in Big Data Projects--a Systematic Literature Review of Experience Reports},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Krasteva, Iva and Ilieva, Sylvia},
  year = {2020},
  pages = {2028--2033},
  publisher = {{IEEE}}
}

@article{Krishna20191081,
  type = {Article},
  title = {Bellwethers: {{A}} Baseline Method for Transfer Learning},
  author = {Krishna, Rahul and Menzies, Tim},
  year = {2019},
  journal = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {11},
  pages = {1081--1105},
  doi = {10.1109/TSE.2018.2821670},
  abstract = {Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of 'bellwethers': given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared. \textcopyright{} 1976-2012 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{kruchten2019:end,
  title = {The End of Agile as We Know It},
  booktitle = {Proceedings of the {{International Conference}} on {{Software}} and {{System Processes}}},
  author = {Kruchten, Philippe},
  year = {2019},
  month = may,
  series = {{{ICSSP}} '19},
  pages = {104},
  publisher = {{IEEE Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1109/ICSSP.2019.00033},
  urldate = {2023-01-15},
  abstract = {It's been 20 years or so, but the end is in sight. We have successfully placed the adjective 'agile' in front of about every important noun in our software development / IT world: agile design, agile testing, agile management, agile database, agile architecture, agile user-interaction.... Agile has won the war. "What is next?" is the question I've been asked again and again. What is the future of software engineering? The next best thing? Is it DevOps, cloud-something, micro-services, AI? The adjective agile has lost some of its weight and novelty, only a few laggards are still asking "what is it?" It is time to reflect on the fundamental aspects of agility: what does it really means, what are the fundamental principles behind it, that made its successes. The agile movement has had some tremendous impact in the way we work, putting the human being and human interaction more central in these processes, by using extensively iterations, direct interactions, and feedback loops. But at the same time, some aspects of agile have become dogmatic, fossilized, and the agile movement has not been always very agile in its application to itself. These dogmatic aspects have slowed the expansion of its own principles to some of the more complex or much larger software development endeavours. Now, the increasing need for speed, the availability of opensource software repositories, the shifts in technology, such as the cloud, the emergence of software ecosystems are creating new needs in terms of process and project management, that can exploit the fundamental principles of agile, beyond the dogma of this or that method, this or that practice. As the amount of software in use is growing and will outgrow the capacity of our industry to maintain and evolve it, the industry faces a massive amount of technical debt, which we do not know well how to mitigate or repay. Agile has been very valuable, but once its lessons are fully integrated in the way we work we have to look beyond and stop repeating it like a mantra. Agile is dead. Long live agility.}
}

@article{Kundisch2022421,
  type = {Article},
  title = {An Update for Taxonomy Designers: {{Methodological}} Guidance from Information Systems Research},
  author = {Kundisch, Dennis and Muntermann, Jan and Oberl{\"a}nder, Anna Maria and Rau, Daniel and R{\"o}glinger, Maximilian and Schoormann, Thorsten and Szopinski, Daniel},
  year = {2022},
  journal = {Business and Information Systems Engineering},
  volume = {64},
  number = {4},
  pages = {421--439},
  doi = {10.1007/s12599-021-00723-x},
  abstract = {Taxonomies are classification systems that help researchers conceptualize phenomena based on their dimensions and characteristics. To address the problem of `ad-hoc' taxonomy building, Nickerson et al. (2013) proposed a rigorous taxonomy development method for information systems researchers. Eight years on, however, the status quo of taxonomy research shows that the application of this method lacks consistency and transparency and that further guidance on taxonomy evaluation is needed. To fill these gaps, this study (1) advances existing methodological guidance and (2) extends this guidance with regards to taxonomy evaluation. Informed by insights gained from an analysis of 164 taxonomy articles published in information systems outlets, this study presents an extended taxonomy design process together with 26 operational taxonomy design recommendations. Representing an update for taxonomy designers, it contributes to the prescriptive knowledge on taxonomy design and seeks to augment both rigorous taxonomy building and evaluation. \textcopyright{} 2021, The Author(s).},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Kurrek2020,
  type = {Article},
  title = {Q-{{Model}}: {{An}} Artificial Intelligence Based Methodology for the Development of Autonomous Robots},
  author = {Kurrek, Philip and Zoghlami, Firas and Jocas, Mark and Stoelen, Martin and Salehi, Vahid},
  year = {2020},
  journal = {Journal of Computing and Information Science in Engineering},
  volume = {20},
  number = {6},
  doi = {10.1115/1.4046992},
  abstract = {The increasing individualization of products reinforces the importance of decoupled factories in production processes. Artificial intelligence (AI) is a recognized technology for problem solving and accelerates automation by enabling systems to act independently. In the field of robotics, there are new deep learning approaches which make robotic control systems human independent. This work provides a literature overview of the current state of development methodologies, showing that there are only limited methods available for the development of artificial intelligent robots. We present a novel development methodology based on artificial intelligence, particularly deep reinforcement learning. The so-called Q-model can enable robots to learn specific tasks independently. In summary, we show how an AI-based methodology assists the development of autonomous robots along the product lifecycle. Copyright \textcopyright{} 2020 by ASME},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{kusmenko2019:engineering,
  title = {On the {{Engineering}} of {{AI-Powered Systems}}},
  booktitle = {2019 34th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering Workshop}} ({{ASEW}})},
  author = {Kusmenko, Evgeny and Pavlitskaya, Svetlana and Rumpe, Bernhard and St{\"u}ber, Sebastian},
  year = {2019},
  month = nov,
  pages = {126--133},
  issn = {2151-0830},
  doi = {10.1109/ASEW.2019.00042},
  abstract = {More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.},
  keywords = {Automation,Codes,Deep learning,MDE; deep learning; neural networks,Neural networks,Ports (computers),Tagging,Training},
  file = {/Users/guru/Zotero/storage/DDJQCW2T/Kusmenko et al. - 2019 - On the Engineering of AI-Powered Systems.pdf;/Users/guru/Zotero/storage/ZT7SMM7I/8967413.html}
}

@inproceedings{kusmenko2019:engineeringa,
  title = {On the Engineering of {{AI-Powered}} Systems},
  booktitle = {2019 34th {{IEEE}}/{{ACM}} International Conference on Automated Software Engineering Workshop ({{ASEW}})},
  author = {Kusmenko, Evgeny and Pavlitskaya, Svetlana and Rumpe, Bernhard and St{\"u}ber, Sebastian},
  year = {2019},
  month = nov,
  pages = {126--133},
  issn = {2151-0830},
  doi = {10.1109/ASEW.2019.00042},
  abstract = {More and more tasks become solvable using deep learning technology nowadays. Consequently, the amount of neural network code in software rises continuously. To make the new paradigm more accessible, frameworks, languages, and tools keep emerging. Although, the maturity of these tools is steadily increasing, we still lack appropriate domain specific languages and a high degree of automation when it comes to deep learning for productive systems. In this paper we present a multi-paradigm language family allowing the AI engineer to model and train deep neural networks as well as to integrate them into software architectures containing classical code. Using input and output layers as strictly typed interfaces enables a seamless embedding of neural networks into component-based models. The lifecycle of deep learning components can then be governed by a compiler accordingly, e.g. detecting when (re-)training is necessary or when network weights can be shared between different network instances. We provide a compelling case study, where we train an autonomous vehicle for the TORCS simulator. Furthermore, we discuss how the methodology automates the AI development process if neural networks are changed or added to the system.},
  keywords = {Automation,Codes,deep learning,Deep learning,MDE,neural networks,Neural networks,Ports (computers),primary,Tagging,Training},
  file = {/Users/guru/Zotero/storage/LZ88RQBP/Kusmenko et al_2019_On the engineering of AI-Powered systems_2019 34th IEEEACM international conference on automated software engineering workshop (ASEW).pdf}
}

@inproceedings{kusmenko2019engineering,
  title = {On the Engineering of {{AI-powered}} Systems},
  booktitle = {2019 34th {{IEEE}}/{{ACM}} International Conference on Automated Software Engineering Workshop ({{ASEW}})},
  author = {Kusmenko, Evgeny and Pavlitskaya, Svetlana and Rumpe, Bernhard and St{\"u}ber, Sebastian},
  year = {2019},
  pages = {126--133},
  organization = {{IEEE}}
}

@inproceedings{Laato2022113,
  type = {Conference Paper},
  title = {{{AI}} Governance in the System Development Life Cycle: {{Insights}} on Responsible Machine Learning Engineering},
  author = {Laato, Samuli and Birkstedt, Teemu and Mantymaki, Matti and Minkkinen, Matti and Mikkonen, Tommi},
  year = {2022},
  series = {Proceedings - 1st {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}, {{CAIN}} 2022},
  pages = {113--123},
  doi = {10.1145/3522664.3528598},
  abstract = {In this study we explore the incorporation of artificial intelligence (AI) governance to system development life cycle (SDLC) models. We conducted expert interviews among AI and SDLC professionals and analyzed the interview data using qualitative coding and clustering to extract AI governance concepts. Subsequently, we mapped these concepts onto three stages in the machine learning (ML) system development process: (1) design, (2) development, and (3) operation. We discovered 20 governance concepts, some of which are relevant to more than one of the three stages. Our analysis highlights AI governance as a complex process that involves multiple activities and stakeholders. As development projects are unique, the governance requirements and processes also vary. This study is a step towards understanding how AI governance is conceptually connected to ML systems' management processes through the project life cycle. CCS CONCEPTS \textbullet{} Software and its engineering Software creation and management. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Latinovic2021146,
  type = {Conference Paper},
  title = {Automation and Artificial Intelligence in Software Engineering: {{Experiences}}, Challenges, and Opportunities},
  author = {Latinovic, Milan and {Pammer-Schindler}, Viktoria},
  year = {2021},
  series = {Proceedings of the {{Annual Hawaii International Conference}} on {{System Sciences}}},
  volume = {2020-January},
  pages = {146--155},
  abstract = {Automation and Artificial Intelligence have a transformative influence on many sectors, and software engineers are the actors who engineer this transformation. On the other hand, there is little knowledge of how automation and Artificial Intelligence impact software engineering practice. To answer this question, we conducted semi-structured interviews with experienced software practitioners across frontend and backend development, DevOps, R\&D, integration, and leadership positions. Our findings reveal 1) automation to appear as micro-automation in the sense of automation of tiny and specific tasks, 2) automation as a side product of work, and bottom-up driven in software engineering, and 3) automation as a possible cause for cognitive overhead due to automatically generated notifications. Furthermore, we notice that our interview participants do not expect automation and artificial intelligence tools to change software engineering's essence in the foreseeable future substantially. \textcopyright{} 2021 IEEE Computer Society. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{lee2006applying,
  title = {Applying {{TQM}}, {{CMM}} and {{ISO}} 9001 in Knowledge Management for Software Development Process Improvement},
  author = {Lee, Ming-Chang and Chang, To},
  year = {2006},
  journal = {International Journal of Services and Standards},
  volume = {2},
  number = {1},
  pages = {101--115},
  publisher = {{Inderscience Publishers}}
}

@article{Lee2021,
  type = {Article},
  title = {Understanding Digital Transformation in Advanced Manufacturing and Engineering: {{A}} Bibliometric Analysis, Topic Modeling and Research Trend Discovery},
  author = {Lee, Ching-Hung and Liu, Chien-Liang and Trappey, Amy J.C. and Mo, John P.T. and Desouza, Kevin C.},
  year = {2021},
  journal = {Advanced Engineering Informatics},
  volume = {50},
  doi = {10.1016/j.aei.2021.101428},
  abstract = {Digital transformation (DT) is the process of combining digital technologies with sound business models to generate great value for enterprises. DT intertwines with customer requirements, domain knowledge, and theoretical and empirical insights for value propagations. Studies of DT are growing rapidly and heterogeneously, covering the aspects of product design, engineering, production, and life-cycle management due to the fast and market-driven industrial development under Industry 4.0. Our work addresses the challenge of understanding DT trends by presenting a machine learning (ML) approach for topic modeling to review and analyze advanced DT technology research and development. A systematic review process is developed based on the comprehensive DT in manufacturing systems and engineering literature (i.e., 99 articles). Six dominant topics are identified, namely smart factory, sustainability and product-service systems, construction digital transformation, public infrastructure-centric digital transformation, techno-centric digital transformation, and business model-centric digital transformation. The study also contributes to adopting and demonstrating the ML-based topic modeling for intelligent and systematic bibliometric analysis, particularly for unveiling advanced engineering research trends through domain literature. \textcopyright{} 2021},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Lee2022,
  type = {Article},
  title = {Motivating Members' Involvement to Effectually Conduct Collaborative Software Process Tailoring},
  author = {Lee, Jung-Chieh and Chen, Chung-Yang},
  year = {2022},
  journal = {Empirical Software Engineering},
  volume = {27},
  number = {7},
  doi = {10.1007/s10664-022-10225-3},
  abstract = {Contemporary business and software environments are highly competitive and rapidly evolving, resulting in software projects that are highly customized and changeable during development. Therefore, software process tailoring (SPT) is important as software teams conduct SPT to adjust shared development processes and evolve the project to better meet unique and dynamic needs. SPT is a special type of teamwork in which members' active participation and critical input are necessary for understanding and synthesizing various business and technical concerns that may be divergent and conflictual and then jointly identifying an integrated tailoring solution. In this context, this study examines members' decisive and critical involvement in SPT and adopts a motivational perspective to explore how motivation can facilitate SPT performance. Specifically, we use empowerment theory to develop a model to theorize and examine how psychological empowerment (PE) in terms of meaningfulness, autonomy, potency, and impact motivates software teams to efficiently and effectively conduct SPT. The model also considers the power distance (PD) to understand how it functions in team-based critical thinking and decisional processes to energize team members' participative effort. The investigation surveyed 102 software development teams and used partial least squares (PLS) to analyze the data. The results show that PE in terms of the four components has various influences on SPT performance and that PD has nonsignificant moderating effects. This study contributes to the software engineering literature by uncovering the contextual mechanism underlying the relationship between PE and PD in SPT. The limitations and possible extensions of this study are also outlined for future research. \textcopyright{} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{leitão2015:chapter,
  title = {Chapter 22 - {{A Survey}} on {{Factors}} That {{Impact Industrial Agent Acceptance}}},
  booktitle = {Industrial {{Agents}}},
  author = {Leit{\~a}o, Paulo and Karnouskos, Stamatis},
  editor = {Leit{\~a}o, Paulo and Karnouskos, Stamatis},
  year = {2015},
  month = jan,
  pages = {401--429},
  publisher = {{Morgan Kaufmann}},
  address = {{Boston}},
  doi = {10.1016/B978-0-12-800341-1.00022-X},
  urldate = {2023-01-15},
  abstract = {Although the agent technology has been with us some decades now, its acceptance in industry is still limited. As such, we conducted a survey and investigated the main factors that impact it. Thus, key industrial agent aspects are investigated\textemdash i.e., design, technology, intelligence/algorithms, standardization, hardware, challenges, applications, and cost. The results are analyzed and discussed, confirming that a decision on agent utilization in productive industrial systems is a complex undertaking, and there are still many issues to be resolved in order to lead to a wider acceptance of industrial agents.},
  isbn = {978-0-12-800341-1},
  langid = {english},
  keywords = {Industrial agent acceptance,Industrial agent applications,Industrial agent challenges,Industrial agent cost,industrial agent design,Industrial agent hardware,Industrial agent intelligence/algorithms,Industrial agent standardization,Industrial agent technology,Industrial agents,Survey},
  file = {/Users/guru/Zotero/storage/6SYJPE26/Leitão_Karnouskos_2015_Chapter 22 - A Survey on Factors that Impact Industrial Agent Acceptance_Industrial Agents.pdf;/Users/guru/Zotero/storage/7PDZGDRI/B978012800341100022X.html}
}

@article{leitão2015:intelligent,
  title = {Intelligent Products: {{The}} Grace Experience},
  shorttitle = {Intelligent Products},
  author = {Leit{\~a}o, Paulo and Rodrigues, Nelson and Barbosa, Jos{\'e} and Turrin, Claudio and Pagani, Arnaldo},
  year = {2015},
  month = sep,
  journal = {Control Engineering Practice},
  volume = {42},
  pages = {95--105},
  issn = {0967-0661},
  doi = {10.1016/j.conengprac.2015.05.001},
  urldate = {2023-01-15},
  abstract = {Product intelligence is a new industrial manufacturing control paradigm aligned with the context of cyber-physical systems and addressing the current requirements of flexibility, reconfigurability and responsiveness. This paradigm introduces benefits in terms of improvement of the entire product׳s life-cycle, and particularly the product quality and customization, aiming the customer satisfaction. This paper presents an implementation of a system of intelligent products, developed under the scope of the GRACE project, where an agent-based solution was deployed in a factory plant producing laundry washing machines. The achieved results show an increase of the production and energy efficiency, an increase of the product quality and customization, as well as a reduction of the scrap costs.},
  langid = {english},
  keywords = {Factory automation,Intelligent and reconfigurable control,Intelligent products,Multi-agent systems},
  file = {/Users/guru/Zotero/storage/XYCBXT2T/Leitão et al_2015_Intelligent products_Control Engineering Practice.pdf;/Users/guru/Zotero/storage/UR5T9LPE/S096706611500091X.html}
}

@inproceedings{lenarduzzi2021software,
  title = {Software Quality for Ai: {{Where}} We Are Now?},
  booktitle = {International Conference on Software Quality},
  author = {Lenarduzzi, Valentina and Lomio, Francesco and Moreschini, Sergio and Taibi, Davide and Tamburri, Damian Andrew},
  year = {2021},
  pages = {43--53},
  organization = {{Springer}}
}

@inproceedings{lewis2021software,
  title = {Software Architecture Challenges for {{ML}} Systems},
  booktitle = {2021 {{IEEE}} International Conference on Software Maintenance and Evolution ({{ICSME}})},
  author = {Lewis, Grace A and Ozkaya, Ipek and Xu, Xiwei},
  year = {2021},
  pages = {634--638},
  organization = {{IEEE}}
}

@article{li2020:extensive,
  title = {An Extensive Study of Class-Level and Method-Level Test Case Selection for Continuous Integration},
  author = {Li, Yingling and Wang, Junjie and Yang, Yun and Wang, Qing},
  year = {2020},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {167},
  pages = {110614},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2020.110614},
  urldate = {2023-01-15},
  abstract = {Continuous Integration (CI) is an important practice in agile development. With the growth of integration system, running all tests to verify the quality of submitted code, is clearly uneconomical. This paper aims at selecting a proper test subset for continuous integration so as to reduce test cost as much as possible without sacrificing quality. We first propose a static test case selection framework Sapient which aims at selecting a precise test subset towards fully covering all changed code and affected code. There are four major steps: locate changes, build dependency graphs, identify related tests by searching dependency graphs, and extend tests. Based on Sapient, we then develop a class-level test case selection approach FEST and a method-level approach MEST. FEST captures the class-level dependencies, especially the hidden references, which facilitates the modeling of full dependency relations at the class level. MEST combines two dynamic execution rules (i.e., dynamic invocation in reflection and dynamic binding in inheritance) with static dependencies to support building the precise method-level dependencies. Evaluation is conducted on 18 open source projects with 261 continuous integration versions from Eclipse and Apache communities. Results show that both FEST and MEST can detect almost all faults of two baselines (i.e., actual CI testing and ClassSRTS), and find new faults compared with actual CI testing (in 25\% and 26\% versions) and ClassSRTS (in 18\% and 27\% versions) respectively. Furthermore, MEST outperforms FEST in reduced test size, fault detection efficiency and test cost, indicating method-level test case selection can be more effective than class-level selection. This study can further speed up the feedback and improve the fault detection efficiency of CI testing, and has good application prospects for the CI testing in large-scale and complex systems.},
  langid = {english},
  keywords = {Class-level test case selection,Continuous integration,Dynamic execution rules,Method-level test case selection,Program dependencies,Test case selection}
}

@inproceedings{Li202116,
  type = {Conference Paper},
  title = {Stop Building Castles on a Swamp! {{The}} Crisis of Reproducing Automatic Search in Evidence-Based Software Engineering},
  author = {Li, Zheng},
  year = {2021},
  series = {Proceedings - {{International Conference}} on {{Software Engineering}}},
  pages = {16--20},
  doi = {10.1109/ICSE-NIER52604.2021.00012},
  abstract = {The evidence-based approach has increasingly been employed to synthesize empirical findings from the primary research in software engineering. Nevertheless, the reproducibility of evidence-based software engineering (EBSE) studies seems to be underemphasized. In our investigation into the automatic search of 311 sample studies, more than 50\% of the search strings are not reusable; about 87.5\% of the search activities (e.g., search field settings) are unrepeatable; and more than 95\% of the whole automatic search implementations are unreproducible. Considering that searching is a cornerstone of an EBSE study, we are afraid that the reproducibility of the current secondary research could be worse than we can imagine. By analyzing and reporting the root causes of the aforementioned observations, we urge collaboration and cooperation among all the stakeholders in our community to improve the research reproducibility in EBSE. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Li2022,
  type = {Article},
  title = {Automatically Classifying Non-Functional Requirements Using Deep Neural Network},
  author = {Li, Bing and Nong, Xiuwen},
  year = {2022},
  journal = {Pattern Recognition},
  volume = {132},
  doi = {10.1016/j.patcog.2022.108948},
  abstract = {Non-functional requirements are property that software products must have in order to meet the user's business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss. \textcopyright{} 2022},
  publication_stage = {Final},
  source = {Scopus}
}

@article{li2022:automatically,
  title = {Automatically Classifying Non-Functional Requirements Using Deep Neural Network},
  author = {Li, Bing and Nong, Xiuwen},
  year = {2022},
  month = dec,
  journal = {Pattern Recognition},
  volume = {132},
  pages = {108948},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2022.108948},
  urldate = {2023-01-15},
  abstract = {Non-functional requirements are property that software products must have in order to meet the user's business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.},
  langid = {english},
  keywords = {BERT,Bi-LSTM,Multi-sample dropout,N-gram,Non-functional requirements,Non-functional requirements classification},
  file = {/Users/guru/Zotero/storage/P9N5JURJ/S0031320322004289.html}
}

@article{li2022:framework,
  title = {The {{Framework}} of a {{Tiktok-Style Education APP}} for {{ISO Geometrical Production Specification}}},
  author = {Li, Tukun and Gai, Qiuyan and Kong, Chao and Song, Kang and Xu, Yuanping and Luo, Junding and Fu, Hongsheng and {Fillery-Travis}, Annette},
  year = {2022},
  month = jan,
  journal = {Procedia CIRP},
  series = {17th {{CIRP Conference}} on {{Computer Aided Tolerancing}} ({{CAT2022}})},
  volume = {114},
  pages = {25--29},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2022.10.005},
  urldate = {2023-01-15},
  abstract = {In the movement toward industry 4.0, there is a significant change in ISO Geometrical Product Specification and Verification standards (GPS), The latest version of GPS empowers an engineer to describe his/her requirements precisely and unambiguously. However, it also considerably increases the difficulty of ISO GPS education. To this end, the paper proposes an AI-based learning method through a Tiktok-Style education app for vocational training, specifically in the delivery of ISO GPS. Tiktok is a short-form video-sharing app empowered by machine learning. Based on the key concepts learned from Tiktok, the framework of an education APP for ISO GPS, namely GPS-SV, is developed. The framework is detailed and illustrated via a case study.},
  langid = {english},
  keywords = {AI,ISO GPS,knowledge map,semantic web,Vocational Training},
  file = {/Users/guru/Zotero/storage/JYKS8QYP/Li et al_2022_The Framework of a Tiktok-Style Education APP for ISO Geometrical Production_Procedia CIRP.pdf;/Users/guru/Zotero/storage/9CJLGNDJ/S2212827122014299.html}
}

@inproceedings{Lin2014,
  type = {Conference Paper},
  title = {Using Goal Net to Model User Stories in Agile Software Development},
  author = {Lin, Jun and Yu, Han and Shen, Zhiqi and Miao, Chunyan},
  year = {2014},
  series = {2014 {{IEEE}}/{{ACIS}} 15th {{International Conference}} on {{Software Engineering}}, {{Artificial Intelligence}}, {{Networking}} and {{Parallel}}/{{Distributed Computing}}, {{SNPD}} 2014 - {{Proceedings}}},
  doi = {10.1109/SNPD.2014.6888731},
  abstract = {Agile methodologies use user stories to capture software requirements. This often results in team members over emphasizing their understanding of the goals, without proper incorporation of goals from other stakeholders or customers. Existing UML or other goal oriented modeling methods tend to be overly complex for non-technical stakeholders to properly express their goals and communicate them to the agile team. In this paper, we propose a light weight Goal Net based method to model goal requirements in agile software development process to address this problem. It can be used to decompose complex processes into phased goals, and model low level user stories to high level hierarchy goal structures. Our preliminary analysis and studies in educational software engineering contexts show that it can improve agile team's group awareness to project goals and, thus, improve team productivity and artifact quality. The proposed approach was evaluated in university level agile software engineering projects. It has achieved an improvement of over 50 percentage points in terms of the proportion of high quality user stories generated by students compared to the standard user story template used in Scrum, \textcopyright{} 2014 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{linder2012:design,
  title = {The Design Evolution of {{LuminAR}}: A Compact and Kinetic Projected Augmented Reality Interface},
  shorttitle = {The Design Evolution of {{LuminAR}}},
  booktitle = {{{CHI}} '12 {{Extended Abstracts}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Linder, Natan and Maes, Pattie},
  year = {2012},
  month = may,
  series = {{{CHI EA}} '12},
  pages = {1435--1436},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2212776.2212473},
  urldate = {2023-01-15},
  abstract = {LuminAR is a new form factor for a compact and kinetic projected augmented reality interface. This video presents the design evolution iterations of the LuminAR prototypes. In this video we document LuminAR's design process, hardware and software implementation and demonstrate new kinetic interaction techniques. The work presented is motivated through a set of applications that explore scenarios for interactive and kinetic projected augmented reality interfaces. It also opens the door for further explorations of kinetic interaction and promotes the adoption of projected augmented reality as a commonplace user interface modality.},
  isbn = {978-1-4503-1016-1},
  keywords = {augmented reality,kinetic interfaces,tangible computing},
  file = {/Users/guru/Zotero/storage/HY9CPH9B/Linder_Maes_2012_The design evolution of LuminAR_CHI '12 Extended Abstracts on Human Factors in Computing Systems.pdf}
}

@article{liu2021:formal,
  title = {A Formal Specification Animation Method for Operation Validation},
  author = {Liu, Shaoying and Miao, Weikai},
  year = {2021},
  month = aug,
  journal = {Journal of Systems and Software},
  volume = {178},
  pages = {110948},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.110948},
  urldate = {2023-01-15},
  abstract = {Formal specification can benefit software quality by precisely defining the behaviors of operations to prevent primary mistakes in the early phase of software projects, but a remaining challenge is how such a specification can be checked comprehensibly to show whether it satisfies the user's perception of requirements. In this paper, we describe a new technique for animating operation specifications as a means to address this problem. The technique offers new ways to do (1) automatic animation data generation for both input and output of an operation based on pre- and post-conditions, (2) visualized demonstration of the relationships between input and the corresponding output, (3) comprehensible animation of data items, and (4) illustrative animation of logical expressions and the operators used in them. We discuss these issues and present a prototype tool that supports the automation of the proposed technique. We also report an industrial application as a trial experiment to validate the technique. Finally, we conclude the paper and point out future research directions.},
  langid = {english},
  keywords = {Formal specification,Specification animation,Verification and validation},
  file = {/Users/guru/Zotero/storage/7T3RDRI3/S0164121221000455.html}
}

@article{liu2021:threestep,
  title = {A Three-Step Hybrid Specification Approach to Error Prevention},
  author = {Liu, Shaoying},
  year = {2021},
  month = aug,
  journal = {Journal of Systems and Software},
  volume = {178},
  pages = {110975},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.110975},
  urldate = {2023-01-15},
  abstract = {Effectively preventing errors in requirements analysis and design is extremely important for enhancing software productivity and reliability, but how to fulfill this goal remains an open problem. In this paper, we propose a concept of hybrid specification and describe a novel three-step hybrid specification approach to address this problem. We discuss how the three-step approach can be used to effectively prevent errors in the early phases of development. The expected effect of the approach is to strike a good balance between enhancing productivity and ensuring the reliability of the program implemented. We present a controlled experiment to evaluate the effectiveness of the approach. The result of the experiment shows that our method can detect and prevent 28.36\% more errors than a comparable traditional requirements analysis method.},
  langid = {english},
  keywords = {Formal methods,Hybrid specification,Requirements analysis,Software productivity,Software reliability},
  file = {/Users/guru/Zotero/storage/JU7EDZ7N/S0164121221000728.html}
}

@article{liyanage2022:survey,
  title = {A Survey on {{Zero}} Touch Network and {{Service Management}} ({{ZSM}}) for {{5G}} and beyond Networks},
  author = {Liyanage, Madhusanka and Pham, Quoc-Viet and Dev, Kapal and Bhattacharya, Sweta and Maddikunta, Praveen Kumar Reddy and Gadekallu, Thippa Reddy and Yenduri, Gokul},
  year = {2022},
  month = jul,
  journal = {Journal of Network and Computer Applications},
  volume = {203},
  pages = {103362},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2022.103362},
  urldate = {2023-01-15},
  abstract = {Faced with the rapid increase in smart Internet-of-Things (IoT) devices and the high demand for new business-oriented services in the fifth-generation (5G) and beyond network, the management of mobile networks is getting complex. Thus, traditional Network Management and Orchestration (MANO) approaches cannot keep up with rapidly evolving application requirements. This challenge has motivated the adoption of the Zero-touch network and Service Management (ZSM) concept to adapt the automation into network services management. By automating network and service management, ZSM offers efficiency to control network resources and enhance network performance visibility. The ultimate target of the ZSM concept is to enable an autonomous network system capable of self-configuration, self-monitoring, self-healing, and self-optimization based on service-level policies and rules without human intervention. Thus, the paper focuses on conducting a comprehensive survey of E2E ZSM architecture and solutions for 5G and beyond networks. The article begins by presenting the fundamental ZSM architecture and its essential components and interfaces. Then, a comprehensive review of the state-of-the-art for key technical areas, i.e., ZSM automation, cross-domain E2E service lifecycle management, and security aspects, are presented. Furthermore, the paper contains a summary of recent standardization efforts and research projects towards the ZSM realization in 5G and beyond networks. Finally, several lessons learned from the literature and open research problems related to ZSM realization are also discussed in this paper.},
  langid = {english},
  keywords = {5G,6G,Artificial intelligence,Automation,Machine learning,Orchestration,Security,Service management,Zero-touch network and Service Management},
  file = {/Users/guru/Zotero/storage/93HXIGXQ/S1084804522000297.html}
}

@article{lópez2022:quality,
  title = {Quality Measurement in Agile and Rapid Software Development: {{A}} Systematic Mapping},
  shorttitle = {Quality Measurement in Agile and Rapid Software Development},
  author = {L{\'o}pez, Lidia and Burgu{\'e}s, Xavier and {Mart{\'i}nez-Fern{\'a}ndez}, Silverio and Vollmer, Anna Maria and Behutiye, Woubshet and Karhap{\"a}{\"a}, Pertti and Franch, Xavier and Rodr{\'i}guez, Pilar and Oivo, Markku},
  year = {2022},
  month = apr,
  journal = {Journal of Systems and Software},
  volume = {186},
  pages = {111187},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111187},
  urldate = {2023-01-15},
  abstract = {Context: In despite of agile and rapid software development (ARSD) being researched and applied extensively, managing quality requirements (QRs) are still challenging. As ARSD processes produce a large amount of data, measurement has become a strategy to facilitate QR management. Objective: This study aims to survey the literature related to QR management through metrics in ARSD, focusing on: bibliometrics, QR metrics, and quality-related indicators used in quality management. Methods: The study design includes the definition of research questions, selection criteria, and snowballing as search strategy. Results: We selected 61 primary studies (2001\textendash 2019). Despite a large body of knowledge and standards, there is no consensus regarding QR measurement. Terminology is varying as are the measuring models. However, seemingly different measurement models do contain similarities. Conclusion: The industrial relevance of the primary studies shows that practitioners have a need to improve quality measurement. Our collection of measures and data sources can serve as a starting point for practitioners to include quality measurement into their decision-making processes. Researchers could benefit from the identified similarities to start building a common framework for quality measurement. In addition, this could help researchers identify what quality aspects need more focus, e.g., security and usability that have surprisingly few metrics reported.},
  langid = {english},
  keywords = {Agile software development,Metrics,Non-functional requirements,Quality indicators,Quality requirements,Rapid software development},
  file = {/Users/guru/Zotero/storage/F2C7S585/López et al_2022_Quality measurement in agile and rapid software development_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/VAYNA4D2/S0164121221002661.html}
}

@inproceedings{Lu2022101,
  type = {Conference Paper},
  title = {Towards a Roadmap on Software Engineering for Responsible {{AI}}},
  author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Xing, Zhenchang},
  year = {2022},
  series = {Proceedings - 1st {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}, {{CAIN}} 2022},
  pages = {101--112},
  doi = {10.1145/3522664.3528607},
  abstract = {Although AI is transforming the world, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible AI have been issued recently. However, they are high level and difficult to put into practice. On the other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible AI systems, (ii) setting up the development processes incorporating process-oriented practices for responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques. CCS CONCEPTS \textbullet{} Software and its engineering; \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/LVWPQ9SY/3522664.3528607.pdf}
}

@inproceedings{Luo2022355,
  type = {Conference Paper},
  title = {The Power of Prediction: {{Microservice}} Auto Scaling via Workload Learning},
  author = {Luo, Shutian and Xu, Huanle and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and Yang, Guodong and Xu, Chengzhong},
  year = {2022},
  series = {{{SoCC}} 2022 - {{Proceedings}} of the 13th {{Symposium}} on {{Cloud Computing}}},
  pages = {355--369},
  doi = {10.1145/3542929.3563477},
  abstract = {When deploying microservices in production clusters, it is critical to automatically scale containers to improve cluster utilization and ensure service level agreements (SLA). Although reactive scaling approaches work well for monolithic architectures, they are not necessarily suitable for microservice frameworks due to the long delay caused by complex microservice call chains. In contrast, existing proactive approaches leverage end-to-end performance prediction for scaling, but cannot effectively handle microservice multiplexing and dynamic microservice dependencies. In this paper, we present Madu, a proactive microservice auto-scaler that scales containers based on predictions for individual microservices. Madu learns workload uncertainty to handle the highly dynamic dependency between microservices. Additionally, Madu adopts OS-level metrics to optimize resource usage while maintaining good control over scaling overhead. Experiments on large-scale deployments of microservices in Alibaba clusters show that the overall prediction accuracy of Madu can reach as high as 92.3\% on average, which is 13\% higher than the state-of-the-art approaches. Furthermore, experiments running real-world microservice benchmarks in a local cluster of 20 servers show that Madu can reduce the overall resource usage by 1.7X compared to reactive solutions, while reducing end-to-end service latency by 50\%. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Luo20223901,
  type = {Article},
  title = {An In-Depth Study of Microservice Call Graph and Runtime Performance},
  author = {Luo, Shutian and Xu, Huanle and Lu, Chengzhi and Ye, Kejiang and Xu, Guoyao and Zhang, Liping and He, Jian and Xu, Chengzhong},
  year = {2022},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {33},
  number = {12},
  pages = {3901--3914},
  doi = {10.1109/TPDS.2022.3174631},
  abstract = {Loosely-coupled and light-weight microservices running in containers are replacing monolithic applications gradually. Understanding the characteristics of microservices is critical to make good use of microservice architectures. However, there is no comprehensive study about microservice and its related systems in production environments so far. In this paper, we present a solid analysis of large-scale deployments of microservices at Alibaba clusters. Our study focuses on the characterization of microservice dependency as well as its runtime performance. We conduct an in-depth anatomy of microservice call graphs to quantify the difference between them and traditional DAGs of data-parallel jobs. In particular, we observe that microservice call graphs are heavy-tail distributed and their topology is similar to a tree and moreover, many microservices are hot-spots. We also discover that the structure of call graphs for long-term developed applications is much simpler so as to provide better performance. Our investigation on microservice runtime performance indicates most microservices are much more sensitive to CPU interference than memory interference. Moreover, we design resource management policies to efficiently tune memory resources. \textcopyright{} 1990-2012 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{lwakatare2019:taxonomy,
  title = {A {{Taxonomy}} of {{Software Engineering Challenges}} for {{Machine Learning Systems}}: {{An Empirical Investigation}}},
  shorttitle = {A {{Taxonomy}} of {{Software Engineering Challenges}} for {{Machine Learning Systems}}},
  booktitle = {Agile {{Processes}} in {{Software Engineering}} and {{Extreme Programming}}},
  author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Bosch, Jan and Olsson, Helena Holmstr{\"o}m and Crnkovic, Ivica},
  editor = {Kruchten, Philippe and Fraser, Steven and Coallier, Fran{\c c}ois},
  year = {2019},
  volume = {355},
  pages = {227--243},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-19034-7_14},
  urldate = {2023-02-27},
  abstract = {Artificial intelligence enabled systems have been an inevitable part of everyday life. However, efficient software engineering principles and processes need to be considered and extended when developing AI- enabled systems. The objective of this study is to identify and classify software engineering challenges that are faced by different companies when developing software-intensive systems that incorporate machine learning components. Using case study approach, we explored the development of machine learning systems from six different companies across various domains and identified main software engineering challenges. The challenges are mapped into a proposed taxonomy that depicts the evolution of use of ML components in software-intensive system in industrial settings. Our study provides insights to software engineering community and research to guide discussions and future research into applied machine learning.},
  isbn = {978-3-030-19033-0 978-3-030-19034-7},
  langid = {english},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/F9CG5XRB/Lwakatare et al. - 2019 - A Taxonomy of Software Engineering Challenges for .pdf}
}

@inproceedings{Lwakatare2020,
  type = {Conference Paper},
  title = {{{DevOps}} for {{AI}} - Challenges in Development of {{AI-enabled}} Applications},
  author = {Lwakatare, Lucy Ellen and Crnkovic, Ivica and Bosch, Jan},
  year = {2020},
  series = {2020 28th {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}}, {{SoftCOM}} 2020},
  doi = {10.23919/SoftCOM50211.2020.9238323},
  abstract = {When developing software systems that contain Machine Learning (ML) based components, the development process become significantly more complex. The central part of the ML process is training iterations to find the best possible prediction model. Modern software development processes, such as DevOps, have widely been adopted and typically emphasise frequent development iterations and continuous delivery of software changes. Despite the ability of modern approaches in solving some of the problems faced when building ML-based software systems, there are no established procedures on how to combine them with processes in ML workflow in practice today. This paper points out the challenges in development of complex systems that include ML components, and discuss possible solutions driven by the combination of DevOps and ML workflow processes. Industrial cases are presented to illustrate these challenges and the possible solutions. \textcopyright{} 2020 University of Split, FESB.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary,repeated_study}
}

@inproceedings{lwakatare2020:devops,
  title = {{{DevOps}} for {{AI}} \textendash{} Challenges in Development of {{AI-enabled}} Applications},
  booktitle = {2020 International Conference on Software, Telecommunications and Computer Networks ({{SoftCOM}})},
  author = {Lwakatare, Lucy Ellen and Crnkovic, Ivica and Bosch, Jan},
  year = {2020},
  month = sep,
  pages = {1--6},
  issn = {1847-358X},
  doi = {10.23919/SoftCOM50211.2020.9238323},
  abstract = {When developing software systems that contain Machine Learning (ML) based components, the development process become significantly more complex. The central part of the ML process is training iterations to find the best possible prediction model. Modern software development processes, such as DevOps, have widely been adopted and typically emphasise frequent development iterations and continuous delivery of software changes. Despite the ability of modern approaches in solving some of the problems faced when building ML-based software systems, there are no established procedures on how to combine them with processes in ML workflow in practice today. This paper points out the challenges in development of complex systems that include ML components, and discuss possible solutions driven by the combination of DevOps and ML workflow processes. Industrial cases are presented to illustrate these challenges and the possible solutions.},
  keywords = {Agile software development,AI,Buildings,Computer networks,Machine learning,Machine Learning,Predictive models,primary,Software systems,Telecommunications,Training},
  file = {/Users/guru/Zotero/storage/M4CA4Q62/Lwakatare et al. - 2020 - DevOps for AI – challenges in development of AI-en.pdf}
}

@article{lwakatare2020:largescale,
  title = {Large-Scale Machine Learning Systems in Real-World Industrial Settings: {{A}} Review of Challenges and Solutions},
  shorttitle = {Large-Scale Machine Learning Systems in Real-World Industrial Settings},
  author = {Lwakatare, Lucy Ellen and Raj, Aiswarya and Crnkovic, Ivica and Bosch, Jan and Olsson, Helena Holmstr{\"o}m},
  year = {2020},
  month = nov,
  journal = {Information and Software Technology},
  volume = {127},
  pages = {106368},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2020.106368},
  urldate = {2023-01-15},
  abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.},
  langid = {english},
  keywords = {Challenges,Industrial settings,Machine learning systems,SLR,Software engineering,Solutions},
  file = {/Users/guru/Zotero/storage/CZUJP5D5/S0950584920301373.html}
}

@article{lytos2022:modelling,
  title = {Modelling Argumentation in Short Text: {{A}} Case of Social Media Debate},
  shorttitle = {Modelling Argumentation in Short Text},
  author = {Lytos, Anastasios and Lagkas, Thomas and Sarigiannidis, Panagiotis and Argyriou, Vasileios and Eleftherakis, George},
  year = {2022},
  month = feb,
  journal = {Simulation Modelling Practice and Theory},
  volume = {115},
  pages = {102446},
  issn = {1569-190X},
  doi = {10.1016/j.simpat.2021.102446},
  urldate = {2023-01-15},
  abstract = {The technological leaps of artificial intelligence (AI) and the rise of machine learning have triggered significant progress in a plethora of natural language processing (NLP) and natural language understanding tasks. One of these tasks is argumentation mining which has received significant interest in recent years and is regarded as a key domain for future decision-making systems, behaviour modelling, and natural language understanding problems. Until recently, natural language modelling tasks, such as computational argumentation schemes, were often tested in controlled environments, such as persuasive essays, reducing unexpected behaviours that could occur in real-life settings, like a public debate on social media. Additionally, the growing demand for enhancing the trust and the explainability of the AI services has dictated the design and adoption of modelling schemes to increase the confidence in the outcomes of the AI solutions. This paper attempts to explore modelling argumentation in short text and proposes a novel framework for argumentation detection under the name Abstract Framework for Argumentation Detection (AFAD). Moreover, different proof-of-concept implementations are provided to examine the applicability of the proposed framework to very short text developing a rule-based mechanism and compare the results with data-driven solutions. Eventually, a combination of the deployed methods is applied increasing the correct predictions in the minority class on an imbalanced dataset. The findings suggest that the modelling process provides solid grounds for technical research while the hybrid solutions have the potential to be applied to a wide range of NLP-related tasks offering a deeper understanding of human language and reasoning.},
  langid = {english},
  keywords = {Argumentation detection,Argumentation mining,Argumentation modelling,Computational linguistics,Natural language processing (NLP),Semantic similarity,Social media,Symbolic artificial intelligence}
}

@article{maddikunta2022:industry,
  title = {Industry 5.0: {{A}} Survey on Enabling Technologies and Potential Applications},
  shorttitle = {Industry 5.0},
  author = {Maddikunta, Praveen Kumar Reddy and Pham, Quoc-Viet and B, Prabadevi and Deepa, N and Dev, Kapal and Gadekallu, Thippa Reddy and Ruby, Rukhsana and Liyanage, Madhusanka},
  year = {2022},
  month = mar,
  journal = {Journal of Industrial Information Integration},
  volume = {26},
  pages = {100257},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2021.100257},
  urldate = {2023-01-15},
  abstract = {Industry 5.0 is regarded as the next industrial evolution, its objective is to leverage the creativity of human experts in collaboration with efficient, intelligent and accurate machines, in order to obtain resource-efficient and user-preferred manufacturing solutions compared to Industry 4.0. Numerous promising technologies and applications are expected to assist Industry 5.0 in order to increase production and deliver customized products in a spontaneous manner. To provide a very first discussion of Industry 5.0, in this paper, we aim to provide a survey-based tutorial on potential applications and supporting technologies of Industry 5.0. We first introduce several new concepts and definitions of Industry 5.0 from the perspective of different industry practitioners and researchers. We then elaborately discuss the potential applications of Industry 5.0, such as intelligent healthcare, cloud manufacturing, supply chain management and manufacturing production. Subsequently, we discuss about some supporting technologies for Industry 5.0, such as edge computing, digital twins, collaborative robots, Internet of every things, blockchain, and 6G and beyond networks. Finally, we highlight several research challenges and open issues that should be further developed to realize Industry 5.0.},
  langid = {english},
  keywords = {6G,Edge computing,Enabling technologies,Industry 5.0,Internet of Things,Pervasive AI}
}

@article{malavolta2021:mining,
  title = {Mining Guidelines for Architecting Robotics Software},
  author = {Malavolta, Ivano and Lewis, Grace A. and Schmerl, Bradley and Lago, Patricia and Garlan, David},
  year = {2021},
  month = aug,
  journal = {Journal of Systems and Software},
  volume = {178},
  pages = {110969},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.110969},
  urldate = {2023-01-15},
  abstract = {Context: The Robot Operating System (ROS) is the de-facto standard for robotics software. However, ROS-based systems are getting larger and more complex and could benefit from good software architecture practices. Goal: We aim at (i) unveiling the state-of-the-practice in terms of targeted quality attributes and architecture documentation in ROS-based systems, and (ii) providing empirically-grounded guidance to roboticists about how to properly architect ROS-based systems. Methods: We designed and conducted an observational study where we (i) built a dataset of 335 GitHub repositories containing real open-source ROS-based systems, and (ii) mined the repositories to extract and synthesize quantitative and qualitative findings about how roboticists are architecting ROS-based systems. Results: First, we extracted an empirically-grounded overview of the state of the practice for architecting and documenting ROS-based systems. Second, we synthesized a catalog of 47 architecting guidelines for ROS-based systems. Third, the extracted guidelines were validated by 119 roboticists working on real-world open-source ROS-based systems. Conclusion: Roboticists can use our architecting guidelines for applying good design principles to develop robots that meet quality requirements, and researchers can use our results as evidence-based indications about how real-world ROS systems are architected today, thus inspiring future research contributions.},
  langid = {english},
  keywords = {Robotics,ROS,Software architecture},
  file = {/Users/guru/Zotero/storage/5L2ZXKAD/Malavolta et al_2021_Mining guidelines for architecting robotics software_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/P9BNKPFK/S0164121221000662.html}
}

@incollection{mancl2021:future,
  title = {The {{Future}} of {{Software Engineering}}: {{Where Will Machine Learning}}, {{Agile}}, and {{Virtualization Take Us Next}}?},
  shorttitle = {The {{Future}} of {{Software Engineering}}},
  booktitle = {Agile {{Processes}} in {{Software Engineering}} and {{Extreme Programming}} \textendash{} {{Workshops}}},
  author = {Mancl, Dennis and Fraser, Steven D.},
  editor = {Gregory, Peggy and Kruchten, Philippe},
  year = {2021},
  volume = {426},
  pages = {222--230},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-88583-0_23},
  urldate = {2023-02-27},
  abstract = {Software has become the lifeblood of the 21st century, enabling a broad range of commercial, medical, educational, agricultural, and government applications. These applications are designed and deployed through a variety of software best practices. With the onset of the COVID-19 pandemic, developers have embraced virtualization (remote working) and a variety of strategies to manage the complexity of global development on multiple platforms. However, evolving hazards such as network security, algorithm bias, and the combination of careless developers and deliberate attacks continue to be a challenge. An XP2021 panel organized and chaired by Steven Fraser debated the future of software engineering and related topics such education, ethics, and tools. The panel featured Anita Carleton (CMU's SEI), Priya Marsonia (Cognizant), Bertrand Meyer (SIT, Eiffel Software), Landon Noll (Independent Consultant), and Kati Vilkki (Reaktor).},
  isbn = {978-3-030-88582-3 978-3-030-88583-0},
  langid = {english},
  keywords = {extra_paper,primary},
  file = {/Users/guru/Zotero/storage/PGKS982V/Mancl and Fraser - 2021 - The Future of Software Engineering Where Will Mac.pdf;/Users/guru/Zotero/storage/QQ5Y565V/978-3-030-88583-0_23.pdf}
}

@article{Martínez-Fernández2022,
  type = {Article},
  title = {Software Engineering for {{AI-Based}} Systems: {{A}} Survey},
  author = {{Mart{\'i}nez-Fern{\'a}ndez}, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
  year = {2022},
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {31},
  number = {2},
  doi = {10.1145/3487043},
  abstract = {AI-based systems are software systems with functionalities enabled by at least one AI component (e.g., for image-, speech-recognition, and autonomous driving). AI-based systems are becoming pervasive in society due to advances in AI. However, there is limited synthesized knowledge on Software Engineering (SE) approaches for building, operating, and maintaining AI-based systems. To collect and analyze state-of-the-art knowledge about SE for AI-based systems, we conducted a systematic mapping study. We considered 248 studies published between January 2010 and March 2020. SE for AI-based systems is an emerging research area, where more than 2/3 of the studies have been published since 2018. The most studied properties of AI-based systems are dependability and safety. We identified multiple SE approaches for AI-based systems, which we classified according to the SWEBOK areas. Studies related to software testing and software quality are very prevalent, while areas like software maintenance seem neglected. Data-related issues are the most recurrent challenges. Our results are valuable for: researchers, to quickly understand the state-of-the-art and learn which topics need more research; practitioners, to learn about the approaches and challenges that SE entails for AI-based systems; and, educators, to bridge the gap among SE and AI in their curricula. \textcopyright{} 2022 Copyright held by the owner/author(s).},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/4VRUHVIT/3487043 (1).pdf}
}

@article{martinez2022software,
  title = {Software Engineering for {{AI-based}} Systems: A Survey},
  author = {{Mart{\'i}nez-Fern{\'a}ndez}, Silverio and Bogner, Justus and Franch, Xavier and Oriol, Marc and Siebert, Julien and Trendowicz, Adam and Vollmer, Anna Maria and Wagner, Stefan},
  year = {2022},
  journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
  volume = {31},
  number = {2},
  pages = {1--59},
  publisher = {{ACM New York, NY}},
  file = {/Users/guru/Zotero/storage/DURF3MIB/Martínez-Fernández et al. - 2022 - Software engineering for AI-based systems a surve.pdf}
}

@article{maschler2022:insights,
  title = {Insights and {{Example Use Cases}} on {{Industrial Transfer Learning}}},
  author = {Maschler, Benjamin and Vietz, Hannes and Tercan, Hasan and Bitter, Christian and Meisen, Tobias and Weyrich, Michael},
  year = {2022},
  month = jan,
  journal = {Procedia CIRP},
  series = {Leading Manufacturing Systems Transformation \textendash{} {{Proceedings}} of the 55th {{CIRP Conference}} on {{Manufacturing Systems}} 2022},
  volume = {107},
  pages = {511--516},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2022.05.017},
  urldate = {2023-01-15},
  abstract = {Despite the high solution potential of machine learning for common problems in automation technology, there are only few examples of its application in real-world manufacturing practice. In order to find the reason for this phenomenon, the authors identify the hurdles for conventional machine learning using four exemplary use cases namely self-learning robots, wear prediction, visual object detection, and predictive quality in manufacturing. While these use-cases differ in principle, the problems engineers face when using conventional machine learning approaches to solve them are related, such as the lack of manifold training data or high dynamics of industrial processes. The authors showcase that utilizing deep transfer learning and continual learning approaches in the industrial context \textendash{} subsumed under the term industrial transfer learning \textendash{} can overcome these hurdles. Even for industrial transfer learning, there is a deficiency regarding preconditions for the large-scale deployment of such approaches, but unlike in conventional machine learning, it is principally possible to establish those. The article concludes with a discussion of these prerequisites and makes suggestions as to how they could be fulfilled.},
  langid = {english},
  keywords = {Continual Learning,Industrial Application,Simulation to Reality,Transfer Learning,Use Cases},
  file = {/Users/guru/Zotero/storage/U98TB5TS/Maschler et al_2022_Insights and Example Use Cases on Industrial Transfer Learning_Procedia CIRP.pdf;/Users/guru/Zotero/storage/8ARPTM3F/S2212827122003018.html}
}

@article{Masuda20211,
  type = {Article},
  title = {Adaptive Enterprise Architecture for the Digital Healthcare Industry: {{A}} Digital Platform for Drug Development},
  author = {Masuda, Yoshimasa and Zimmermann, Alfred and Viswanathan, Murlikrishna and Bass, Matt and Nakamura, Osamu and Yamamoto, Shuichiro},
  year = {2021},
  journal = {Information (Switzerland)},
  volume = {12},
  number = {2},
  pages = {1--26},
  doi = {10.3390/info12020067},
  abstract = {Enterprise architecture (EA) is useful for effectively structuring digital platforms with digital transformation in information societies. Moreover, digital platforms in the healthcare industry accelerate and increase the efficiency of drug discovery and development processes. However, there is the lack of knowledge concerning relationships between EA and digital platforms, in spite of the needs of it. In this paper, we investigated and analyzed the process of drug design and development within the healthcare industry, together with related work in using an enterprise architecture framework for the digital era named the Adaptive Integrated Digital Architecture Framework (AIDAF), specifically supporting the design of digital platforms there. Based on this analysis, we evaluate a method and propose a new reference architecture for promoting digital platforms in the healthcare industry, with future specific aspects of them making effective use of Artificial Intelligence (AI). The practical and theoretical contributions include: (1) Streamlined processes through digital platforms in organizations. (2) Informal knowledge supply and sharing among organizational members through digital platforms. (3) Efficiency and effectiveness in planning production and business for drug development. The findings indicate that EA with digital platforms using the AIDAF contribute to digital transformation with effectiveness for new drugs in the healthcare industry. \textcopyright{} 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{mayer2021:how,
  title = {How {{Distributed Ledger Technologies}} Affect Business Models of Manufacturing Companies},
  author = {Mayer, Johannes and Niemietz, Philipp and Trauth, Daniel and Bergs, Thomas},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {54th {{CIRP CMS}} 2021 - {{Towards Digitalized Manufacturing}} 4.0},
  volume = {104},
  pages = {152--157},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2021.11.026},
  urldate = {2023-01-15},
  abstract = {Industry 4.0 is characterized by the transformation and networking of production systems as a result of utilizing digital technologies and processing huge amounts of data. The features of Distributed Ledger Technologies have the potential to revolutionize production by creating trust and transparency in stored data, eliminating dependence on single entities and performing automated transactions in real time. Due to the disruptive change coming with an implementation, manufacturers remain skeptical about the technology. To raise awareness of the potential of Distributed Ledger Technology, this paper describes valid use cases and examines the interactions with business models in real world manufacturing scenarios.},
  langid = {english},
  keywords = {Audit,Data Marketplace,Distributed Ledger Technology,Manufacturing,Quality-Tracking,Subscription}
}

@article{mcgraw2020top,
  title = {The Top 10 Risks of Machine Learning Security},
  author = {McGraw, Gary and Bonett, Richie and Shepardson, Victor and Figueroa, Harold},
  year = {2020},
  journal = {Computer},
  volume = {53},
  number = {6},
  pages = {57--61},
  publisher = {{IEEE}}
}

@article{mehmood2023:intentdriven,
  title = {Intent-Driven Autonomous Network and Service Management in Future Cellular Networks: {{A}} Structured Literature Review},
  shorttitle = {Intent-Driven Autonomous Network and Service Management in Future Cellular Networks},
  author = {Mehmood, Kashif and Kralevska, Katina and Palma, David},
  year = {2023},
  month = jan,
  journal = {Computer Networks},
  volume = {220},
  pages = {109477},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2022.109477},
  urldate = {2023-01-15},
  abstract = {Intent-driven networks are an essential stepping stone in the evolution of network and service management towards a truly autonomous paradigm. User centric intents provide an abstracted means of impacting the design, provisioning, deployment and assurance of network infrastructure and services with the help of service level agreements and minimum network capability exposure. The concept of Intent Based Networking (IBN) poses several challenges in terms of the contextual definition of intents, role of different stakeholders, and a generalized architecture. In this review, we provide a comprehensive analysis of the state-of-the-art in IBN including the intent description models, intent lifecycle management, significance of IBN and a generalized architectural framework along with challenges and prospects for IBN in future cellular networks. An analytical study is performed on the data collected from relevant studies primarily focusing on the inter-working of IBN with softwarized networking based on NFV/SDN infrastructures. Critical functions required in the IBN management and service model design are explored with different abstract modeling techniques and a converged architectural framework is proposed. The key findings include: (1) benefits and role of IBN in autonomous networking, (2) improvements needed to integrate intents as fundamental policies for service modeling and network management, (3) need for appropriate representation models for intents in domain agnostic abstract manner, and (4) need to include learning as a fundamental function in autonomous networks. These observations provide the basis for in-depth investigation and standardization efforts for IBN as a fundamental network management paradigm in beyond 5G cellular networks.},
  langid = {english},
  keywords = {Autonomous networking,Context-aware,Intent-based networking,Intents,Service design,SLA},
  file = {/Users/guru/Zotero/storage/YXP9YSIR/Mehmood et al_2023_Intent-driven autonomous network and service management in future cellular_Computer Networks.pdf;/Users/guru/Zotero/storage/E2QSDHQK/S1389128622005114.html}
}

@inproceedings{meinke2018machine,
  title = {Machine Learning for Software Engineering: Models, Methods, and Applications},
  booktitle = {2018 {{IEEE}}/{{ACM}} 40th International Conference on Software Engineering: {{Companion}} ({{ICSE-Companion}})},
  author = {Meinke, Karl and Bennaceur, Amel},
  year = {2018},
  pages = {548--549},
  organization = {{IEEE}}
}

@inproceedings{Mendoza202272,
  type = {Conference Paper},
  title = {On the Effectiveness of Data Balancing Techniques in the Context of {{ML-based}} Test Case Prioritization},
  author = {Mendoza, Jediael and Mycroft, Jason and Milbury, Lyam and Kahani, Nafiseh and Jaskolka, Jason},
  year = {2022},
  series = {{{PROMISE}} 2022 - {{Proceedings}} of the 18th {{International Conference}} on {{Predictive Models}} and {{Data Analytics}} in {{Software Engineering}}, Co-Located with {{ESEC}}/{{FSE}} 2022},
  pages = {72--81},
  doi = {10.1145/3558489.3559073},
  abstract = {Regression testing is the cornerstone of quality assurance of software systems. However, executing regression test cases can impose significant computational and operational costs. In this context, Machine Learning-based Test Case Prioritization (ML-based TCP) techniques rank the execution of regression tests based on their probability of failures and execution time so that the faults can be detected as early as possible during the regression testing. Despite the recent progress of ML-based TCP, even the best reported ML-based TCP techniques can reach 90\% or higher effectiveness in terms of Cost-cognizant Average Percentage of Faults Detected (APFDc) only in 20\% of studied subjects. We argue that the imbalanced nature of used training datasets caused by the low failure rate of regression tests is one of the main reasons for this shortcoming. This work conducts an empirical study on applying 19 state-of the-art data balancing techniques for dealing with imbalanced data sets in the TCP context, based on the most comprehensive publicly available datasets. The results demonstrate that data balancing techniques can improve the effectiveness of the best-known ML-based TCP technique for most subjects, with an average of 0.06 in terms of APFDc. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{meyer2019:concept,
  title = {Towards Concept Based Software Engineering for Intelligent Agents},
  booktitle = {Proceedings of the 7th {{International Workshop}} on {{Realizing Artificial Intelligence Synergies}} in {{Software Engineering}}},
  author = {Meyer, Ole and Gruhn, Volker},
  year = {2019},
  month = may,
  series = {{{RAISE}} '19},
  pages = {42--48},
  publisher = {{IEEE Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1109/RAISE.2019.00015},
  urldate = {2023-01-15},
  abstract = {The development of AI and machine learning applications at an industry mature level while maintaining quality and productivity goals is one of today's major challenges. Research in the field of intelligent agents has achieved many successes in recent years, especially due to various reinforcement learning techniques, and promises a high benefit in times of automation and autonomous systems. Bringing them into production, however, requires optimization against many other criteria than just accuracy. This leads to the emerging field of machine teaching. We already know many of the objectives used there from software engineering research, which has led to many well-established principles in recent decades. One of them is the component-based development whose idea finds an interesting counterpart in hierarchical reinforcement learning. We show that both areas can benefit from each other and introduce our approach of concept based software engineering, which is focused on supporting productivity and quality goals during the development of such systems.},
  keywords = {artificial intelligence,components,machine teaching,primary,reinforcement learning,software engineering},
  file = {/Users/guru/Zotero/storage/25CUH9KA/Meyer_Gruhn_2019_Towards concept based software engineering for intelligent agents_Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering.pdf}
}

@inproceedings{Milosavljevic2013235,
  type = {Conference Paper},
  title = {Kroki: {{A}} Mockup-Based Tool for Participatory Development of Business Applications},
  author = {Milosavljevic, Gordana and Filipovic, Milorad and Marsenic, Vladan and Pejakovic, Darko and Dejanovic, Igor},
  year = {2013},
  series = {{{SoMeT}} 2013 - 12th {{IEEE International Conference}} on {{Intelligent Software Methodologies}}, {{Tools}} and {{Techniques}}, {{Proceedings}}},
  pages = {235--242},
  doi = {10.1109/SoMeT.2013.6645668},
  abstract = {This paper presents Kroki (fr. croquis - sketch), a tool for participatory development of business applications based on mockups. Kroki provides a graphical editor for visual creation of mockups and two engines (web and desktop) for mockup execution. Kroki is developed in order to foster development agility, communication and better understanding of end-user needs. The mockup editor and engines are based on our EUIS (Enterprise User Interface Specification) DSL for specifying user interfaces of business applications. \textcopyright{} 2013 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Miranda20211550,
  type = {Conference Paper},
  title = {An Analysis of {{Monte Carlo}} Simulations for Forecasting Software Projects},
  author = {Miranda, Pedro and Faria, J. Pascoal and Correia, Filipe F. and Fares, Ahmed and Gra{\c c}a, Ricardo and Moreira, Jo{\~a}o Mendes},
  year = {2021},
  series = {Proceedings of the {{ACM Symposium}} on {{Applied Computing}}},
  pages = {1550--1558},
  doi = {10.1145/3412841.3442030},
  abstract = {Forecasts of the effort or delivery date can play an important role in managing software projects, but the estimates provided by development teams are often inaccurate and time-consuming to produce. This is not surprising given the uncertainty that underlies this activity. This work studies the use of Monte Carlo simulations for generating forecasts based on project historical data. We have designed and run experiments comparing these forecasts against what happened in practice and to estimates provided by developers, when available. Comparisons were made based on the mean magnitude of relative error (MMRE). We did also analyze how the forecasting accuracy varies with the amount of work to be forecasted and the amount of historical data used. To minimize the requirements on input data, delivery date forecasts for a set of user stories were computed based on takt time of past stories (time elapsed between the completion of consecutive stories); effort forecasts were computed based on full-time equivalent (FTE) hours allocated to the implementation of past stories. The MMRE of delivery date forecasting was 32\% in a set of 10 runs (for different projects) of Monte Carlo simulation based on takt time. The MMRE of effort forecasting was 20\% in a set of 5 runs of Monte Carlo simulation based on FTE allocation, much smaller than the MMRE of 134\% of developers' estimates. A better forecasting accuracy was obtained when the number of historical data points was 20 or higher. These results suggest that Monte Carlo simulations may be used in practice for delivery date and effort forecasting in agile projects, after a few initial sprints. \textcopyright{} 2021 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{mitzutani2021:semantic,
  title = {Semantic Data Integration with {{DevOps}} to Support Engineering Process of Intelligent Building Automation Systems},
  booktitle = {Proceedings of the 8th {{ACM International Conference}} on {{Systems}} for {{Energy-Efficient Buildings}}, {{Cities}}, and {{Transportation}}},
  author = {Mitzutani, Iori and Ramanathan, Ganesh and Mayer, Simon},
  year = {2021},
  month = nov,
  series = {{{BuildSys}} '21},
  pages = {294--297},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3486611.3492413},
  urldate = {2023-01-15},
  abstract = {The reliable infrastructure of building automation (BA) systems forms the foundation of smart environments and energy systems in our building towards increasing occupant comfort and safety while reducing the ecological footprint of buildings. This is achieved through the processing of data points collected from sensors and the control of installed actuators, and increasingly incorporates machine learning components. However, engineering of BA systems is intricately linked with the planning, installation, (pre-)commissioning, and operation of building services such as HVAC, and it requires an extensive amount of manual coordination which is often prone to errors, many of which are only detected late in the lifecycle and tends to lose transparency in data provenance. To address this, we propose the application of DevOps, a highly successful paradigm in the field of software engineering, to BA engineering process coordination. In addition, the possibility of using semantic data to develop artifacts such as requirements, construction, and devices of BA systems opens up the avenue of achieving continuous verification of the system as it is built and commissioned. Concretely, we propose a novel approach that integrates a semantic reasoner using the machine-understandable data of the building along with interactions facilitated by Web of Thing Thing Description to the DevOps workflow. The proposed approach is expected to ameliorate limitations of existing workflow management methods and thus provide transparency in the data provenance to gain trust for data-driven AI applications for BA.},
  isbn = {978-1-4503-9114-6},
  keywords = {building automation,cyber-physical systems,DevOps,explainable CPS,extra_paper,primary,provenance,semantic data},
  file = {/Users/guru/Zotero/storage/4QDZT2JJ/Mitzutani et al_2021_Semantic data integration with DevOps to support engineering process of_Proceedings of the 8th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation.pdf}
}

@article{moniruzzaman2020:blockchain,
  title = {Blockchain for Smart Homes: {{Review}} of Current Trends and Research Challenges},
  shorttitle = {Blockchain for Smart Homes},
  author = {Moniruzzaman, {\relax Md} and Khezr, Seyednima and Yassine, Abdulsalam and Benlamri, Rachid},
  year = {2020},
  month = may,
  journal = {Computers \& Electrical Engineering},
  volume = {83},
  pages = {106585},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2020.106585},
  urldate = {2023-01-15},
  abstract = {The increasing popularity of smart home applications has introduced unprecedented demand for improving the underlying information technology infrastructure to ensure the transparency, security, and privacy of user data. Blockchain is a promising technology capable of addressing such demand. This paper provides a comprehensive review of current advancements and highlights the major progress of employing blockchain in smart home systems. We first discuss blockchain techniques and the prerequisites of a smart home to adapt blockchain technology. Then, we present various applications of blockchain for smart homes and discuss the most commonly used methods in the literature. We also present two case studies to show how blockchain empowers smart home transactions by eliminating the middleman. This paper also reports on the major challenges pertaining to interoperability protocols, security and privacy, data collection and sharing, data analytics, and latency. Finally, the paper identifies potentially promising areas for future research.},
  langid = {english},
  keywords = {Adaptive requirements,Automation,Blockchain technology,Homecare,Interoperability,P2P Data marketplace,Smart city,Smart home},
  file = {/Users/guru/Zotero/storage/8FQW3Y7I/Moniruzzaman et al_2020_Blockchain for smart homes_Computers & Electrical Engineering.pdf;/Users/guru/Zotero/storage/98W9UA8Z/S0045790619316465.html}
}

@inproceedings{moreschini2022:mlops,
  title = {{{MLOps}} for Evolvable {{AI}} Intensive Software Systems},
  booktitle = {2022 {{IEEE}} International Conference on Software Analysis, Evolution and Reengineering ({{SANER}})},
  author = {Moreschini, Sergio and Lomio, Francesco and H{\"a}stbacka, David and Taibi, Davide},
  year = {2022},
  month = mar,
  pages = {1293--1294},
  issn = {1534-5351},
  doi = {10.1109/SANER53432.2022.00155},
  abstract = {DevOps practices are the de facto sandard when developing software. The increased adoption of machine learning (ML) to solve problems urges us to adapt all the current approaches to developing a new standard that can take full benefit from the new solution. In this work we propose a graphical representation for DevOps for ML-based applications, namely MLOps, and also outline open research challenges. The pipeline aims to get the best of both worlds by maintaining the simple and iconic pipeline of DevOps, yet improving it by adding new circular steps for ML incorporation. This aims to create an ML-based development subsystem that can be self-maintained, and is capable of evolving side-by-side with the software development.},
  keywords = {AIOps,Codes,Conferences,DevOps,Machine learning,MLOps,Pipelines,primary,Software,Software Engineering,Software systems,Task analysis},
  file = {/Users/guru/Zotero/storage/PA9ZQMEW/Moreschini et al. - 2022 - MLOps for evolvable AI intensive software systems.pdf}
}

@inproceedings{Mostofi2021515,
  type = {Conference Paper},
  title = {Fast and Efficient Performance Tuning of Microservices},
  author = {Mostofi, Vahid MirzaEbrahim and Krishnamurthy, Diwakar and Arlitt, Martin},
  year = {2021},
  series = {{{IEEE International Conference}} on {{Cloud Computing}}, {{CLOUD}}},
  volume = {2021-September},
  pages = {515--520},
  doi = {10.1109/CLOUD53861.2021.00067},
  abstract = {The microservice architecture is being increasingly adopted. Microservices often rely on containerization technology, facilitating agile development and permitting flexible deployment on cloud platforms. Many microservice applications are interactive. Consequently, there is a need for pre-deployment performance tuning techniques to ensure that an application will meet its end user response time requirements post-deployment. Additionally, the tuning process should be efficient, i.e., allocate just enough resources to minimize costs in cloud-based deployments. Furthermore, the tuning process needs to be fast to facilitate agile deployments. We design and evaluate a technique called MOAT (Microservice Application Performance Tuner) that embodies these requiremenis. MOAT conducts iterative performance tests to determine resource allocations for the individual microservices in an application for any given workload. It exploits a novel optimization technique that identifies resource allocations while requiring only a limited number of performance tests to explore the tuning space. Validation using an experimental system shows that MOAT outperforms a competing approach based on Bayesian optimization in terms of both solution speed and resource allocation efficiency. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{muccini2021software,
  title = {Software Architecture for Ml-Based Systems: What Exists and What Lies Ahead},
  booktitle = {2021 {{IEEE}}/{{ACM}} 1st Workshop on {{AI}} Engineering-Software Engineering for {{AI}} ({{WAIN}})},
  author = {Muccini, Henry and Vaidhyanathan, Karthik},
  year = {2021},
  pages = {121--128},
  organization = {{IEEE}}
}

@inproceedings{Muhammad2021,
  type = {Conference Paper},
  title = {Methods and Guidelines for Incorporating Human Factors Requirements in Automated Vehicles Development},
  author = {Muhammad, Amna Pir},
  year = {2021},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2857},
  abstract = {Automated vehicles (AV) are transforming the future of the transportation and improving the quality of life. However, due to their societal impact in urban environments, AV development challenges the current development process. Particularly, it is unclear how human factors requirements can be communicated to developers of AI-based AV. It is quite challenging especially in agile development, where the focus is on continuous deployment and rapid release cycles with short lead-Times. Due to the importance of human factors and its impact on trust, acceptance, and safety of AV in urban environments, my work aims at providing a suitable requirements engineering perspective and method. \textcopyright{} 2021 CEUR-WS. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {extra_paper,primary},
  file = {/Users/guru/Zotero/storage/RU8FPDY4/ds1.pdf;/Users/guru/Zotero/storage/TL7RL39H/Muhammad - 2021 - Methods and guidelines for incorporating human fac.pdf}
}

@article{munappy2022:data,
  title = {Data Management for Production Quality Deep Learning Models: {{Challenges}} and Solutions},
  shorttitle = {Data Management for Production Quality Deep Learning Models},
  author = {Munappy, Aiswarya Raj and Bosch, Jan and Olsson, Helena Holmstr{\"o}m and Arpteg, Anders and Brinne, Bj{\"o}rn},
  year = {2022},
  month = sep,
  journal = {Journal of Systems and Software},
  volume = {191},
  pages = {111359},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2022.111359},
  urldate = {2023-01-15},
  abstract = {Deep learning (DL) based software systems are difficult to develop and maintain in industrial settings due to several challenges. Data management is one of the most prominent challenges which complicates DL in industrial deployments. DL models are data-hungry and require high-quality data. Therefore, the volume, variety, velocity, and quality of data cannot be compromised. This study aims to explore the data management challenges encountered by practitioners developing systems with DL components, identify the potential solutions from the literature and validate the solutions through a multiple case study. We identified 20 data management challenges experienced by DL practitioners through a multiple interpretive case study. Further, we identified 48 articles through a systematic literature review that discuss the solutions for the data management challenges. With the second round of multiple case study, we show that many of these solutions have limitations and are not used in practice due to a combination of four factors: high cost, lack of skill-set and infrastructure, inability to solve the problem completely, and incompatibility with certain DL use cases. Thus, data management for data-intensive DL models in production is complicated. Although the DL technology has achieved very promising results, there is still a significant need for further research in the field of data management to build high-quality datasets and streams that can be used for building production-ready DL systems. Furthermore, we have classified the data management challenges into four categories based on the availability of the solutions.},
  langid = {english},
  keywords = {Challenges,Data management,Deep learning,Production quality DL models,Solutions,Validation},
  file = {/Users/guru/Zotero/storage/SKGEHZHD/Munappy et al_2022_Data management for production quality deep learning models_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/89NG737J/S0164121222000905.html}
}

@article{mylonas2022:eden,
  title = {Eden {{Library}}: {{A}} Long-Term Database for Storing Agricultural Multi-Sensor Datasets from {{UAV}} and Proximal Platforms},
  shorttitle = {Eden {{Library}}},
  author = {Mylonas, Nikos and Malounas, Ioannis and Mouseti, Sofia and Vali, Eleanna and {Espejo-Garcia}, Borja and Fountas, Spyros},
  year = {2022},
  month = dec,
  journal = {Smart Agricultural Technology},
  volume = {2},
  pages = {100028},
  issn = {2772-3755},
  doi = {10.1016/j.atech.2021.100028},
  urldate = {2023-01-15},
  abstract = {In modern agriculture, visual recognition systems based on deep learning are arising to allow autonomous machines to execute field operations in crops. However, for obtaining high performances, these methods need high amounts of data, which are usually scarce in agriculture. The main reason is that building an agricultural dataset covering exhaustively a specific problem is challenging, as visual characteristics of the symptoms may change, and there is a high dependency on environmental factors, such as temperature, humidity and light conditions. Therefore, an efficient methodology is necessary to consistently cover the entire workflow for creating an agricultural dataset, from the image acquisition to its online publication. This paper presents the Eden Library, a platform for contributing to this existing gap in open access crop/plant databases covering proximal and aerial images. The complete workflow on the design and deployment of the platform is also explained and discussed. This workflow is relevant because the provided datasets are thought to be maintained and enriched along the time, and they do not just remain as a static research output covering only specific species, growth stages, and conditions. The image annotations of plants and symptoms are provided, saving users from manually annotating images. Currently, the Eden Library covers 15 different crops, 9 weeds and 30 disorders (pests, diseases and nutrient deficiencies). Eden Library aspires to close this gap by providing a large and diversified image collection of plants, organized in a consistent manner, in order to boost further vision-based and AI-enabled field applications.},
  langid = {english},
  keywords = {Artificial intelligence,Computer vision,Dataset,Deep learning,Open-data,Precision agriculture,Robotics},
  file = {/Users/guru/Zotero/storage/TPYDVDZ4/Mylonas et al_2022_Eden Library_Smart Agricultural Technology.pdf;/Users/guru/Zotero/storage/YZQ32MKI/S2772375521000289.html}
}

@article{Nadeem2022364,
  type = {Article},
  title = {A Flexible Framework for Requirement Management ({{FFRM}}) from Software Architecture toward Distributed Agile Framework},
  author = {Nadeem, Rao and Amir Latif, Rana M. and Hussain, Khalid and Jhanjhi, N.Z. and Humayun, Mamoona},
  year = {2022},
  journal = {Open Computer Science},
  volume = {12},
  number = {1},
  pages = {364--377},
  doi = {10.1515/comp-2022-0239},
  abstract = {GSD is a Global software development environment where data are distributed to more than two sites. These sites may be located on national or continental borders. Every year it expands its development volume, which has become a trend for the software development business. Because of increasing demand, it has become a common type of business. The value of offshore software development has increased immensely. According to the recent predictions, one-quarter of US software-related business shifts offshore, including integration and management-related services. GSD helps explore resources from other countries, increasing knowledge and enhancing operational efficiency. Therefore, globalization has changed the development nature of software. Different organizations face several challenges, such as coordination and requirement ambiguity, during the change management process in GSD. Traceability accommodates these changes in forward and backward direction. However, it gives rise to several challenges like less client involvement because of its distributed nature and challenging to manage the requirement ambiguity due to increased cost. Therefore, the Flexible Framework for Requirement Management (FFRM) must handle the abovementioned issues. \textcopyright{} 2022 Rao Nadeem et al., published by De Gruyter.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Nagowah2013381,
  type = {Conference Paper},
  title = {{{RTET}} - {{A}} Round Trip Engineering Tool},
  author = {Nagowah, Leckraj and Goolfee, Zarah and Bergue, Chris},
  year = {2013},
  series = {2013 {{International Conference}} of {{Information}} and {{Communication Technology}}, {{ICoICT}} 2013},
  pages = {381--387},
  doi = {10.1109/ICoICT.2013.6574606},
  abstract = {Generating codes from models and performing round trip engineering is a key concern in software development. It is vital that software related artifacts such as source codes and models remain in synchronization throughout the development process. Although there are a number of round trip engineering tools available, only a few of them have been adopted by the developers' community. The existing tools perform considerable round trip engineering, but, even then, developers have still lots of work to do to implement a complete system. Many of the developers would like to be able to automatically generate a deployable web application from a model and/or reverse engineer their source codes into models in only a few mouse clicks. This paper introduces our Round Trip Engineering Tool-RTET which follows the Model Driven Engineering (MDE) paradigm to generate a CRUD oriented application and performs reverse engineering to better meet requirements of developers in Java. The prototype is able to automatically generate a working version of a tiered application with a JSP presentation, EJB manager classes with built in functions in Java, and with an appropriate database model. RTET is also able to generate MVC web applications with JSF views and appropriate managed beans. These files are derived from an entity bean which itself is derived from a simple class diagram using eUML2 plug-in. From an existing user interface in JSP or JSF, RTET can also reverse engineer the page to generate an EJB manager class with all its CRUD functions and the corresponding entity bean. \textcopyright{} 2013 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Nahar2022413,
  type = {Conference Paper},
  title = {Collaboration Challenges in Building {{ML-Enabled}} Systems: {{Communication}}, Documentation, Engineering, and Process},
  author = {Nahar, Nadia and Zhou, Shurui and Lewis, Grace and Kastner, Christian},
  year = {2022},
  series = {Proceedings - {{International Conference}} on {{Software Engineering}}},
  volume = {2022-May},
  pages = {413--425},
  doi = {10.1145/3510003.3510209},
  abstract = {The introduction of machine learning (ML) components in software projects has created the need for software engineers to collabo-rate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through inter-views with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common col-laboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/55DJIW73/Nahar et al. - 2022 - Collaboration challenges in building ML-Enabled sy.pdf}
}

@article{nahhas2023:integration,
  title = {On the {{Integration}} of {{Google Cloud}} and {{SAP HANA}} for {{Adaptive Supply Chain}} in {{Retailing}}},
  author = {Nahhas, Abdulrahman and Haertel, Christian and Daase, Christian and Volk, Matthias and Ramesohl, Achim and Steigerwald, Heiko and Zeier, Alexander and Turowski, Klaus},
  year = {2023},
  month = jan,
  journal = {Procedia Computer Science},
  series = {4th {{International Conference}} on {{Industry}} 4.0 and {{Smart Manufacturing}}},
  volume = {217},
  pages = {1857--1866},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.12.386},
  urldate = {2023-01-15},
  abstract = {The retailing industry witnessed a significant shift in the past years, which introduced significant modifications in the standard procedures and regular practices of supply chains (SC). These necessary modifications and worldwide distress caused major disruptions and instabilities in SC. Organizations started developing digital-transformation strategies, which include the integration and analysis of external data sources to detect disruptions in SC to retain a competitive advantage in the market. Developing such strategies requires exploring current technologies and investigating the use of cloud capabilities for adaptive supply chain management (A-SCM). The lack of flexibility in strategic and operative supply chain practices causes a significant loss of optimization potential. Therefore, in this study, we investigate data sources and solution techniques that might be beneficial to developing A-SCM practices. Based on the conducted literature analysis and cloud technologies, we present a hybrid architecture for A-SCM, integrating Google Cloud technologies, SAP HANA, simulation, and optimization components.},
  langid = {english},
  keywords = {Adaptive supply chain,External data sources,Google Cloud,Optimization techniques,Retail industry,SAP HANA},
  file = {/Users/guru/Zotero/storage/9LEIFIM5/Nahhas et al_2023_On the Integration of Google Cloud and SAP HANA for Adaptive Supply Chain in_Procedia Computer Science.pdf;/Users/guru/Zotero/storage/BWDE7HNX/S1877050922024711.html}
}

@article{nasser2022:lightweight,
  title = {A Lightweight Federated Learning Based Privacy Preserving {{B5G}} Pandemic Response Network Using Unmanned Aerial Vehicles: {{A}} Proof-of-Concept},
  shorttitle = {A Lightweight Federated Learning Based Privacy Preserving {{B5G}} Pandemic Response Network Using Unmanned Aerial Vehicles},
  author = {Nasser, Nidal and Fadlullah, Zubair Md and Fouda, Mostafa M. and Ali, Asmaa and Imran, Muhammad},
  year = {2022},
  month = mar,
  journal = {Computer Networks},
  volume = {205},
  pages = {108672},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2021.108672},
  urldate = {2023-01-15},
  abstract = {The concept of an intelligent pandemic response network is gaining momentum during the current novel coronavirus disease (COVID-19) era. A heterogeneous communication architecture is essential to facilitate collaborative and intelligent medical analytics in the fifth generation and beyond (B5G) networks to intelligently learn and disseminate pandemic-related information and diagnostic results. However, such a technique raises privacy issues pertaining to the health data of the patients. In this paper, we envision a privacy-preserving pandemic response network using a proof-of-concept, aerial\textendash terrestrial network system serving mobile user entities/equipment (UEs). By leveraging the unmanned aerial vehicles (UAVs), a lightweight federated learning model is proposed to collaboratively yet privately learn medical (e.g., COVID-19) symptoms with high accuracy using the data collected by individual UEs using ambient sensors and wearable devices. An asynchronous weight updating technique is introduced in federated learning to avoid redundant learning and save precious networking as well as computing resources of the UAVs/UEs. A use-case where an Artificial Intelligence (AI)-based model is employed for COVID-19 detection from radiograph images is presented to demonstrate the effectiveness of our proposed approach.},
  langid = {english},
  keywords = {5G,Artificial intelligence (AI),Beyond 5G (B5G),Edge computing,Federated learning,Pandemic,Unmanned aerial vehicle (UAV)},
  file = {/Users/guru/Zotero/storage/9LBLUYQR/S1389128621005466.html}
}

@inproceedings{Naujokat2011167,
  type = {Conference Paper},
  title = {Tailoring Process Synthesis to Domain Characteristics},
  author = {Naujokat, Stefan and Lamprecht, Anna-Lena and Steffen, Bernhard},
  year = {2011},
  series = {Proceedings - 2011 16th {{IEEE International Conference}} on {{Engineering}} of {{Complex Computer Systems}}, {{ICECCS}} 2011},
  pages = {167--175},
  doi = {10.1109/ICECCS.2011.24},
  abstract = {PROPHETS is our flexible framework for the synthesis of processes from libraries of basic services. In this paper we demonstrate how its synthesis strategy can be tailored to the considered application domain. For this purpose, PROPHETS provides a number of configuration options, such as different data exchange formats (e.g. shared variables and pipelining) for the resulting process, as well as structural and temporal logic constraints for minimizing the inherent search space. We illustrate the impact of adequate synthesis tailoring by contrasting two real-life case studies with diametric characteristics. \textcopyright{} 2011 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Nayeri2023,
  type = {Article},
  title = {Towards a Responsive Supply Chain Based on the Industry 5.0 Dimensions: {{A}} Novel Decision-Making Method},
  author = {Nayeri, Sina and Sazvar, Zeinab and Heydari, Jafar},
  year = {2023},
  journal = {Expert Systems with Applications},
  volume = {213},
  doi = {10.1016/j.eswa.2022.119267},
  abstract = {Recently, managers and researchers have introduced the concept of Industry 5.0 that tried to improve Industry 4.0 by adding some crucial principles such as sustainability, resiliency, and human-centricity. Due to the importance of this concept, the present work develops a decision-making framework to investigate a responsive SC considering the Industry 5.0 dimensions named responsive Supply Chain 5.0 for healthcare systems. To do this, at the outset, the main related indicators and strategies are identified according to the experts and literature. Then, a new version of the best-worst method named stochastic best-worst method is developed to compute the indicators' weights. Afterwards, by employing the Fuzzy Vlse Kriterijumsk Optimizacija Kompromisno Resenje method, the feasible alternatives are ranked. The outputs indicate that responsiveness, sustainability, resilience, and human-centricity respectively are the most important aspects. On the other side, the high service level, cost reduction, recovery, and safety are the most important criteria in their corresponding aspects. Moreover, the achieved results show that using advanced technologies and collaboration and information sharing are the best alternatives. Also, to show the capability and advantages of the proposed method, we compare this method with the traditional methods (e.g., the analytical hierarchy process and the traditional best-worst method). In terms of the theoretical implications, this work has introduced the responsive supply chain 5.0 and developed a novel method. On the other hand, in terms of the managerial implications, the present work can give a comprehensive perspective to healthcare managers for understanding the concept of Industry 5.0 in the SC problem to improve the responsiveness of their businesses. \textcopyright{} 2022 Elsevier Ltd},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{neto2008multimodal,
  title = {Multimodal Interfaces Design Issues: The Fusion of Well-Designed Voice and Graphical User Interfaces},
  booktitle = {Proceedings of the 26th Annual {{ACM}} International Conference on {{Design}} of Communication},
  author = {Neto, Americo Talarico and Fortes, Renata Pontin M and {da Silva Filho}, Adalberto G},
  year = {2008},
  pages = {277--278},
  file = {/Users/guru/Zotero/storage/45NNT42J/Neto et al. - 2008 - Multimodal interfaces design issues the fusion of.pdf}
}

@inproceedings{Neto2021320,
  type = {Conference Paper},
  title = {Knowledge-Based Risk Management: {{A}} Systematic Literature Review},
  author = {Neto, Ademar and Perkusich, Mirko and Dantas, Emanuel and Ramos, Felipe and Costa, Alexandre and Almeida, Hyggo and Perkusich, Angelo},
  year = {2021},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {320--329},
  doi = {10.1145/3474624.3474635},
  abstract = {Knowledge management can enhance project risk management effectiveness by supporting the reuse of risk-related knowledge, supporting decision-making. This paper explores this hypothesis by identifying and analyzing the state of the art of studies addressing knowledge-based risk management, seeking solutions that present the process of reusing risk-related knowledge. To this end, we conducted a Systematic Literature Review, applying a hybrid approach that combined a search on the Scopus database with snowballing (backward and forward). We applied Thematic Analysis to the primary studies to identify the main attributes of existing knowledge-based risk management solutions and their relationships. In total, we evaluated 1363 papers and identified 7 reported solutions on the topic. The resulting thematic network comprises one higher-level theme: the Risk Repository; and five middle-level: Knowledge capture, Knowledge codification, Embed Knowledge, Shared Knowledge, and Knowledge application. An implication for research is the need to propose tools and algorithms to support the reuse of knowledge for project risk management. Also, provide practical assessments of the feasibility and utility of these solutions. Furthermore, the Thematic Network can be generalized and used as a basis for researchers who intend to explore knowledge-based risk management. For practitioners, the Thematic Network can help them understand reusing risk knowledge and use the resources already existing in the company to improve organizational learning in risk management. \textcopyright{} 2021 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{nguyen-duc2020:continuous,
  title = {Continuous Experimentation on Artificial Intelligence Software: A Research Agenda},
  shorttitle = {Continuous Experimentation on Artificial Intelligence Software},
  booktitle = {Proceedings of the 28th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {{Nguyen-Duc}, Anh and Abrahamsson, Pekka},
  year = {2020},
  month = nov,
  series = {{{ESEC}}/{{FSE}} 2020},
  pages = {1513--1516},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3368089.3417039},
  urldate = {2023-01-15},
  abstract = {Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an ``unknown unknown'' problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies.},
  isbn = {978-1-4503-7043-1},
  keywords = {AI software,AI system,Continuous Experimentation,Industrial Artificial Intelligence},
  file = {/Users/guru/Zotero/storage/E56E9ILF/Nguyen-Duc_Abrahamsson_2020_Continuous experimentation on artificial intelligence software_Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundatio.pdf}
}

@inproceedings{Nguyen-Duc20201513,
  type = {Conference Paper},
  title = {Continuous Experimentation on Artificial Intelligence Software: {{A}} Research Agenda},
  author = {{Nguyen-Duc}, Anh and Abrahamsson, Pekka},
  year = {2020},
  series = {{{ESEC}}/{{FSE}} 2020 - {{Proceedings}} of the 28th {{ACM Joint Meeting European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  pages = {1513--1516},
  doi = {10.1145/3368089.3417039},
  abstract = {Moving from experiments to industrial level AI software development requires a shift from understanding AI/ ML model attributes as a standalone experiment to know-how integrating and operating AI models in a large-scale software system. It is a growing demand for adopting state-of-the-art software engineering paradigms into AI development, so that the development efforts can be aligned with business strategies in a lean and fast-paced manner. We describe AI development as an "unknown unknown"problem where both business needs and AI models evolve over time. We describe a holistic view of an iterative, continuous approach to develop industrial AI software basing on business goals, requirements and Minimum Viable Products. From this, five areas of challenges are presented with the focus on experimentation. In the end, we propose a research agenda with seven questions for future studies. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/34XBMXB4/Nguyen-Duc and Abrahamsson - 2020 - Continuous experimentation on artificial intellige.pdf}
}

@inproceedings{nguyen2021:humanintheloop,
  title = {Human-in-the-Loop {{XAI-enabled}} Vulnerability Detection, Investigation, and Mitigation},
  booktitle = {2021 36th {{IEEE}}/{{ACM}} International Conference on Automated Software Engineering ({{ASE}})},
  author = {Nguyen, Tien N. and Choo, Raymond},
  year = {2021},
  month = nov,
  pages = {1210--1212},
  issn = {2643-1572},
  doi = {10.1109/ASE51524.2021.9678840},
  abstract = {The need for cyber resilience is increasingly important in our technology-dependent society, where computing systems, devices and data will continue to be the target of cyber attackers. Hence, we propose a conceptual framework called `Human-in-the-Loop Explainable-AI-Enabled Vulnerability Detection, Investigation, and Mitigation' (HXAI-VDIM). Specifically, instead of resolving complex scenario of security vulnerabilities as an output of an AI/ML model, we integrate the security analyst or forensic investigator into the man-machine loop and leverage explainable AI (XAI) to combine both AI and Intelligence Assistant (IA) to amplify human intelligence in both proactive and reactive processes. Our goal is that HXAI-VDIM integrates human and machine in an interactive and iterative loop with security visualization that utilizes human intelligence to guide the XAI-enabled system and generate refined solutions.},
  keywords = {Analytical models,Artificial intelligence,Computational modeling,Explainable AI,Forensics,Human intelligence,Human-in-the-Loop,Investigation,Man-machine systems,Mitigation,Security,Vulnerability Detection},
  file = {/Users/guru/Zotero/storage/NMIKL4VF/Nguyen and Choo - 2021 - Human-in-the-loop XAI-enabled vulnerability detect.pdf}
}

@article{nishant2020:artificial,
  title = {Artificial Intelligence for Sustainability: {{Challenges}}, Opportunities, and a Research Agenda},
  shorttitle = {Artificial Intelligence for Sustainability},
  author = {Nishant, Rohit and Kennedy, Mike and Corbett, Jacqueline},
  year = {2020},
  month = aug,
  journal = {International Journal of Information Management},
  volume = {53},
  pages = {102104},
  issn = {0268-4012},
  doi = {10.1016/j.ijinfomgt.2020.102104},
  urldate = {2023-01-15},
  abstract = {Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.},
  langid = {english},
  keywords = {Agenda for practice,AI,Artificial intelligence,Climate change,Environmental governance,Natural environment,Research agenda,Sustainability},
  file = {/Users/guru/Zotero/storage/X553C695/S0268401220300967.html}
}

@article{Nistala2022,
  type = {Article},
  title = {Towards Digitalization of Requirements: Generating Context-Sensitive User Stories from Diverse Specifications},
  author = {Nistala, Padmalata V. and Rajbhoj, Asha and Kulkarni, Vinay and Soni, Shivani and Nori, Kesav V. and Reddy, Raghu},
  year = {2022},
  journal = {Automated Software Engineering},
  volume = {29},
  number = {1},
  doi = {10.1007/s10515-022-00324-2},
  abstract = {Requirements Engineering in the industry is expertise-driven, heavily manual, and centered around various types of requirement specification documents being prepared and maintained. These specification documents are in diverse formats and vary depending on whether it is a business requirement document, functional specification, interface specification, client specification, and so on. These diverse specification documents embed crucial product knowledge such as functional decomposition of the domain into features, feature hierarchy, feature types and their specific feature characteristics, dependencies, business context, etc. Moreover, in a product development scenario, thousands of pages of requirement specification documentation is created over the years. Comprehending functionality and its associated context from large volumes of specification documents is a highly complex task. To address this problem, we propose to digitalize the requirement specification documents into processable models. This paper discusses the salient aspects involved in the digitalization of requirements knowledge from diverse requirement specification documents. It proposes an AI engine for the automatic transformation of diverse text-based requirement specifications into machine-processable models using NLP techniques and the generation of context-sensitive user stories. The paper describes the key requirement abstractions and concepts essential in an industrial scenario, the conceptual meta-model, and DizReq engine (AI engine for digitalizing requirements) implementation for automatically transforming diverse requirement specifications into user stories embedding the business context. The evaluation results from digitalizing specifications of an IT product suite are discussed: mean feature extraction efficiency is 40 features/file, mean user story extraction efficiency is 71 user stories/file, feature extraction accuracy is 94\%, and requirement extraction accuracy is 98\%. \textcopyright{} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{nouacer2020:framework,
  title = {Towards a Framework of Key Technologies for Drones},
  author = {Nouacer, R{\'e}da and Hussein, Mahmoud and Espinoza, Huascar and Ouhammou, Yassine and Ladeira, Matheus and Casti{\~n}eira, Rodrigo},
  year = {2020},
  month = sep,
  journal = {Microprocessors and Microsystems},
  volume = {77},
  pages = {103142},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2020.103142},
  urldate = {2023-01-15},
  abstract = {The potential applications for drones, especially those in manned areas or into non-segregated airspace, are currently not possible without the development and validation of certain key enabling technologies: ``detect and avoid'', ``air traffic management'', ``command and control (C2) link'', and ``security and cyber resilience''. This issue has a high impact on European innovation as identified by SESAR JU and demands R\&D investments and incentives for the convergence of shared technologies and markets. In this paper, we discuss the vision of COMP4DRONES project to complement SESAR JU efforts with a particular focus on safe software and hardware drone architectures. COMP4DRONES will bear a holistically designed ecosystem ranging from application to electronic components. The ecosystem aims at supporting (1) efficient customization and incremental assurance of drone-embedded platforms, (2) safe autonomous decision making concerning individual or cooperative missions, (3) trustworthy drone-to-drone and drone-to- ground communications even in presence of malicious attackers and under the intrinsic platform constraints, and (4) agile and cost-effective design and assurance of drone modules and systems. Lead applications driving ecosystem development and benchmarking on the fields of transport, inspection, logistic, precision agriculture, and parcel delivery will be produced.},
  langid = {english},
  keywords = {Automation and Control Systems,Autonomy,Composition,Drones,Interoperability,Safety,Security},
  file = {/Users/guru/Zotero/storage/55LQNRUH/S0141933120303094.html}
}

@article{Nussbaumer202334,
  type = {Conference Paper},
  title = {A Framework for Applying Ethics-by-Design to Decision Support Systems for Emergency Management},
  author = {Nussbaumer, Alexander and Pope, Andrew and Neville, Karen},
  year = {2023},
  journal = {Information Systems Journal},
  volume = {33},
  number = {1},
  pages = {34--55},
  doi = {10.1111/isj.12350},
  abstract = {The development and utilisation of new information and communication technologies presents opportunities and risks, which bring ethical issues to the forefront. Any attempt to minimise the potential negative consequences to individuals, organisations and society resulting from the use of these technologies is challenging. In order to address these challenges, this paper presents an ethics-by-design approach that has been developed and implemented in the context of Decision Supports Systems for Emergency Management. Such systems help manage large and cross-border disasters by supporting decision makers to respond on emergencies in a reasonable way by taking follow-up actions into account. The approach taken in this paper specifically provides means to support the ethical dimensions of these decisions. Actions taken during disasters can have ramifications that persist long after a disaster has passed. The ethics-by-design approach presented here not only informs the design of systems, but also considers the role and training of the decision makers in the design process. The paper builds on the literature on ethics in information systems and makes a contribution to theory by providing a framework to ensure ethical considerations are embedded into the design of systems. \textcopyright{} 2021 The Authors. Information Systems Journal published by John Wiley \& Sons Ltd.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{okoli2010:guide,
  title = {A {{Guide}} to {{Conducting}} a {{Systematic Literature Review}} of {{Information Systems Research}}},
  author = {Okoli, Chitu and Schabram, Kira},
  year = {2010},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.1954824},
  urldate = {2023-02-07},
  abstract = {Levy and Ellis (2006) and Webster and Watson (2002) lament the fact that information systems (IS) scholars tend to be unaware of the need for structure in literature reviews. Even today, the rigorous, standardized methodology for conducting a systematic literature review (SLR) that has developed from the health sciences and other fields is virtually unknown in IS research. In this paper, we adapt Fink's (2005, p. 3) definition of a research literature review as our operative definition of a systematic literature review: "a systematic, explicit, [comprehensive, (p. 17)] and reproducible method for identifying, evaluating, and synthesizing the existing body of completed and recorded work produced by researchers, scholars, and practitioners." Although there exists an abundance of guides to conducting such reviews in other research fields, none entirely meet the unique needs of IS researchers. In response to this shortage of guides, we present here the features and value of a systematic literature review, and adapt the methodology to the particular context of IS research.},
  langid = {english}
}

@article{Okonkwo2022,
  type = {Article},
  title = {Revision-Bot: {{A}} Chatbot for Studying Past Questions in Introductory Programming},
  author = {Okonkwo, Chinedu Wilfred and {Ade-Ibijola}, Abejide},
  year = {2022},
  journal = {IAENG International Journal of Computer Science},
  volume = {49},
  number = {3},
  abstract = {Students struggle to get good grades in Computer Programming courses. One of the most important ways to improve the pass rate of a difficult subject, such as programming, is to implement a learning support system based on Artificial Intelligence (AI) technology. This system can provide students with a customised learning experience. This paper proposed a Revision-Bot (abbreviated as called RevBot), an intelligent interactive system that assists students in practicing past exam questions in a Python programming course. RevBot was created and implemented using the Snatchbot Chatbot API. An assessment was conducted to establish the usefulness of RevBot, and the results show that RevBot can help students improve their performance in the Python programming course. \textcopyright{} 2022, IAENG International Journal of Computer Science. All Rights Reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Olufemi20214300,
  type = {Article},
  title = {Research Trends on {{CAPTCHA}}: {{A}} Systematic Literature},
  author = {Olufemi, Igbekele Emmanuel and Adebiyi, Ayodele Ariyo and Ibikunle, Francis A. and Adebiyi, Marion O. and Oludayo, Olugbara O.},
  year = {2021},
  journal = {International Journal of Electrical and Computer Engineering},
  volume = {11},
  number = {5},
  pages = {4300--4312},
  doi = {10.11591/ijece.v11i5.pp4300-4312},
  abstract = {The advent of technology has crept into virtually all sectors and this has culminated in automated processes making use of the Internet in executing various tasks and actions. Web services have now become the trend when it comes to providing solutions to mundane tasks. However, this development comes with the bottleneck of authenticity and intent of users. Providers of these Web services, whether as a platform, as a software or as an Infrastructure use various human interaction proof's (HIPs) to validate authenticity and intent of its users. Completely automated public turing test to tell computer and human apart (CAPTCHA), a form of IDS in web services is advantageous. Research into CAPTCHA can be grouped into two -CAPTCHA development and CAPTCH recognition. Selective learning and convolutionary neural networks (CNN) as well as deep convolutionary neural network (DCNN) have become emerging trends in both the development and recognition of CAPTCHAs. This paper reviews critically over fifty article publications that shows the current trends in the area of the CAPTCHA scheme, its development and recognition mechanisms and the way forward in helping to ensure a robust and yet secure CAPTCHA development in guiding future research endeavor in the subject domain. \textcopyright{} 2021 Institute of Advanced Engineering and Science. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Oparin2020201,
  type = {Conference Paper},
  title = {Automated Tools for the Development of Microservice Compositions for Hybrid Scientific Computations},
  author = {Oparin, G.A. and Bogdanova, V.G. and Pashinin, A.A.},
  year = {2020},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2638},
  pages = {201--213},
  abstract = {In recent years, a significant amount of research is focused on the development of tools for creating composite web-services for solving both business and scientific complex problems. This study discusses tools for building compositions or ensembles of microservices (depending on the method of integration) developed based on the HPCSOMAS framework. These tools are oriented on the application in a package of applied microservices for solving computationally complex problems of structural analysis and parametric synthesis of controlled dynamic systems in a heterogeneous high-performance computing environment. In particular, binary dynamic systems are studied using the Boolean constraint method for both their qualitative analysis and synthesis of laws to control these systems. Creating and executing composite services is carried out on a semantic peer-to-peer network of agents. The HPCSOMAS framework supports two modes of these processes, both the static creation and application of a composite service based on the procedural formulation of the problem and dynamic, based on the declarative formulation. In the first case, agents deployed on the network perform hierarchical control over the execution of the composition of microservices, in the second case, decentralized asynchronous management of the ensemble of microservices. Both operating modes are automated, and the validity of the resulting composite service is checked based on a logical approach. The tools are aimed both at a professional programmer and the end-user, a specialist in the subject domain. The HPCSOMAS framework supports the execution of composite microservices in a hybrid computing infrastructure, which includes both cloud and on-premises resources. Copyright \textcopyright{} 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)},
  publication_stage = {Final},
  source = {Scopus}
}

@article{ozkaya2020really,
  title = {What Is Really Different in Engineering {{AI-enabled}} Systems?},
  author = {Ozkaya, Ipek},
  year = {2020},
  journal = {IEEE Software},
  volume = {37},
  number = {4},
  pages = {3--6},
  publisher = {{IEEE}}
}

@article{P2021,
  type = {Article},
  title = {Prediction of Risk Percentage in Software Projects by Training Machine Learning Classifiers},
  author = {P, Gouthaman and Sankaranarayanan, Suresh},
  year = {2021},
  journal = {Computers and Electrical Engineering},
  volume = {94},
  doi = {10.1016/j.compeleceng.2021.107362},
  abstract = {Recently, software project failures have been increasing due to lack of planning and budget constraints. In this regard, identifying the suitable software model with the consideration of risk factors is imperative. Therefore, this study investigates the key software models utilized in the industry through an interaction with software development experts and literature survey. In this study, 15 standard indicators were chosen where a survey was conducted through a questionnaire. The major performance metrics which were taken into consideration are network, security, software, machine learning, internet of things and application programming interface. We proposed a novel framework for the received dataset through questionnaire in which the machine learning classifiers were applied and risk predictions for each of the identified software models were accomplished. Using this outcome, software product managers can identify the appropriate software model according to the software requirements along with risk prediction percentage. \textcopyright{} 2021 Elsevier Ltd},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{paasivaara2013:teaching,
  title = {Teaching Students Global Software Engineering Skills Using Distributed Scrum},
  booktitle = {Proceedings of the 2013 {{International Conference}} on {{Software Engineering}}},
  author = {Paasivaara, Maria and Lassenius, Casper and Damian, Daniela and R{\"a}ty, Petteri and Schr{\"o}ter, Adrian},
  year = {2013},
  month = may,
  series = {{{ICSE}} '13},
  pages = {1128--1137},
  publisher = {{IEEE Press}},
  address = {{San Francisco, CA, USA}},
  urldate = {2023-01-15},
  abstract = {In this paper we describe distributed Scrum augmented with best practices in global software engineering (GSE) as an important paradigm for teaching critical competencies in GSE. We report on a globally distributed project course between the University of Victoria, Canada and Aalto University, Finland. The project-driven course involved 16 students in Canada and 9 students in Finland, divided into three cross-site Scrum teams working on a single large project. To assess learning of GSE competencies we employed a mixed-method approach including 13 post-course interviews, pre-, post-course and iteration questionnaires, observations, recordings of Daily Scrums as well as collection of project asynchronous communication data. Our analysis indicates that the Scrum method, along with supporting collaboration practices and tools, supports the learning of important GSE competencies, such as distributed communication and teamwork, building and maintaining trust, using appropriate collaboration tools, and inter-cultural collaboration.},
  isbn = {978-1-4673-3076-3}
}

@inproceedings{paasivaara2015:learning,
  title = {Learning Global Agile Software Engineering Using Same-Site and Cross-Site Teams},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 2},
  author = {Paasivaara, Maria and Blincoe, Kelly and Lassenius, Casper and Damian, Daniela and Sheoran, Jyoti and Harrison, Francis and Chhabra, Prashant and Yussuf, Aminah and Isotalo, Veikko},
  year = {2015},
  month = may,
  series = {{{ICSE}} '15},
  pages = {285--294},
  publisher = {{IEEE Press}},
  address = {{Florence, Italy}},
  urldate = {2023-01-15},
  abstract = {We describe an experience in teaching global software engineering (GSE) using distributed Scrum augmented with industrial best practices. Our unique instructional technique had students work in both same-site and cross-site teams to contrast the two modes of working. The course was a collaboration between Aalto University, Finland and University of Victoria, Canada. Fifteen Canadian and eight Finnish students worked on a single large project, divided into four teams, working on interdependent user stories as negotiated with the industrial product owner located in Finland. Half way through the course, we changed the teams so each student worked in both a local and a distributed team. We studied student learning using a mixed-method approach including 14 post-course interviews, pre-course and Sprint questionnaires, observations, meeting recordings, and repository data from git and Flowdock, the primary communication tool. Our results show no significant differences between working in distributed vs. non-distributed teams, suggesting that Scrum helps alleviate many GSE problems. Our post-course interviews and survey data allows us to explain this effect; we found that students over time learned to better self-select tasks with less inter-team dependencies, to communicate more, and to work better in teams.}
}

@article{pachouly2022:systematic,
  title = {A Systematic Literature Review on Software Defect Prediction Using Artificial Intelligence: {{Datasets}}, {{Data Validation Methods}}, {{Approaches}}, and {{Tools}}},
  shorttitle = {A Systematic Literature Review on Software Defect Prediction Using Artificial Intelligence},
  author = {Pachouly, Jalaj and Ahirrao, Swati and Kotecha, Ketan and Selvachandran, Ganeshsree and Abraham, Ajith},
  year = {2022},
  month = may,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {111},
  pages = {104773},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2022.104773},
  urldate = {2023-01-15},
  abstract = {Delivering high-quality software products is a challenging task. It needs proper coordination from various teams in planning, execution, and testing. Many software products have high numbers of defects revealed in a production environment. Software failures are costly regarding money, time, and reputation for a business and even life-threatening if utilized in critical applications. Identifying and fixing software defects in the production system is costly, which could be a trivial task if detected before shipping the product. Binary classification is commonly used in existing software defect prediction studies. With the advancements in Artificial Intelligence techniques, there is a great potential to provide meaningful information to software development teams for producing quality software products. An extensive survey for Software Defect Prediction is necessary for exploring datasets, data validation methods, defect detection, and prediction approaches and tools. The survey infers standard datasets utilized in early studies lack adequate features and data validation techniques. According to the finding of the literature survey, the standard datasets has few labels, resulting in insufficient details regarding defects. Systematic Literature Reviews (SLR) on Software Defect Prediction are limited. Hence this SLR presents a comprehensive analysis of defect datasets, dataset validation, detection, prediction approaches, and tools for Software Defect Prediction. The survey exhibits the futuristic recommendations that will allow researchers to develop a tool for Software Defect Prediction. The survey introduces the architecture for developing a software prediction dataset with adequate features and statistical data validation techniques for multi-label classification for software defects.},
  langid = {english},
  keywords = {Artificial intelligence,Classification,Machine learning,Software defect prediction}
}

@article{paleyes2020challenges,
  title = {Challenges in Deploying Machine Learning: A Survey of Case Studies},
  author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D},
  year = {2020},
  journal = {ACM Computing Surveys (CSUR)},
  publisher = {{ACM New York, NY}}
}

@inproceedings{Paleyes202254,
  type = {Conference Paper},
  title = {An Empirical Evaluation of Flow Based Programming in the Machine Learning Deployment Context},
  author = {Paleyes, Andrei and Cabrera, Christian and Lawrence, Neil D.},
  year = {2022},
  series = {Proceedings - 1st {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}, {{CAIN}} 2022},
  pages = {54--64},
  doi = {10.1145/3522664.3528601},
  abstract = {As use of data driven technologies spreads, software engineers are more often faced with the task of solving a business problem using data-driven methods such as machine learning (ML) algorithms. Deployment of ML within large software systems brings new challenges that are not addressed by standard engineering practices and as a result businesses observe high rate of ML deployment project failures. Data Oriented Architecture (DOA) is an emerging approach that can support data scientists and software developers when addressing such challenges. However, there is a lack of clarity about how DOA systems should be implemented in practice. This paper proposes to consider Flow-Based Programming (FBP) as a paradigm for creating DOA applications. We empirically evaluate FBP in the context of ML deployment on four applications that represent typical data science projects. We use Service Oriented Architecture (SOA) as a baseline for comparison. Evaluation is done with respect to different application domains, ML deployment stages, and code quality metrics. Results reveal that FBP is a suitable paradigm for data collection and data science tasks, and is able to simplify data collection and discovery when compared with SOA. We discuss the advantages of FBP as well as the gaps that need to be addressed to increase FBP adoption as a standard design paradigm for DOA. CCS CONCEPTS \textbullet{} Software and its engineering \textrightarrow{} Software design tradeoffs; \textbullet{} Computing methodologies \textrightarrow{} Machine learning. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {extra_paper,primary},
  file = {/Users/guru/Zotero/storage/TDQXXRWX/Paleyes et al. - 2022 - An empirical evaluation of flow based programming .pdf}
}

@article{Palopak2023,
  type = {Article},
  title = {Knowledge Diffusion Trajectories of Agile Software Development Research: {{A}} Main Path Analysis},
  author = {Palopak, Yulianus and Huang, Sun-Jen and Ratnasari, Wiwit},
  year = {2023},
  journal = {Information and Software Technology},
  volume = {156},
  doi = {10.1016/j.infsof.2022.107131},
  abstract = {Context: The dramatic growth of agile software development (ASD) research has resulted in a large number of diverse theoretical and empirical publications. The citation relationships among these publications indicate knowledge dissemination across and within academia or scientists. Objective: This study offers a comprehensive understanding of the ASD literature by exploring the knowledge diffusion path through the citation network of publications that have made significant contributions to its research development. Method: We employ a quantitative citation-based methodology, main path analysis (MPA), to examine the citation relationship of 1431 scientific articles published in the Web of Science (WoS) between 2001 and 2021 and visualize the MPA results using Pajek software. Results: Through citation analysis this study discovers knowledge diffusion trajectories of publications concerning ASD method. Our key results present 32 publications identified along the key-route main path as the most influential ones in the trajectories of ASD. There are three phases of ASD research development: introduction, evaluation, and deployment and expansion. Using the multiple-global main path, we further uncover the publication trends from a set of recent papers and reveal four sub-themes: tailoring of agile practices, large-scale agile context, challenges and success factors of large-scale organizations, and agile global software development. Conclusions: Although there was little academic interest in the initial phase, ASD-related publication and citation trends have consistently increased over time. The historical development of ASD methods was established in three distinct phases of publications in the domain. Each phase presents a narrative of agile methods' development with different focuses. The most recent trends of ASD publications tend to focus on the agile tailoring and scaling process in the global and distributed environment. \textcopyright{} 2022},
  publication_stage = {Final},
  source = {Scopus}
}

@article{palopak2023:knowledge,
  title = {Knowledge Diffusion Trajectories of Agile Software Development Research: {{A}} Main Path Analysis},
  shorttitle = {Knowledge Diffusion Trajectories of Agile Software Development Research},
  author = {Palopak, Yulianus and Huang, Sun-Jen and Ratnasari, Wiwit},
  year = {2023},
  month = apr,
  journal = {Information and Software Technology},
  volume = {156},
  pages = {107131},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.107131},
  urldate = {2023-01-15},
  abstract = {Context The dramatic growth of agile software development (ASD) research has resulted in a large number of diverse theoretical and empirical publications. The citation relationships among these publications indicate knowledge dissemination across and within academia or scientists. Objective This study offers a comprehensive understanding of the ASD literature by exploring the knowledge diffusion path through the citation network of publications that have made significant contributions to its research development. Method We employ a quantitative citation-based methodology, main path analysis (MPA), to examine the citation relationship of 1431 scientific articles published in the Web of Science (WoS) between 2001 and 2021 and visualize the MPA results using Pajek software. Results Through citation analysis this study discovers knowledge diffusion trajectories of publications concerning ASD method. Our key results present 32 publications identified along the key-route main path as the most influential ones in the trajectories of ASD. There are three phases of ASD research development: introduction, evaluation, and deployment and expansion. Using the multiple-global main path, we further uncover the publication trends from a set of recent papers and reveal four sub-themes: tailoring of agile practices, large-scale agile context, challenges and success factors of large-scale organizations, and agile global software development. Conclusions Although there was little academic interest in the initial phase, ASD-related publication and citation trends have consistently increased over time. The historical development of ASD methods was established in three distinct phases of publications in the domain. Each phase presents a narrative of agile methods' development with different focuses. The most recent trends of ASD publications tend to focus on the agile tailoring and scaling process in the global and distributed environment.},
  langid = {english},
  keywords = {Agile methods,Agile software development,Citation analysis,Key-route analysis,Knowledge diffusion,Main path analysis},
  file = {/Users/guru/Zotero/storage/N6GHWNTR/S0950584922002403.html}
}

@article{Pan2022,
  type = {Article},
  title = {Test Case Selection and Prioritization Using Machine Learning: A Systematic Literature Review},
  author = {Pan, Rongqi and Bagherzadeh, Mojtaba and Ghaleb, Taher A. and Briand, Lionel},
  year = {2022},
  journal = {Empirical Software Engineering},
  volume = {27},
  number = {2},
  doi = {10.1007/s10664-021-10066-6},
  abstract = {Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published studies. We summarize the results related to our research questions in a high-level summary that can be used as a taxonomy for classifying future TSP studies. \textcopyright{} 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{panichella2015can,
  title = {How Can i Improve My App? Classifying User Reviews for Software Maintenance and Evolution},
  booktitle = {2015 {{IEEE}} International Conference on Software Maintenance and Evolution ({{ICSME}})},
  author = {Panichella, Sebastiano and Di Sorbo, Andrea and Guzman, Emitza and Visaggio, Corrado A and Canfora, Gerardo and Gall, Harald C},
  year = {2015},
  pages = {281--290},
  organization = {{IEEE}}
}

@article{pantanowitz2022:rules,
  title = {Rules of Engagement: {{Promoting}} Academic-Industry Partnership in the Era of Digital Pathology and Artificial Intelligence},
  shorttitle = {Rules of Engagement},
  author = {Pantanowitz, Liron and Bui, Marilyn M. and Chauhan, Chhavi and ElGabry, Ehab and Hassell, Lewis and Li, Zaibo and Parwani, Anil V. and Salama, Mohamed E. and Sebastian, Manu M. and Tulman, David and Vepa, Suryanarayana and Becich, Michael J.},
  year = {2022},
  month = jan,
  journal = {Academic Pathology},
  volume = {9},
  number = {1},
  pages = {100026},
  issn = {2374-2895},
  doi = {10.1016/j.acpath.2022.100026},
  urldate = {2023-01-15},
  abstract = {Academic industry partnership (AIP) represents an important alliance between academic researchers and industry that helps translate technology and complete the innovation cycle within academic health systems. Despite diverging missions and skillsets the culture for academia and industry is changing in response to the current digital era which is spawning greater collaboration between physicians and businesses in this marketplace. In the field of pathology, this is further driven by the fact that traditional funding sources cannot keep pace with the innovation needed in digital pathology and artificial intelligence. This concept article from the Digital Pathology Association (DPA) describes the rules of engagement for pathology innovators in academia and for their corporate partners to help establish best practices in this critical area. Stakeholders include pathologists, basic and translational researchers, university technology transfer and sponsored research offices, as well as industry relations officers. The article discusses the benefits and pitfalls of an AIP, reviews different partnership models, examines the role of pathologists in the innovation cycle, explains various agreements that may need to be signed, covers conflict of interest and intellectual property issues, and offers recommendations for ensuring successful partnerships.},
  langid = {english},
  keywords = {Academic-industry partnership,Artificial intelligence,Conflict of interest,Digital pathology,Industry,Innovation,Intellectual property,Patent,Sponsored research,University}
}

@article{Pasuksmit2022,
  type = {Article},
  title = {Story Points Changes in Agile Iterative Development: {{An}} Empirical Study and a Prediction Approach},
  author = {Pasuksmit, Jirat and Thongtanunam, Patanamon and Karunasekera, Shanika},
  year = {2022},
  journal = {Empirical Software Engineering},
  volume = {27},
  number = {6},
  doi = {10.1007/s10664-022-10192-9},
  abstract = {Story Points (SP) are an effort unit that is used to represent the relative effort of a work item. In Agile software development, SP allows a development team to estimate their delivery capacity and facilitate the sprint planning activities. Although Agile embraces changes, SP changes after the sprint planning may negatively impact the sprint plan. To minimize the impact, there is a need to better understand the SP changes and an automated approach to predict the SP changes. Hence, to better understand the SP changes, we examine the prevalence, accuracy, and impact of information changes on SP changes. Through the analyses based on 19,349 work items spread across seven open-source projects, we find that on average, 10\% of the work items have SP changes. These work items typically have SP value increased by 58\%-100\% relative to the initial SP value when they were assigned to a sprint. We also find that the unchanged SP reflect the development time better than the changed SP. Our qualitative analysis shows that the work items with changed SP often have the information changes relating to updating the scope of work. Our empirical results suggest that SP and the scope of work should be reviewed prior or during sprint planning to achieve a reliable sprint plan. Yet, it could be a tedious task to review all work items in the product (or sprint) backlog. Therefore, we develop a classifier to predict whether a work item will have SP changes after being assigned to a sprint. Our classifier achieves an AUC of 0.69-0.8, which is significantly better than the baselines. Our results suggest that to better manage and prepare for the unreliability in SP estimation, the team can leverage our insights and the classifier during the sprint planning. To facilitate future studies, we provide the replication package and the datasets, which are available online. \textcopyright{} 2022, The Author(s).},
  publication_stage = {Final},
  source = {Scopus}
}

@article{paz2016:model,
  title = {A {{Model}} to {{Guide Dynamic Adaptation Planning}} in {{Self-Adaptive Systems}}},
  author = {Paz, Andr{\'e}s and Arboleda, Hugo},
  year = {2016},
  month = mar,
  journal = {Electronic Notes in Theoretical Computer Science},
  series = {{{CLEI}} 2015, the {{XLI Latin American Computing Conference}}},
  volume = {321},
  pages = {67--88},
  issn = {1571-0661},
  doi = {10.1016/j.entcs.2016.02.005},
  urldate = {2023-01-15},
  abstract = {Self-adaptive enterprise applications have the ability to continuously reconfigure themselves according to changes in their execution contexts or user requirements. The infrastructure managing such systems is based on IBM's MAPE-K reference model: a Monitor and an Analyzer to sense and interpret context data, a Planner and an Executor to create and apply structural adaptation plans, and a Knowledge manager to share relevant information. In this paper we present a formal model, built on the principles of constraint satisfaction, to address dynamic adaptation planning for self-adaptive enterprise applications. We formalize, modify and extend the approach presented in [H. Arboleda, J. F. D\'iaz, V. Vargas, and J.-C. Royer, ``Automated reasoning for derivation of modeldriven spls,'' in SPLC'10 MAPLE'10, 2010, pp. 181\textendash 188] for working with self-adaptation infrastructures in order to provide automated reasoning on the dynamic creation of structural adaptation plans. We use a running example to demonstrate the applicability of such model, even in situations where complex interactions arise between context elements and the target self-adaptive enterprise application.},
  langid = {english},
  keywords = {Automated Reasoning,Dynamic Adaptation Planning,Self-Adaptive Enterprise Applications},
  file = {/Users/guru/Zotero/storage/KV4IECSE/S1571066116300056.html}
}

@article{peischl2022:testing,
  title = {Testing Anticipatory Systems: {{A}} Systematic Mapping Study on the State of the Art},
  shorttitle = {Testing Anticipatory Systems},
  author = {Peischl, Bernhard and Tazl, Oliver A. and Wotawa, Franz},
  year = {2022},
  month = oct,
  journal = {Journal of Systems and Software},
  volume = {192},
  pages = {111387},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2022.111387},
  urldate = {2023-01-15},
  abstract = {Context: Systems exhibiting anticipatory behavior are controlling devices that are influencing decisions critical to business with increasing frequency, but testing such systems has received little attention from the artificial intelligence or software engineering communities. Goal: In this article, we describe research activities being carried out to test anticipatory systems and explore how this research contributes to the body of knowledge. In addition, we review the types of addressed anticipatory applications and point out open issues and trends. Method: This systematic mapping study was conducted to classify and analyze the literature on testing anticipatory systems, enabling us to highlight the most relevant topics and potential gaps in this field. Results: We identified 206 studies that contribute to the testing of systems that exhibit anticipatory behavior. The papers address testing at stages such as context sensing, inferring higher-level concepts from the sensed data, predicting the future context, and intelligent decision-making. We also identified agent testing as a trend, among others. Conclusion: The existing literature on testing anticipatory systems has originated from various research communities, such as those on autonomous agents and quality engineering. Although researchers have recently exhibited increasing interest in testing anticipatory systems, theoretical knowledge about testing such systems is lacking.},
  langid = {english},
  keywords = {Anticipatory systems,Artificial intelligence,Mapping study,Software testing,Validation,Verification},
  file = {/Users/guru/Zotero/storage/VC8L42T4/Peischl et al_2022_Testing anticipatory systems_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/6B493N56/S016412122200108X.html}
}

@inproceedings{Peixoto20141,
  type = {Conference Paper},
  title = {The Issues of Solving Staffing and Scheduling Problems in Software Development Projects},
  author = {Peixoto, Daniela C.C. and Mateus, Geraldo R. and Resende, Rodolfo F.},
  year = {2014},
  series = {Proceedings - {{International Computer Software}} and {{Applications Conference}}},
  pages = {1--10},
  doi = {10.1109/COMPSAC.2014.96},
  abstract = {Search-Based Software Engineering (SBSE) applies search-based optimization techniques in order to solve complex Software Engineering problems. In the recent years there has been a dramatic increase in the number of SBSE applications in areas such as Software Test, Requirements Engineering, and Project Planning. Our focus is on the analysis of the literature in Project Planning, specifically the researches conducted in software project scheduling and resource allocation. SBSE project scheduling and resource allocation solutions basically use optimization algorithms. Considering the results of a previous Systematic Literature Review, in this work, we analyze the issues of adopting these optimization algorithms in what is considered typical settings found in software development organizations. We found few evidence signaling that the expectations of software development organizations are being attended. \textcopyright{} 2014 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{pereira2019:dms,
  title = {A {{DMS}} to {{Support Industrial Process Decision-Making}}: A Contribution under {{Industry}} 4.0},
  shorttitle = {A {{DMS}} to {{Support Industrial Process Decision-Making}}},
  author = {Pereira, M. T. and Silva, A. and Ferreira, L. P. and S{\'a}, J. C. and Silva, F. J. G.},
  year = {2019},
  month = jan,
  journal = {Procedia Manufacturing},
  series = {29th {{International Conference}} on {{Flexible Automation}} and {{Intelligent Manufacturing}} ( {{FAIM}} 2019), {{June}} 24-28, 2019, {{Limerick}}, {{Ireland}}, {{Beyond Industry}} 4.0: {{Industrial Advances}}, {{Engineering Education}} and {{Intelligent Manufacturing}}},
  volume = {38},
  pages = {613--620},
  issn = {2351-9789},
  doi = {10.1016/j.promfg.2020.01.079},
  urldate = {2023-01-15},
  abstract = {This paper presents the development of an Information System (IS) an automotive manufacturing.The system was developed to reduce waste and increase productivity, through better decision-making. The used methodology to implement it was the Business Process Management (BPM). A DMS (Document Management System) for decision making support was built, reducing the action time and providing faster maintenance of all needed data. As a result, the users time decision making and the administrator maintenance was reduced by a total of 26 minutes, corresponding to a 1.61 \texteuro{} reduction per unit built, which means an efficiency of 41\%. The present work fits in the company's strategy for Industry 4.0 and a more sustainable environment, being a positive driver of Industry 4.0 implementation and transformation.},
  langid = {english},
  keywords = {Business Process Management,Decision Support System,Document Management System,Industry 4.0,Information System},
  file = {/Users/guru/Zotero/storage/DA6NEYHS/S2351978920300809.html}
}

@article{Perkusich2020,
  type = {Article},
  title = {Intelligent Software Engineering in the Context of Agile Software Development: {{A}} Systematic Literature Review},
  author = {Perkusich, Mirko and {Chaves e Silva}, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorg{\^o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2020},
  journal = {Information and Software Technology},
  volume = {119},
  doi = {10.1016/j.infsof.2019.106241},
  abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an ``intelligent technique'' as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers. \textcopyright{} 2019 Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{perkusich2020:intelligent,
  title = {Intelligent Software Engineering in the Context of Agile Software Development: {{A}} Systematic Literature Review},
  shorttitle = {Intelligent Software Engineering in the Context of Agile Software Development},
  author = {Perkusich, Mirko and {Chaves e Silva}, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorg{\^o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2020},
  month = mar,
  journal = {Information and Software Technology},
  volume = {119},
  pages = {106241},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2019.106241},
  urldate = {2023-01-15},
  abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an ``intelligent technique'' as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.},
  langid = {english},
  keywords = {Agile software development,Artificial intelligence,Bayesian networks,Intelligent software engineering,Machine learning,Search-based software engineering},
  file = {/Users/guru/Zotero/storage/84PDW6A5/S0950584919302587.html}
}

@inproceedings{Perkusich2021235,
  type = {Conference Paper},
  title = {Evaluating a {{Bayesian}} Network to Predict Customer Satisfaction in Scrum Software Development Projects: {{An}} Empirical Study with One Company},
  author = {Perkusich, Mirko and Guimaraes, Gleyser and Gorgonio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2021},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2021-July},
  pages = {235--240},
  doi = {10.18293/SEKE2021-112},
  abstract = {Using knowledge-based systems for helping agile teams to improve their performance is not a fact in the industry. In previous work, we have presented Kaizen, a knowledge-based Bayesian network for assisting Scrum teams in diagnosing their value stream in light of the predicted Customer Satisfaction and, consequently, improve their performance. This study assesses Kaizen's accuracy to predict Customer Satisfaction using real-world data. We adopted Kaizen for one software development company and collected data from 18 projects using an online questionnaire. We collected two types of data: inputs for Kaizen and the expected Customer satisfaction. We used the first type of collected data as inputs for Kaizen to calculate the predicted Customer satisfaction. Then, we assessed Kaizen's accuracy by comparing the predicted (i.e., calculated) and expected (i.e., collected) Customer satisfaction using face value and the average Brier score. Considering the face value, Kaizen predicted Customer Satisfaction correctly for 14 out of the 18 projects. The average Brier Score was 0.16. The model predicts, with satisfactory accuracy, the Customer Satisfaction and systemizes the process for Scrum teams to self-diagnose, enabling for causal analysis and supporting their continuous improvement. \textcopyright{} 2021 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Pfaff2022,
  type = {Conference Paper},
  title = {A Code Search Engine for Software Ecosystems},
  author = {Pfaff, Chris and Baninemeh, Elena and Farshidi, Siamak and Jansen, Slinger},
  year = {2022},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {3245},
  abstract = {Searching and reusing source code play an increasingly significant role in the daily tasks of software developers. While code repositories, such as GitHub and Stackoverflow, may provide some results, a code search engine is generally considered most helpful when searching for code snippets as they typically crawl data from a wide range of code repositories. Code search engines enable software developers to search for code snippets using search terms. The accuracy of the search results can be increased if the searchers' intent can be modeled and predicted correctly. This study proposes a novel code search engine to model user intents through a dialogue system and then suggests a ranked list of code snippets that can meet user requirements. \textcopyright{} 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0)},
  publication_stage = {Final},
  source = {Scopus}
}

@article{pinheiro2014:development,
  title = {Development of a Mechanical Maintenance Training Simulator in {{OpenSimulator}} for {{F-16}} Aircraft Engines},
  author = {Pinheiro, Andr{\'e} and Fernandes, Paulo and Maia, Ana and Cruz, Gon{\c c}alo and Pedrosa, Daniela and Fonseca, Benjamim and Paredes, Hugo and Martins, Paulo and Morgado, Leonel and Rafael, Jorge},
  year = {2014},
  month = dec,
  journal = {Entertainment Computing},
  volume = {5},
  number = {4},
  pages = {347--355},
  issn = {1875-9521},
  doi = {10.1016/j.entcom.2014.06.002},
  urldate = {2023-01-15},
  abstract = {Mechanical maintenance of F-16 engines is carried out as a team effort involving 3\textendash 4 skilled engine technicians, but the details of its procedures and requisites change constantly, to improve safety, optimize resources, and respond to knowledge learned from field outcomes. This provides a challenge for development of training simulators, since simulated actions risk becoming obsolete rapidly and require costly reimplementation. This paper presents the development of a 3D mechanical maintenance training simulator for this context, using a low-cost simulation platform and a software architecture that separates simulation control from simulation visualization, in view of enabling more agile adaptation of simulators. This specific simulator aims to enable technician training to be enhanced with cooperation and context prior to the training phase with actual physical engines. We provide data in support of the feasibility of this approach, describing the requirements that were identified with the Portuguese Air Force, the overall software architecture of the system, the current stage of the prototype, and the outcomes of the first field tests with users.},
  langid = {english},
  keywords = {Aircraft engine maintenance,Cooperation,OpenSimulator,Task coordination,Virtual learning,Virtual worlds},
  file = {/Users/guru/Zotero/storage/XZBBWL6Q/S1875952114000184.html}
}

@article{Pohl2011201,
  type = {Article},
  title = {Intelligent Software for Ecological Building Design},
  author = {Pohl, Jens and Assal, Hisham and Pohl, Kym Jason},
  year = {2011},
  journal = {Intelligent Decision Technologies},
  volume = {5},
  number = {3},
  pages = {201--217},
  doi = {10.3233/IDT-2011-0107},
  abstract = {Building design is a complex process because of the number of elements and issues involved and the number of relationships that exist among them. Adding sustainability issues to the list increases the complexity of design by an order of magnitude. There is a need for computer assistance to manage the increased complexity of design and to provide intelligent collaboration in formulating acceptable design solutions. Software development technology today offers opportunities to design and build an intelligent software system environment that can serve as a reliable intelligent partner to the human designer. In this paper the authors discuss the requirements for an intelligent software design environment, explain the major challenges in designing this environment, propose an architecture for an intelligent design support system for sustainable design and present the existing technologies that can be used to implement that architecture. \textcopyright{} 2011 - IOS Press and the authors.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{prada2020:agentbased,
  title = {Agent-Based Testing of Extended Reality Systems},
  booktitle = {2020 {{IEEE}} 13th International Conference on Software Testing, Validation and Verification ({{ICST}})},
  author = {Prada, Rui and Prasetya, I. S. W. B. and Kifetew, Fitsum and Dignum, Frank and Vos, Tanja E. J. and Lander, Jason and Donnart, Jean-yves and Kazmierowski, Alexandre and Davidson, Joseph and Fernandes, Pedro M.},
  year = {2020},
  month = oct,
  pages = {414--417},
  issn = {2159-4848},
  doi = {10.1109/ICST46399.2020.00051},
  abstract = {Testing for quality assurance (QA) is a crucial step in the development of Extended Reality (XR) systems that typically follow iterative design and development cycles. Bringing automation to these testing procedures will increase the productivity of XR developers. However, given the complexity of the XR environments and the User Experience (UX) demands, achieving this is highly challenging. We propose to address this issue through the creation of autonomous cognitive test agents that will have the ability to cope with the complexity of the interaction space by intelligently explore the most prominent interactions given a test goal and support the assessment of affective properties of the UX by playing the role of users.},
  keywords = {agent-based testing,AI-based testing,Cognition,Computational modeling,Extended reality,Games,Software testing,Task analysis,testing computer game,testing virtual reality,user experience testing}
}

@article{Pradel2018291,
  type = {Article},
  title = {A Framework for Mapping Design for Additive Manufacturing Knowledge for Industrial and Product Design},
  author = {Pradel, Patrick and Zhu, Zicheng and Bibb, Richard and Moultrie, James},
  year = {2018},
  journal = {Journal of Engineering Design},
  volume = {29},
  number = {6},
  pages = {291--326},
  doi = {10.1080/09544828.2018.1483011},
  abstract = {Design for Additive Manufacturing (DfAM) is a growing field of enquiry. Over the past few years, the scientific community has begun to explore this topic to provide a basis for supporting professional design practice. However, current knowledge is still largely fragmented, difficult to access and inconsistent in language and presentation. This paper seeks to collate and organise this dispersed but growing body of knowledge, using a single and coherent conceptual framework. The framework is based on a generic design process model and consists of five parts: Conceptual design, Embodiment design, Detail design and Process planning and Process selection. 81 articles on DfAM are mapped onto the framework to provide, for the first time, a clear summary of the state of the art across the whole design process. Nine directions for the future of DfAM research are then proposed. \textcopyright{} 2018, \textcopyright{} 2018 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Pradhan2019402,
  type = {Conference Paper},
  title = {Back to Basics - Redefining Quality Measurement for Hybrid Software Development Organizations},
  author = {Pradhan, Satya and Nanniyur, Venky},
  year = {2019},
  series = {Proceedings - {{International Symposium}} on {{Software Reliability Engineering}}, {{ISSRE}}},
  volume = {2019-October},
  pages = {402--412},
  doi = {10.1109/ISSRE.2019.00047},
  abstract = {As the software industry transitions from a license-based model to a subscription-based Software-as-aService (SaaS) model, many software development groups are using a hybrid development model that incorporates Agile and Waterfall methodologies in different parts of the organization. The traditional metrics used for measuring software quality in Waterfall or Agile paradigms do not apply to this new hybrid methodology. In addition, to respond to higher quality demands from customers and to gain a competitive advantage in the market, many companies are starting to prioritize quality as a strategic differentiator. As a result, quality metrics are included in the decision-making activities all the way up to the executive level, including Board of Director reviews. This paper presents key challenges associated with measuring software quality in organizations using the hybrid development model. We developed a framework called PIER (Prevention-InspectionEvaluation-Removal) to provide a comprehensive metric definition for hybrid organizations. The framework includes quality measurements, quality enforcement, and quality decision points at different organizational levels and project milestones during the software development life cycle (SDLC). The metrics framework defined in this paper is being used for all Cisco Systems products used in customer premises. Preliminary field metrics data for one of the product groups show quality improvement after implementation of the proposed measurement system. \textcopyright{} 2019 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{pradhan2021:large,
  title = {Large Scale Quality Transformation in Hybrid Development Organizations \textendash{} {{A}} Case Study},
  author = {Pradhan, Satya and Nanniyur, Venky},
  year = {2021},
  month = jan,
  journal = {Journal of Systems and Software},
  volume = {171},
  pages = {110836},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2020.110836},
  urldate = {2023-01-15},
  abstract = {As the software industry transitions to a subscription-based software-as-a-service (SaaS) model, software development companies are transforming to hybrid development organizations with increased adoption of Agile and Continuous Integration/ Continuous Delivery (CI/CD) development practices for newer products while continuing to use Waterfall methods for older products. This transformation is a huge undertaking impacting all aspects of the software development life cycle (SDLC), including the quality management system. This paper presents a case study of a large-scale transformation of a legacy quality management system to a modern system developed and implemented at Cisco Systems. The framework for this transformation is defined by six distinct areas: metrics, process, measurement, reporting, quality analytics, and culture \& leadership. Our implementation leveraged recent advances in Machine Learning (ML), Artificial Intelligence (AI), connected data, integrated operations, and big data technologies to solve the challenges created by a hybrid software development organization. We believe this case study will help researchers and industry leaders understand the benefits and potential challenges of such sizeable transformations.},
  langid = {english},
  keywords = {Agile,Hybrid development organization,Quality management system,Quality transformation,Waterfall},
  file = {/Users/guru/Zotero/storage/SF8HUBAN/Pradhan_Nanniyur_2021_Large scale quality transformation in hybrid development organizations – A case_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/2HSNBQSP/S0164121220302284.html}
}

@inproceedings{pyae2018investigating,
  title = {Investigating the Usability and User Experiences of Voice User Interface: A Case of {{Google}} Home Smart Speaker},
  booktitle = {Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct},
  author = {Pyae, Aung and Joelsson, Tapani N},
  year = {2018},
  pages = {127--131}
}

@article{qureshi2023:medical,
  title = {Medical Image Segmentation Using Deep Semantic-Based Methods: {{A}} Review of Techniques, Applications and Emerging Trends},
  shorttitle = {Medical Image Segmentation Using Deep Semantic-Based Methods},
  author = {Qureshi, Imran and Yan, Junhua and Abbas, Qaisar and Shaheed, Kashif and Riaz, Awais Bin and Wahid, Abdul and Khan, Muhammad Waseem Jan and Szczuko, Piotr},
  year = {2023},
  month = feb,
  journal = {Information Fusion},
  volume = {90},
  pages = {316--352},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2022.09.031},
  urldate = {2023-01-15},
  abstract = {Semantic-based segmentation (Semseg) methods play an essential part in medical imaging analysis to improve the diagnostic process. In Semseg technique, every pixel of an image is classified into an instance, where each class is corresponded by an instance. In particular, the semantic segmentation can be used by many medical experts in the domain of radiology, ophthalmologists, dermatologist, and image-guided radiotherapy. The authors present perspectives on the development of an architectural, and operational mechanism of each machine learning-based semantic segmentation approach with merits and demerits. In this regard, researchers have proposed different Semseg methods and examined their performance in a variety of applications such as medical image analysis (e.g., medical image classification and segmentation). A review of recent advances in Semseg techniques are presented in this paper by applying computational image processing and machine learning methods. This article is further presented a comprehensive investigation on how different architectures are helpful for medical image segmentation. Finally, advantages, open challenges, and possible future directions are elaborated in the discussion part, beneficial to the research community to understand the significance of the available medical imaging segmentation technology based on Semseg and thus deliver robust segmentation solutions.},
  langid = {english},
  keywords = {Deep learning,Medical imaging,Optimization techniques,Semantic segmentation,Transfer learning},
  file = {/Users/guru/Zotero/storage/4SFQKZPA/S1566253522001695.html}
}

@inproceedings{Rabe2021129,
  type = {Conference Paper},
  title = {Development Methodologies for Safety Critical Machine Learning Applications in the Automotive Domain: {{A}} Survey},
  author = {Rabe, Martin and Milz, Stefan and Mader, Patrick},
  year = {2021},
  series = {{{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  pages = {129--141},
  doi = {10.1109/CVPRW53098.2021.00023},
  abstract = {Enabled by recent advances in the field of machine learning, the automotive industry pushes towards automated driving. The development of traditional safety-critical automotive software is subject to rigorous processes, ensuring its dependability while decreasing the probability of failures. However, the development and training of machine learning applications substantially differs from traditional software development. The processes and methodologies traditionally prescribed are unfit to account for specifics like, e.g., the importance of datasets for a development. We perform a systematic mapping study surveying methodologies proposed for the development of machine learning applications in the automotive domain. We map the identified primary publications to a general machine learning-based development process and preliminary assess their maturity. The reviews's goal is providing a holistic view of current and previous research contributing to ML-aware development processes and identifying challenges that need more attention. Additionally, we list methods, network architectures, and datasets used within these publications. Our meta-study identifies that model training and model VV received by far the most research attention accompanied by the most mature evaluations. The remaining development phases, concerning domain specification, data management, and model integration, appear underrepresented and in need of more thorough research. Additionally, we identify and aggregate typically methods applied when developing automated driving applications like models, datasets and simulators showing the state of practice in this field. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/KK4E9E3V/Rabe et al. - 2021 - Development methodologies for safety critical mach.pdf}
}

@inproceedings{Radhakrishnan2021,
  type = {Conference Paper},
  title = {Explainable Artificial Intelligence ({{XAI}}) in Project Management Curriculum: {{Exploration}} and Application to Time, Cost, and Risk},
  author = {Radhakrishnan, Ben D. and Jaurez, James J. and Altamirano, Nelson},
  year = {2021},
  series = {{{ASEE Annual Conference}} and {{Exposition}}, {{Conference Proceedings}}},
  publication_stage = {Final},
  source = {Scopus}
}

@article{rafael2020:industry,
  title = {An {{Industry}} 4.0 Maturity Model for Machine Tool Companies},
  author = {Rafael, Lizarralde Dorronsoro and Jaione, Ganzarain Epelde and Cristina, L{\'o}pez and Ibon, Serrano Lasa},
  year = {2020},
  month = oct,
  journal = {Technological Forecasting and Social Change},
  volume = {159},
  pages = {120203},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2020.120203},
  urldate = {2023-01-15},
  abstract = {Industry 4.0 is the new productive paradigm that is driving the 4th industrial revolution. The specific difficulties in adapting to this new approach pose challenges for Machine Tool (MT) companies, mostly Small and Medium Enterprises (SME). In order to implement a route appropriately, tools such as the Maturity Model (MM) can be very useful, since they help to evaluate the initial state of the company and to plan a development road map. Over the last few years, several MMs geared towards Industry 4.0 have been created and released, some of which are specifically aimed at certain sectors. However, there is none specially developed for an industry of such vital importance as the MT sector. This article presents a new MM adapted to this type of company, with a design based on previously validated developments and standards relating to MM. A representative company in the sector was chosen as a case study for testing the new tool, with excellent results. It was decided that the next step should be to disseminate it to more companies so that it could become a standard MM in the sector.},
  langid = {english},
  keywords = {Digitalization,Implementation process,Industry 4.0,Maturity model,Technological providers},
  file = {/Users/guru/Zotero/storage/NXWDY3DC/Rafael et al_2020_An Industry 4_Technological Forecasting and Social Change.pdf;/Users/guru/Zotero/storage/KNLJ7X58/S0040162520310295.html}
}

@article{rahman2021:secure,
  title = {A Secure, Private, and Explainable {{IoHT}} Framework to Support Sustainable Health Monitoring in a Smart City},
  author = {Rahman, Md Abdur and Hossain, M. Shamim and Showail, Ahmad J. and Alrajeh, Nabil A. and Alhamid, Mohammed F.},
  year = {2021},
  month = sep,
  journal = {Sustainable Cities and Society},
  volume = {72},
  pages = {103083},
  issn = {2210-6707},
  doi = {10.1016/j.scs.2021.103083},
  urldate = {2023-01-15},
  abstract = {Internet of Health Things (IoHT) have allowed connected health paradigm ubiquitous. 5\,G supported healthcare vertical allows IoHT to offer connected health monitoring with quality of service and ultra-low latency. Deep learning has shown potential in processing massive amount of IoHT data that are generated daily, automate connected healthcare workflows, and help in decision making processes. However, three important challenges need to be addressed to attain long term healthcare-related sustainability \textendash{} data security, data privacy, and social acceptance of deep learning process. In this paper, we propose a framework that will allow healthcare sustainability through the following contributions 1) ensure privacy of training dataset, 2) support the aggregation of the global model gradients through a private Blockchain-brokered entity, 3) support trustworthiness and provenance of the federated clients by blockchain and off-chain, 4) share the dataset, train the model and share trained model among the federated clients in an encrypted fashion, and 5) add explainability and reasoning of deep learning process to make the model acceptable by the society. We will present the detailed design of our proposed sustainable system, the implementation details and test results. The test results show promising prospect of achieving sustainability of IoHT-enabled connected health applications.},
  langid = {english},
  keywords = {5G healthcare vertical,Blockchain,Connected living,Explainable AI,Internet of health things,Off-chain,Sustainable cities},
  file = {/Users/guru/Zotero/storage/5NB7Y3Z3/S221067072100367X.html}
}

@inproceedings{Rahman202221,
  type = {Conference Paper},
  title = {Challenges in Machine Learning Application Development: {{An}} Industrial Experience Report},
  author = {Rahman, Md Saidur and Khomh, Foutse and Rivera, Emilio and Gueheneuc, Yann-Gael and Lehnert, Bernd},
  year = {2022},
  series = {Proceedings - {{Workshop}} on {{Software Engineering}} for {{Responsible AI}}, {{SE4RAI}} 2022},
  pages = {21--28},
  doi = {10.1145/3526073.3527593},
  abstract = {SAP is the market leader in enterprise application software offering an end-to-end suite of applications and services to enable their customers worldwide to operate their business. Especially, retail customers of SAP deal with millions of sales transactions for their day-to-day business. Transactions are created during retail sales at the point of sale (POS) terminals and those transactions are then sent to some central servers for validations and other business operations. A considerable proportion of the retail transactions may have inconsistencies or anomalies due to many technical and human errors. SAP provides an automated process for error detection but still requires a manual process by dedicated employees using workbench software for correction. However, manual corrections of these errors are time-consuming, labor-intensive, and might be prone to further errors due to incorrect modifications. Thus, automated detection and correction of transaction errors are very important regarding their potential business values and the improvement in the business workflow. In this paper, we report on our experience from a project where we develop an AI-based system to automatically detect transaction errors and propose corrections. We identify and discuss the challenges that we faced during this collaborative research and development project, from two distinct perspectives: Software Engineering and Machine Learning. We report on our experience and insights from the project with guidelines for the identified challenges. We collect developers' feedback for qualitative analysis of our findings. We believe that our findings and recommendations can help other researchers and practitioners embarking into similar endeavours. CCS CONCEPTS \textbullet{} Software and its engineering \textrightarrow{} Programming teams. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/M8AUM8GN/Rahman et al. - 2022 - Challenges in machine learning application develop.pdf}
}

@incollection{raj2020:chapter,
  title = {Chapter {{One}} - {{Stepping}} into the Digitally Instrumented and Interconnected Era},
  booktitle = {Advances in {{Computers}}},
  author = {Raj, Pethuru and Lin, Jenn-Wei},
  editor = {Raj, Pethuru and Evangeline, Preetha},
  year = {2020},
  month = jan,
  series = {The {{Digital Twin Paradigm}} for {{Smarter Systems}} and {{Environments}}: {{The Industry Use Cases}}},
  volume = {117},
  pages = {1--34},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.adcom.2019.09.008},
  urldate = {2023-01-15},
  abstract = {This chapter is to tell all about the digitization-inspired possibilities and opportunities and how software-defined cloud centers are the best fit for hosting and running digital applications. Also, how the next-generation data analytics can be smartly accomplished through cloud platforms and infrastructures is also explained in detail. We are to describe some of the impactful developments and technological advancements brewing in the IT space, how the tremendous amount of data getting produced and processed through cloud systems is to impact the IT and business domains, and how next-generation IT infrastructures are accordingly getting refactored, remedied and readied for the impending big data-induced challenges, how likely the move of the data analytics discipline toward fulfilling the digital universe requirements of extracting and extrapolating actionable insights for the knowledge-parched is, and finally for the establishment and sustenance of the dreamt smarter planet. In short, the uninhibited explosion of digitized systems and connected devices pour out a tremendous amount of multi-structured data and the impending challenge is to make sense out of the data heaps. Data analytics is the way to go and in the recent past, the overwhelming trend is to empower our everyday systems with machine and deep learning algorithms to automatically learn out of data heaps and streams in order to be distinctively intelligent in their actions and reactions. This chapter is specially prepared to put a stimulating foundation for explaining the nitty-gritty of the Digital Twin paradigm.},
  langid = {english},
  keywords = {Artificial intelligence,Cyber-physical systems,Edge computing,Real-time applications,The internet of things},
  file = {/Users/guru/Zotero/storage/DJR6ZKRJ/Raj_Lin_2020_Chapter One - Stepping into the digitally instrumented and interconnected era_Advances in Computers.pdf;/Users/guru/Zotero/storage/AA88DC6L/S0065245819300518.html}
}

@incollection{raj2022:chapter,
  title = {Chapter {{Five}} - {{The}} Edge {{AI}} Paradigm: {{Technologies}}, Platforms and Use Cases},
  shorttitle = {Chapter {{Five}} - {{The}} Edge {{AI}} Paradigm},
  booktitle = {Advances in {{Computers}}},
  author = {Raj, Pethuru and Akilandeswari, J. and Marimuthu, M.},
  editor = {Raj, Pethuru and Saini, Kavita and Surianarayanan, Chellammal},
  year = {2022},
  month = jan,
  series = {Edge/{{Fog Computing Paradigm}}: {{The Concept Platforms}} and {{Applications}}},
  volume = {127},
  pages = {139--182},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.adcom.2022.02.003},
  urldate = {2023-01-15},
  abstract = {Two of the most interesting topics in the technology world today are edge computing and artificial intelligence (Ai). Individually each of them has done exceedingly well in contributing for the betterment of the society. Now if they converge, we can solidly expect a paradigm shift and the impacts will be mesmerizingly profound and phenomenal. The fusion of these two popular technologies is laying down a spectacular foundation for creating new kinds of experiences and opportunities for people. Newer possibilities can easily come up across business verticals. There will be premium and people-centric services emerging and evolving for automating and accelerating a number of manual activities in our daily lives. This unique combination can result in fructifying the longstanding demand of people IT. These also smoothen the path and clear the route for the age-old transition from business IT to people IT. This convergence directly influences peoples' lives in a distinguishing and deft manner. This can save time, improve privacy, reduce network traffic, and enable applications or devices to be optimized for specific environments. Precisely speaking, the convergence of these two strategically sound technologies can bring forth noteworthy advancements and accomplishments for the total society. In this chapter, we are to see how this unique linkage is going to be hugely beneficial for institutions, individuals and innovators. The union is being termed as ``Edge AI.''},
  langid = {english},
  keywords = {Artificial intelligence,Edge AI,Edge computing,Edge data analytics,Edge device clouds},
  file = {/Users/guru/Zotero/storage/HGIPHYUJ/S0065245822000328.html}
}

@incollection{ralph2009proposal,
  title = {A Proposal for a Formal Definition of the Design Concept},
  booktitle = {Design Requirements Engineering: {{A}} Ten-Year Perspective},
  author = {Ralph, Paul and Wand, Yair},
  year = {2009},
  pages = {103--136},
  publisher = {{Springer}}
}

@article{Ramazanzadeh2022141,
  type = {Article},
  title = {{{ASATM}}: {{Automated}} Security Assistant of Threat Models in Intelligent Transportation Systems},
  author = {Ramazanzadeh, Mohammad Ali and Barzegar, Behnam and Motameni, Homayun},
  year = {2022},
  journal = {IET Computers and Digital Techniques},
  volume = {16},
  number = {5-6},
  pages = {141--158},
  doi = {10.1049/cdt2.12045},
  abstract = {The evolution of technology has led to the appearance of smart cities. An essential element in such cities is smart mobility that covers the subjects related to Intelligent Transportation Systems (ITS). The problem is that the ITS vulnerabilities may considerably harm the life quality and safety status of human beings living in smart cities. In fact, software and hardware systems are more exposed to security risks and threats. To reduce threats and secure software design, threat modelling has been proposed as a preventive solution in the software design phase. On the other hand, threat modelling is always criticised for being time consuming, complex, difficult, and error prone. The approach proposed in this study, that is, Automated Security Assistant of Threat Models (ASATM), is an automated solution that is capable of achieving a high level of security assurance. By defining concepts and conceptual modelling as well as implementing automated security assistant algorithms, ASATM introduces a new approach to identifying threats, extracting security requirements, and designing secure software. The proposed approach demonstrates a quantitative classification of security at three levels (insecure, secure, and threat), twelve sub-levels (nominal scale and colour scale), and a five-layer depth (human understandability and conditional probability). In this study, to evaluate the effectiveness of our approach, an example with various security parameters and scenarios was tested and the results confirmed the superiority of the proposed approach over the latest threat modelling approaches in terms of method, learning, and model understanding. \textcopyright{} 2022 The Authors. IET Computers \& Digital Techniques published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Ramos2018149,
  type = {Conference Paper},
  title = {A Non-Functional Requirements Recommendation System for Scrum-Based Projects},
  author = {Ramos, Felipe and Costa, Alexandre and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo},
  year = {2018},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2018-July},
  pages = {149--154},
  doi = {10.18293/SEKE2018-107},
  abstract = {Agile software development focuses on quick delivery and flexibility to change. Despite being effective in delivering quality functional requirements, agile practices tend to neglect non-functional requirements until the later stages of software development. This work focuses on Scrum, the most popular agile method, and presents a non-functional requirements recommendation system to support Scrum practitioners on their early identification. The solution is based on instrumenting the Scrum process to extract useful data and the use of collaborative filtering and item recommendation. To evaluate the recommendations, we conducted off-line experiments with data collected from 12 Scrum practitioners through a survey. The data was analyzed using 10-fold cross-validation. As a result, our proposed solution showed a recall rate of up to 81\%, which indicates that it is a promising approach to recommend non-functional requirements given a set of functional requirements identified by project stakeholders. \textcopyright{} 2018 Universitat zu Koln. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Ramos201926,
  type = {Conference Paper},
  title = {Evaluating Software Developers' Acceptance of a Tool for Supporting Agile Non-Functional Requirement Elicitation},
  author = {Ramos, Felipe and Pedro, Ant{\^o}nio and Cesar, Marcos and Costa, Alexandre and Perkusich, Mirko and Almeida, Hyggo and Perkusich, Angelo},
  year = {2019},
  series = {Proceedings of the {{International Conference}} on {{Software Engineering}} and {{Knowledge Engineering}}, {{SEKE}}},
  volume = {2019-July},
  pages = {26--31},
  doi = {10.18293/SEKE2019-107},
  abstract = {Due to the need for flexibility to requirements changes, agile software development methods have been attracting the attention of academic and industrial domains. Unlike traditional approaches, agile methods focus on the rapid delivery of business value to customers through empirical and incremental development processes. Despite being effective in delivering quality functional requirements, agile practices generally neglect non-functional requirements until the later stages of software development. However, neglecting non-functional requirements during requirements analysis can lead to project failures. In this paper, we present the NFRec tool, which aims to support software developers in the elicitation of non-functional requirements in the context of agile software development. Additionally, we report the results from a case study to evaluate the acceptance of the NFRec tool from the point of view of software developers of four projects from a Brazilian software company. To gather information about the tool acceptance, we applied a questionnaire based on the indicators from the Technology Acceptance Model. Overall, the four teams considered the NFRec tool useful and easy to use for supporting the management of non-functional requirements in agile projects. \textcopyright{} 2019 Knowledge Systems Institute Graduate School. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{randolph2009guide,
  title = {A Guide to Writing the Dissertation Literature Review},
  author = {Randolph, Justus},
  year = {2009},
  journal = {Practical Assessment, Research, and Evaluation},
  volume = {14},
  number = {1},
  pages = {13}
}

@article{rani2023:decade,
  title = {A Decade of Code Comment Quality Assessment: {{A}} Systematic Literature Review},
  shorttitle = {A Decade of Code Comment Quality Assessment},
  author = {Rani, Pooja and Blasi, Arianna and Stulova, Nataliia and Panichella, Sebastiano and Gorla, Alessandra and Nierstrasz, Oscar},
  year = {2023},
  month = jan,
  journal = {Journal of Systems and Software},
  volume = {195},
  pages = {111515},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2022.111515},
  urldate = {2023-01-15},
  abstract = {Code comments are important artifacts in software systems and play a paramount role in many software engineering (SE) tasks related to maintenance and program comprehension. However, while it is widely accepted that high quality matters in code comments just as it matters in source code, assessing comment quality in practice is still an open problem. First and foremost, there is no unique definition of quality when it comes to evaluating code comments. The few existing studies on this topic rather focus on specific attributes of quality that can be easily quantified and measured. Existing techniques and corresponding tools may also focus on comments bound to a specific programming language, and may only deal with comments with specific scopes and clear goals (e.g., Javadoc comments at the method level, or in-body comments describing TODOs to be addressed). In this paper, we present a Systematic Literature Review (SLR) of the last decade of research in SE to answer the following research questions: (i) What types of comments do researchers focus on when assessing comment quality? (ii) What quality attributes (QAs) do they consider? (iii) Which tools and techniques do they use to assess comment quality?, and (iv) How do they evaluate their studies on comment quality assessment in general? Our evaluation, based on the analysis of 2353 papers and the actual review of 47 relevant ones, shows that (i) most studies and techniques focus on comments in Java code, thus may not be generalizable to other languages, and (ii) the analyzed studies focus on four main QAs of a total of 21~QAs identified in the literature, with a clear predominance of checking consistency between comments and the code. We observe that researchers rely on manual assessment and specific heuristics rather than the automated assessment of the comment quality attributes, with evaluations often involving surveys of students and the authors of the original studies but rarely professional developers.},
  langid = {english},
  keywords = {Code comments,Documentation quality,Systematic literature review},
  file = {/Users/guru/Zotero/storage/S9QVBN92/S0164121222001911.html}
}

@article{rasool2022:security,
  title = {Security and Privacy of Internet of Medical Things: {{A}} Contemporary Review in the Age of Surveillance, Botnets, and Adversarial {{ML}}},
  shorttitle = {Security and Privacy of Internet of Medical Things},
  author = {Rasool, Raihan Ur and Ahmad, Hafiz Farooq and Rafique, Wajid and Qayyum, Adnan and Qadir, Junaid},
  year = {2022},
  month = may,
  journal = {Journal of Network and Computer Applications},
  volume = {201},
  pages = {103332},
  issn = {1084-8045},
  doi = {10.1016/j.jnca.2022.103332},
  urldate = {2023-01-15},
  abstract = {Internet of Medical Things (IoMT) supports traditional healthcare systems by providing enhanced scalability, efficiency, reliability, and accuracy of healthcare services. It enables the development of smart hardware as well as software platforms that operate on the basis of communication systems and the algorithms that process the data collected by the sensors to support decision-making. Although IoMT is involved in large-scale services provisioning in the medical paradigm; however, the resource-constrained nature of these devices makes them vulnerable to immense security and privacy issues. These vulnerabilities are not only disastrous for IoMT but threaten the whole healthcare ecosystem, which can in turn bring human lives in danger. During the past few years, threat vectors against IoMT have been evolved in terms of scalability, complexity, and diversity, which makes it challenging to detect and provide stringent defense solutions against these attacks. In this paper, we classify security and privacy challenges against different IoMT variants based on their actual usage in the healthcare domain. We provide a comprehensive attack taxonomy on the overall IoMT infrastructure comprising different device variants as well as elaborate taxonomies of security protocols to mitigate attacks against different devices, algorithms and describe their strengths and weaknesses. We also outline the security and privacy requirements for the development of novel security solutions for all the attack types against IoMT. Finally, we provide a comprehensive list of current challenges and future research directions that must be considered while developing sustainable security solutions for the IoMT infrastructure.},
  langid = {english},
  keywords = {Edge computing,Healthcare,Internet of medical things,Internet of things,Privacy,Security},
  file = {/Users/guru/Zotero/storage/ANYNHBPE/S1084804522000017.html}
}

@inproceedings{rastogi2017:new,
  title = {New {{Directions}} for {{Container Debloating}}},
  booktitle = {Proceedings of the 2017 {{Workshop}} on {{Forming}} an {{Ecosystem Around Software Transformation}}},
  author = {Rastogi, Vaibhav and Niddodi, Chaitra and Mohan, Sibin and Jha, Somesh},
  year = {2017},
  month = nov,
  series = {{{FEAST}} '17},
  pages = {51--56},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3141235.3141241},
  urldate = {2023-01-15},
  abstract = {Application containers, such as Docker containers, are light-weight virtualization environments that "contain" applications together with their resources and configuration information. While they are becoming increasingly popular as a method for agile software deployment, current techniques for preparing containers add unnecessary bloat into them: they often include unneeded files that increase the container size by several orders of magnitude. This not only leads to storage and network transfer issues but also security concerns. The problem is well-recognized but available solutions are mostly ad-hoc and not largely deployed. Our previous work, Cimplifier, on debloating containers uses dynamic analysis to identify the resources necessary to a container and then debloat it. However, the dynamic analysis uses model executions or test runs, which if incomplete, may not allow detection of all the necessary resources. Therefore, it is important to explore other directions towards container debloating. In this paper, we discuss two of them: a new intermediate representation allowing incorporation of multiple techniques, such as dynamic analysis and static analysis, for debloating; and test case augmentation using symbolic execution.},
  isbn = {978-1-4503-5395-3},
  keywords = {containers,debloating,least privilege},
  file = {/Users/guru/Zotero/storage/93NDXDB2/Rastogi et al_2017_New Directions for Container Debloating_Proceedings of the 2017 Workshop on Forming an Ecosystem Around Software Transformation.pdf}
}

@article{Rathee2021,
  type = {Article},
  title = {Artificial Intelligence- ({{AI-}}) Enabled Internet of Things ({{IoT}}) for Secure Big Data Processing in Multihoming Networks},
  author = {Rathee, Geetanjali and Khelifi, Adel and Iqbal, Razi},
  year = {2021},
  journal = {Wireless Communications and Mobile Computing},
  volume = {2021},
  doi = {10.1155/2021/5754322},
  abstract = {The automated techniques enabled with Artificial Neural Networks (ANN), Internet of Things (IoT), and cloud-based services affect the real-time analysis and processing of information in a variety of applications. In addition, multihoming is a type of network that combines various types of networks into a single environment while managing a huge amount of data. Nowadays, the big data processing and monitoring in multihoming networks provide less attention while reducing the security risk and efficiency during processing or monitoring the information. The use of AI-based systems in multihoming big data with IoT- and AI-integrated systems may benefit in various aspects. Although multihoming security issues and their analysis have been well studied by various scientists and researchers; however, not much attention is paid towards big data security processing in multihoming especially using automated techniques and systems. The aim of this paper is to propose an IoT-based artificial network to process and compute big data processing by ensuring a secure communication multihoming network using the Bayesian Rule (BR) and Levenberg-Marquardt (LM) algorithms. Further, the efficiency and effect on multihoming information processing using an AI-assisted mechanism are experimented over various parameters such as classification accuracy, classification time, specificity, sensitivity, ROC, and F-measure. \textcopyright{} 2021 Geetanjali Rathee et al.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{reiss1996software,
  title = {Software Tools and Environments},
  author = {Reiss, Steven P},
  year = {1996},
  journal = {ACM Computing Surveys (CSUR)},
  volume = {28},
  number = {1},
  pages = {281--284},
  publisher = {{ACM New York, NY, USA}}
}

@article{renggli2019:ease,
  title = {Ease.Ml/Ci and {{Ease}}.Ml/Meter in Action: Towards Data Management for Statistical Generalization},
  shorttitle = {Ease.Ml/Ci and {{Ease}}.Ml/Meter in Action},
  author = {Renggli, Cedric and Hubis, Frances Ann and Karla{\v s}, Bojan and Schawinski, Kevin and Wu, Wentao and Zhang, Ce},
  year = {2019},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {12},
  pages = {1962--1965},
  issn = {2150-8097},
  doi = {10.14778/3352063.3352110},
  urldate = {2023-01-15},
  abstract = {Developing machine learning (ML) applications is similar to developing traditional software --- it is often an iterative process in which developers navigate within a rich space of requirements, design decisions, implementations, empirical quality, and performance. In traditional software development, software engineering is the field of study which provides principled guidelines for this iterative process. However, as of today, the counterpart of "software engineering for ML" is largely missing --- developers of ML applications are left with powerful tools (e.g., TensorFlow and PyTorch) but little guidance regarding the development lifecycle itself. In this paper, we view the management of ML development life-cycles from a data management perspective. We demonstrate two closely related systems, ease.ml/ci and ease.ml/meter, that provide some "principled guidelines" for ML application development: ci is a continuous integration engine for ML models and meter is a "profiler" for controlling overfitting of ML models. Both systems focus on managing the "statistical generalization power" of datasets used for assessing the quality of ML applications, namely, the validation set and the test set. By demonstrating these two systems we hope to spawn further discussions within our community on building this new type of data management systems for statistical generalization.},
  file = {/Users/guru/Zotero/storage/4RR6JX87/Renggli et al_2019_Ease_Proceedings of the VLDB Endowment.pdf}
}

@article{retolaza2021:design,
  title = {Design to Cost; a Framework for Large Industrial Products},
  author = {Retolaza, Iban and Ezpeleta, I{\~n}igo and Santos, Adrian and Diaz, Iban and Martinez, Felix},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {31st {{CIRP Design Conference}} 2021 ({{CIRP Design}} 2021)},
  volume = {100},
  pages = {828--833},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2021.05.036},
  urldate = {2023-01-15},
  abstract = {One of the main aspects that must be considered while developing any industrial project is its cost. Design to Cost (DtC) processes allow the design team to manage any kind of project based on the costs of each of the phases and of the elements, by applying a number of methodologies, techniques, cost models and tools. In this paper, a DtC framework is proposed, taking other DtC centered research as references and focusing on a probabilistic approach, in order to lead the early design stages of large industrial products, where most of the information is unclear or is still being defined. A literature review about DtC methodologies and techniques is first carried out in order to analyze, compare and classify them and identify the strengths and weaknesses of each one.},
  langid = {english},
  keywords = {Cost estimation,Cost management,Cost model,Design to Cost,DfX},
  file = {/Users/guru/Zotero/storage/WK4Y73H8/Retolaza et al_2021_Design to cost\; a framework for large industrial products_Procedia CIRP.pdf;/Users/guru/Zotero/storage/WNC7VH9T/S2212827121004984.html}
}

@inproceedings{reza2021:mooclet,
  title = {The {{MOOClet Framework}}: {{Unifying Experimentation}}, {{Dynamic Improvement}}, and {{Personalization}} in {{Online Courses}}},
  shorttitle = {The {{MOOClet Framework}}},
  booktitle = {Proceedings of the {{Eighth ACM Conference}} on {{Learning}} @ {{Scale}}},
  author = {Reza, Mohi and Kim, Juho and Bhattacharjee, Ananya and Rafferty, Anna N. and Williams, Joseph Jay},
  year = {2021},
  month = jun,
  series = {L@{{S}} '21},
  pages = {15--26},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3430895.3460128},
  urldate = {2023-01-15},
  abstract = {How can educational platforms be instrumented to accelerate the use of research to improve students' experiences? We show how modular components of any educational interface - e.g. explanations, homework problems, even emails - can be implemented using the novel MOOClet software architecture. Researchers and instructors can use these augmented MOOClet components for: (1) Iterative Cycles of Randomized Experiments that test alternative versions of course content; (2) Data-Driven Improvement using adaptive experiments that rapidly use data to give better versions of content to future students, on the order of days rather than months. A MOOClet supports both manual and automated improvement using reinforcement learning; (3) Personalization by delivering alternative versions as a function of data about a student's characteristics or subgroup, using both expert-authored rules and data mining algorithms. We provide an open-source web service for implementing MOOClets (www.mooclet.org) that has been used with thousands of students. The MOOClet framework provides an ecosystem that transforms online course components into collaborative micro-laboratories, where instructors, experimental researchers, and data mining/machine learning researchers can engage in perpetual cycles of experimentation, improvement, and personalization.},
  isbn = {978-1-4503-8215-1},
  keywords = {A/B comparisons,dynamic improvement,education technology,massive open online courses,multi-armed bandits,personalization,randomized experiments},
  file = {/Users/guru/Zotero/storage/6A3T8UKI/Reza et al_2021_The MOOClet Framework_Proceedings of the Eighth ACM Conference on Learning @ Scale.pdf}
}

@inproceedings{Rique2022,
  type = {Conference Paper},
  title = {Empirically Derived Use Cases for Software Analytics},
  author = {Rique, Thiago and Dantas, Emanuel and Perkusich, Mirko and Gorgonio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  series = {2022 30th {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}}, {{SoftCOM}} 2022},
  doi = {10.23919/SoftCOM55329.2022.9911514},
  abstract = {[Background] Software engineering activities provide large volumes of data that software analytics tools can use to support decision-making. However, adopting such tools depends on the usefulness of the information provided regarding the needs of practitioners. While the needs of developers have been well-researched, the needs of managers are not getting as much attention. [Aims] This study provides an in-depth analysis of the needs of software practitioners involved in managerial decision-making from one organization that performs research, development, and innovation projects with industry partners. [Method] We identified and represented such needs as use cases by interviewing people in leadership positions and analyzing the collected data using Grounded Theory coding techniques, i.e., open and selective coding. [Results] Our analysis resulted in 19 software analytics use cases which we classified into four dimensions: quality, people, project management, and knowledge management. The use cases in the quality and project management dimensions were the most mentioned ones. [Conclusions] Although our results are particularly relevant to organizations similar to the one described herein, they aim to serve as input for implementing new analytics solutions by practitioners and researchers. \textcopyright{} 2022 University of Split, FESB.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Rique2022388,
  type = {Conference Paper},
  title = {Use Cases for Software Development Analytics: {{A}} Case Study},
  author = {Rique, Thiago and Dantas, Emanuel and Perkusich, Mirko and Gorg{\^o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
  year = {2022},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {388--397},
  doi = {10.1145/3555228.3555239},
  abstract = {Context Software engineering activities provide practitioners with large volumes of data that software analytics tools can use for many purposes, including defect prediction and effort estimation. However, the adoption of such tools depends on the information they provide and the real needs of practitioners. While existing research has focused on what developers need, the needs of managers are not well understood. Aims This study provides an in-depth analysis of the information needs of software practitioners from one organization that performs research, development, and innovation projects with industry partners. Understanding these practitioners' needs enables the development of better analytics solutions to support managerial decision-making. Method We interviewed practitioners in leadership positions and analyzed the collected data using Grounded Theory coding techniques, i.e., open and selective coding. Results We identified 19 software analytics use cases and classified them into four dimensions: quality, people, project management, and knowledge management. We also elicited several indicators to meet the identified use cases and captured key aspects concerning the organization's analytics scenario. Conclusions Although our results are particularly relevant to organizations similar to the one in which we conducted the study, they aim to serve as input for implementing new analytics solutions by practitioners and researchers in general. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Rivero2020,
  type = {Conference Paper},
  title = {Deployment of a Machine Learning System for Predicting Lawsuits against Power Companies: {{Lessons}} Learned from an Agile Testing Experience for Improving Software Quality},
  author = {Rivero, Luis and Diniz, Jo{\~a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton},
  year = {2020},
  series = {{{ACM International Conference Proceeding Series}}},
  doi = {10.1145/3439961.3439991},
  abstract = {The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{rocha2018:dkdonto,
  title = {{{DKDOnto}}: {{An Ontology}} to {{Support Software Development}} with {{Distributed Teams}}},
  shorttitle = {{{DKDOnto}}},
  author = {Rocha, Rodrigo and Ara{\'u}jo, Arthur and Cordeiro, Diogo and Ximenes, Assuero and Teixeira, Jean and Silva, Gabriel and da Silva, Daliton and Espinhara, Diogo and Fernandes, Renan and Ambrosio, Jo{\~a}o and Duarte, Marcos and Azevedo, Ryan},
  year = {2018},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 22nd {{International Conference}}, {{KES-2018}}, {{Belgrade}}, {{Serbia}}},
  volume = {126},
  pages = {373--382},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2018.07.271},
  urldate = {2023-01-15},
  abstract = {The Distributed Software Development has become an option for software companies to expand their perspective and work with dispersed teams, exploiting the advantages brought by this approach. However, this way of developing software enables new challenges to arise, such as the inexistence of a formal, normalized model of a project's data and artifacts accessible to all the individuals involved, which makes it harder for them to communicate, understand each other and what is specified on the project's artifacts. This paper proposes a knowledge base called DKDOnto, a domain-specific ontology for distributed development, aiming to help projects with a common vocabulary, allowing to assist better the distributed software development process.},
  langid = {english},
  keywords = {Distributed Software Development,Ontology,Software Engineering},
  file = {/Users/guru/Zotero/storage/6LMMEICD/Rocha et al_2018_DKDOnto_Procedia Computer Science.pdf;/Users/guru/Zotero/storage/LCXJBIB2/S187705091831247X.html}
}

@article{rodríguez2016:artificial,
  title = {Artificial Intelligence in Service-Oriented Software Design},
  author = {Rodr{\'i}guez, Guillermo and Soria, {\'A}lvaro and Campo, Marcelo},
  year = {2016},
  month = aug,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {53},
  pages = {86--104},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2016.03.009},
  urldate = {2023-01-15},
  abstract = {Service-Oriented Architecture (SOA) has gained considerable popularity for the development of distributed enterprise-wide applications within the software industry. The SOA paradigm promotes the reusability and integrability of software in heterogeneous environments by means of open standards. Most software companies capitalize on SOA by discovering and composing services already accessible over the Internet, whereas other organizations need internal control of applications and develop new services with quality-attribute properties tailored to their particular environment. Therefore, based on architectural and business requirements, developers can elaborate different alternatives within a SOA framework to design their software applications. Each of these alternatives will imply trade-offs among quality attributes, such as performance, dependability and availability, among others. In this context, Artificial Intelligence (AI) can assist developers in dealing with service-oriented design with the positive impact on scalability and management of generic quality attributes. In this paper, we offer a detailed, conceptualized and synthesized analysis of AI research works that have aimed at discovering, composing, or developing services. We also identify open research issues and challenges in the aforementioned research areas. The results of the characterization of 69 contemporary approaches and potential research directions for the areas are also shown. It is concluded that AI has aimed at exploiting the semantic resources and achieving quality-attribute properties so as to produce flexible and adaptive-to-change service discovery, composition, and development.},
  langid = {english},
  keywords = {Artificial Intelligence,Service-oriented design,Web service composition,Web service development,Web services discovery},
  file = {/Users/guru/Zotero/storage/4AVFBKBX/S0952197616300677.html}
}

@book{roger2015software,
  title = {Software Engineering: A Practitioner's Approach},
  author = {Roger, S Pressman and Bruce, R Maxin},
  year = {2015},
  publisher = {{McGraw-Hill Education}}
}

@inproceedings{Roque20212506,
  type = {Conference Paper},
  title = {{{BotCovid}}: {{Development}} and Evaluation of a Chatbot to Combat Misinformation about {{COVID-19}} in Brazil {${_\ast}$}},
  author = {Roque, Geicianfran and Cavalcanti, Andreia and Nascimento, Jose and Souza, Rafael and Queiroz, Sergio},
  year = {2021},
  series = {Conference {{Proceedings}} - {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}}},
  pages = {2506--2511},
  doi = {10.1109/SMC52423.2021.9658693},
  abstract = {The COVID-19 pandemic has caused a global disruption in society and its healthcare systems. Access to information emerges as an essential tool in combating the pandemic. Technological resources play a central role in providing access to information, for better or worse. They allow almost unlimited and instantaneous access to information. However, false or inaccurate information spreads at high speed, which is especially dangerous in the midst of a pandemic. We have developed and evaluated a chatbot based on selected information from reliable sources to answer questions about COVID-19. We assembled an extensive database of 600 questions about COVID-19 to increase the accuracy, and reliability of the chatbot for Brazilian users. We performed an evaluation of BotCovid regarding its functionality, compatibility and reliability with a group of 52 users, the obtained results indicated high satisfaction with the prototype. Twenty users were randomly selected to evaluate the chatbot's usability, which was analyzed by the System Usability Scale, with an average final score of 83.25, indicating excellent usability. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Ros20203841,
  type = {Article},
  title = {Data-Driven Software Design with Constraint Oriented Multi-Variate Bandit Optimization ({{COMBO}})},
  author = {Ros, Rasmus and Hammar, Mikael},
  year = {2020},
  journal = {Empirical Software Engineering},
  volume = {25},
  number = {5},
  pages = {3841--3872},
  doi = {10.1007/s10664-020-09856-1},
  abstract = {Context: Software design in e-commerce can be improved with user data through controlled experiments (i.e. A/B tests) to better meet user needs. Machine learning-based algorithmic optimization techniques extends the approach to large number of variables to personalize software to different user needs. So far the optimization techniques has only been applied to optimize software of low complexity, such as colors and wordings of text. Objective: In this paper, we introduce the COMBO toolkit with capability to model optimization variables and their relationship constraints specified through an embedded domain-specific language. The toolkit generates personalized software configurations for users as they arrive in the system, and the configurations improve over time in in relation to some given metric. COMBO has several implementations of machine learning algorithms and constraint solvers to optimize the model with user data by software developers without deep optimization knowledge. Method: The toolkit was validated in a proof-of-concept by implementing two features that are relevant to Apptus, an e-commerce company that develops algorithms for web shops. The algorithmic performance was evaluated in simulations with realistic historic user data. Results: The validation shows that the toolkit approach can model and improve relatively complex features with many types of variables and constraints, without causing noticeable delays for users. Conclusions: We show that modeling software hierarchies in a formal model facilitates algorithmic optimization of more complex software. In this way, using COMBO, developers can make data-driven and personalized software products. \textcopyright{} 2020, The Author(s).},
  publication_stage = {Final},
  source = {Scopus}
}

@article{ruparelia2010software,
  title = {Software Development Lifecycle Models},
  author = {Ruparelia, Nayan B},
  year = {2010},
  journal = {ACM SIGSOFT Software Engineering Notes},
  volume = {35},
  number = {3},
  pages = {8--13},
  publisher = {{ACM New York, NY, USA}}
}

@inproceedings{ruz2010:componentbased,
  title = {Component-Based Generic Approach for Reconfigurable Management of Component-Based {{SOA}} Applications},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Monitoring}}, {{Adaptation}} and {{Beyond}}},
  author = {Ruz, Cristian and Baude, Fran{\c c}oise and Sauvan, Bastien},
  year = {2010},
  month = dec,
  series = {{{MONA}} '10},
  pages = {25--32},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1929566.1929570},
  urldate = {2023-01-15},
  abstract = {Service Oriented Architecture (SOA) applications can be composed by sets of loosely coupled interacting heterogenous services from different providers. The Service Component Architecture (SCA) specification allows to build hierarchical applications, applying the principles of SOA and Component Based Software Engineering (CBSE). However, concerns like dynamic management, including reconfiguration and distribution handling for composite services are left as platform specific matters. In this context, monitoring and management tasks are not trivial, since compositions and required QoS levels can change depending on the effective location that services and components are deployed onto. Service Level Agreements (SLA) can also evolve during the lifecycle of the deployed application. Several solutions for monitoring and adaptation of QoS-aware service compositions have been proposed so far, but they have rarely been designed in an integrated way and with evolution capabilities in mind. In this work we advocate that a component based approach is an adequate one in order to implement a reconfigurable framework to handle tasks of monitoring and management of hierarchical component-based SOA applications. Our approach allows to address concerns like monitoring, SLA management and adaptation strategies, possibly autonomous ones, as a component-based distributed application. The main advantage is the capability to reconfigure this management architecture at runtime whenever needed, allowing to dynamically adapt it to the possibly evolving non functional requirements of the managed application. The framework is illustrated through a scenario of a composite SOA application that is dynamically augmented with components to tackle non-functional concerns as it is needed. We describe an implementation over an SCA compliant platform that allows distribution and architectural reconfiguration of components.},
  isbn = {978-1-4503-0422-1},
  keywords = {component-based software engineering,management,monitoring,reconfiguration,SLA monitoring},
  file = {/Users/guru/Zotero/storage/RAAG644U/Ruz et al_2010_Component-based generic approach for reconfigurable management of_Proceedings of the 3rd International Workshop on Monitoring, Adaptation and Beyond.pdf}
}

@article{saeik2021:task,
  title = {Task Offloading in {{Edge}} and {{Cloud Computing}}: {{A}} Survey on Mathematical, Artificial Intelligence and Control Theory Solutions},
  shorttitle = {Task Offloading in {{Edge}} and {{Cloud Computing}}},
  author = {Saeik, Firdose and Avgeris, Marios and Spatharakis, Dimitrios and Santi, Nina and Dechouniotis, Dimitrios and Violos, John and Leivadeas, Aris and Athanasopoulos, Nikolaos and Mitton, Nathalie and Papavassiliou, Symeon},
  year = {2021},
  month = aug,
  journal = {Computer Networks},
  volume = {195},
  pages = {108177},
  issn = {1389-1286},
  doi = {10.1016/j.comnet.2021.108177},
  urldate = {2023-01-15},
  abstract = {Next generation communication networks are expected to accommodate a high number of new and resource-voracious applications that can be offered to a large range of end users. Even though end devices are becoming more powerful, the available local resources cannot cope with the requirements of these applications. This has created a new challenge called task offloading, where computation intensive tasks need to be offloaded to more resource powerful remote devices. Naturally, the Cloud Computing is a well-tested infrastructure that can facilitate the task offloading. However, Cloud Computing as a centralized and distant infrastructure creates significant communication delays that cannot satisfy the requirements of the emerging delay-sensitive applications. To this end, the concept of Edge Computing has been proposed, where the Cloud Computing capabilities are repositioned closer to the end devices at the edge of the network. This paper provides a detailed survey of how the Edge and/or Cloud can be combined together to facilitate the task offloading problem. Particular emphasis is given on the mathematical, artificial intelligence and control theory optimization approaches that can be used to satisfy the various objectives, constraints and dynamic conditions of this end-to-end application execution approach. The survey concludes with identifying open challenges and future directions of the problem at hand.},
  langid = {english},
  keywords = {Artificial intelligence,Control theory,Edge Computing,Mathematical optimization,Resource allocation,Task offloading},
  file = {/Users/guru/Zotero/storage/DTDCXKP5/Saeik et al_2021_Task offloading in Edge and Cloud Computing_Computer Networks.pdf;/Users/guru/Zotero/storage/JIMW99VM/S1389128621002322.html}
}

@article{Saha2023,
  type = {Article},
  title = {Warehouse Site Selection for the Automotive Industry Using a Fermatean Fuzzy-Based Decision-Making Approach},
  author = {Saha, Abhijit and Pamucar, Dragan and Gorcun, Omer F. and Raj Mishra, Arunodaya},
  year = {2023},
  journal = {Expert Systems with Applications},
  volume = {211},
  doi = {10.1016/j.eswa.2022.118497},
  abstract = {The automotive industry is one of the most competitive sectors, and it requires a well-structured logistics system to meet the industry' vital requirements such as just-in-time, lean and agile supply chain operations, productivity and sustainability. Well-located and well-designed warehouses can make reaching these aims for the automotive industry possible and more accessible. Hence, determining a location for a warehouse is a highly critical, tactical, and managerial resolution for the automotive industry, as there is a strong correlation between well-located warehouses and the well-structured logistics network in the automotive industry. Although the WSS is a significant decision-making problem, we observed four critical and severe gaps in the existing literature: (1) the authors preferred to apply traditional objective \& subjective frames, and they overlooked existing highly complicated uncertainties. (2) The number of studies focusing on the WSS problem in the automotive industry is surprisingly scarce. (3) It is not sufficiently clear how these factors used in the previous studies were determined, which causes doubts about their reliability. (4) there is no satisfactory evidence of which approaches were used to identify the factors in the previous papers. By considering these gaps, we propose two approaches which can be accepted as a novelty of the paper. First is the extension of the Delphi techniques based on the Fermetean fuzzy sets (FFs) used for identifying the criteria. It also combines the two traditional approaches (i.e., literature review and professionals' evaluations to identify the criteria) with the FF-Delphi technique. The second is the Double Normalized MARCOS approach based on FFs (FF- DN MARCOS) implemented to identify the weights of the criteria and ranking performance of the alternatives. The proposed model was implemented to identify the best warehouse location for the automotive manufacturing company. The results show that the C1 ``energy availability \& cost'' criterion is the most influential criterion and the C5 proximity to port and customs criterion is the second most crucial factor. Then we executed a comprehensive sensitivity analysis, and the results approved the suggested model's validity and robustness despite excessive modifications in the criteria weights. \textcopyright{} 2022 Elsevier Ltd},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{sahibuddin2019:keynote,
  title = {Keynote Speech 1 : {{AI}} for 4ir: {{Sofware}} Engineering Challenges},
  booktitle = {2019 2nd International Conference of Computer and Informatics Engineering ({{IC2IE}})},
  author = {Sahibuddin, Shamsul Bin},
  year = {2019},
  month = sep,
  pages = {1--1},
  doi = {10.1109/IC2IE47452.2019.8940877},
  abstract = {Artificial intelligence (AI) is gaining a strong foothold across numerous applications, lowering the barriers to the use and availability of data. The development of AI cannot be separated from the Industrial Revolution 4.0. AI itself is the senses cognitive and machine learning process in problem- solving. AI systems modeled after the way that human brains operate; taking in information and activating neurons that themselves are not smart, but combine to create probabilistic models that mimic our pattern-recognizing intelligence. Considering this notion of similarity, it subsequently seems abundantly evident that problems facing human-created artificial intelligence will be those that plague human intelligence. The Cyber-Physical System (CPS), together with the Internet of Things, Big Data, Cloud Computing, and Industrial Wireless Networks, are the core technologies related to Industry 4.0. It is crucial to identify the specificities of AI software in software engineering to cope with the Industrial Revolution 4.0. The software used in AI-related with industrial revolution 4.0 needs to follow the rules of software engineering as the AI used is a part of the application used in industrial revolutions 4.0. The challenges in the software related AI can be solved using the PDCA approach, SDLC model, and DevOps practices.},
  keywords = {Artificial intelligence,Brain modeling,Neurons,primary,Probabilistic logic,Software,Software engineering,Wireless networks},
  file = {/Users/guru/Zotero/storage/3YTY2LLF/Sahibuddin_2019_Keynote speech 1 _2019 2nd international conference of computer and informatics engineering (IC2IE).pdf}
}

@article{Saidani2021,
  type = {Article},
  title = {On the Impact of {{Continuous Integration}} on Refactoring Practice: {{An}} Exploratory Study on {{TravisTorrent}}},
  author = {Saidani, Islem and Ouni, Ali and Mkaouer, Mohamed Wiem and Palomba, Fabio},
  year = {2021},
  journal = {Information and Software Technology},
  volume = {138},
  doi = {10.1016/j.infsof.2021.106618},
  abstract = {Context: The ultimate goal of Continuous Integration (CI) is to support developers in integrating changes into production constantly and quickly through automated build process. While CI provides developers with prompt feedback on several quality dimensions after each change, such frequent and quick changes may in turn compromise software quality without Refactoring. Indeed, recent work emphasized the potential of CI in changing the way developers perceive and apply refactoring. However, we still lack empirical evidence to confirm or refute this assumption. Objective: We aim to explore and understand the evolution of refactoring practices, in terms of frequency, size and involved developers, after the switch to CI in order to emphasize the role of this process in changing the way Refactoring is applied. Method: We collect a corpus of 99,545 commits and 89,926 refactoring operations extracted from 39 open-source GitHub projects that adopt Travis CI and analyze the changes using Multiple Regression Analysis (MRA). Results: Our study delivers several important findings. We found that the adoption of CI is associated with a drop in the refactoring size as recommended, while refactoring frequency as well as the number (and its related rate) of developers that perform refactoring are estimated to decrease after the shift to CI, indicating that refactoring is less likely to be applied in CI context. Conclusion: Our study uncovers insights about CI theory and practice and adds evidence to existing knowledge about CI practices related especially to quality assurance. Software developers need more customized refactoring tool support in the context of CI to better maintain and evolve their software systems. \textcopyright{} 2021 Elsevier B.V.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{salahirad2023:mapping,
  title = {Mapping the Structure and Evolution of Software Testing Research over the Past Three Decades},
  author = {Salahirad, Alireza and Gay, Gregory and Mohammadi, Ehsan},
  year = {2023},
  month = jan,
  journal = {Journal of Systems and Software},
  volume = {195},
  pages = {111518},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2022.111518},
  urldate = {2023-01-15},
  abstract = {Background: The field of software testing is growing and rapidly-evolving. Aims: Based on keywords assigned to publications, we seek to identify predominant research topics and understand how they are connected and have evolved. Methods: We apply co-word analysis to map the topology of testing research as a network where author-assigned keywords are connected by edges indicating co-occurrence in publications. Keywords are clustered based on edge density and frequency of connection. We examine the most popular keywords, summarize clusters into high-level research topics examine how topics connect, and examine how the field is changing. Results: Testing research can be divided into 16 high-level topics and 18 subtopics. Creation guidance, automated test generation, evolution and maintenance, and test oracles have particularly strong connections to other topics, highlighting their multidisciplinary nature. Emerging keywords relate to web and mobile apps, machine learning, energy consumption, automated program repair and test generation, while emerging connections have formed between web apps, test oracles, and machine learning with many topics. Random and requirements-based testing show potential decline. Conclusions: Our observations, advice, and map data offer a deeper understanding of the field and inspiration regarding challenges and connections to explore. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board.},
  langid = {english},
  keywords = {Bibliometrics,Co-word analysis,Software testing},
  file = {/Users/guru/Zotero/storage/QAPNFW4Q/Salahirad et al_2023_Mapping the structure and evolution of software testing research over the past_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/7AMZR5C8/S0164121222001947.html}
}

@article{saleme2020:mulsemedia,
  title = {Mulsemedia {{DIY}}: {{A Survey}} of {{Devices}} and a {{Tutorial}} for {{Building Your Own Mulsemedia Environment}}},
  shorttitle = {Mulsemedia {{DIY}}},
  author = {Saleme, Est{\^e}v{\~a}o B. and Covaci, Alexandra and Mesfin, Gebremariam and Santos, Celso A. S. and Ghinea, Gheorghita},
  year = {2020},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {3},
  pages = {1--29},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3319853},
  urldate = {2023-02-28},
  abstract = {Multisensory experiences have been increasingly applied in Human-Computer Interaction (HCI). In recent years, it is commonplace to notice the development of haptic, olfactory, and even gustatory displays to create more immersive experiences. Companies are proposing new additions to the multisensory world and are unveiling new products that promise to offer amazing experiences exploiting mulsemedia\textemdash multiple sensorial media\textemdash where users can perceive odors, tastes, and the sensation of wind blowing against their face. Whilst researchers, practitioners and users alike are faced with a wide range of such new devices, relatively little work has been undertaken to summarize efforts and initiatives in this area. The current article addresses this shortcoming in two ways: first, by presenting a survey of devices targeting senses beyond that of sight and hearing and, second, by describing an approach to guide newcomers and experienced practitioners alike to build their own mulsemedia environment, both in a desktop setting and in an immersive 360\textdegree{} environment.},
  langid = {english},
  file = {/Users/guru/Zotero/storage/JQ36MYKW/Saleme et al. - 2020 - Mulsemedia DIY A Survey of Devices and a Tutorial.pdf}
}

@inproceedings{Salman2015666,
  type = {Conference Paper},
  title = {Are Students Representatives of Professionals in Software Engineering Experiments?},
  author = {Salman, Iflaah and Misirli, Ayse Tosun and Juristo, Natalia},
  year = {2015},
  series = {Proceedings - {{International Conference}} on {{Software Engineering}}},
  volume = {1},
  pages = {666--676},
  doi = {10.1109/ICSE.2015.82},
  abstract = {Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments. \textcopyright{} 2015 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{saltz2019ski,
  title = {{{SKI}}: An {{Agile}} Framework for Data Science},
  booktitle = {2019 {{IEEE}} International Conference on Big Data (Big Data)},
  author = {Saltz, Jeffrey and Suthrland, Alex},
  year = {2019},
  pages = {3468--3476},
  organization = {{IEEE}}
}

@article{Saltz2022,
  type = {Article},
  title = {Current Approaches for Executing Big Data Science Projects\textemdash a Systematic Literature Review},
  author = {Saltz, Jeffrey S. and Krasteva, Iva},
  year = {2022},
  journal = {PeerJ Computer Science},
  volume = {8},
  doi = {10.7717/PEERJ-CS.862},
  abstract = {There is an increasing number of big data science projects aiming to create value for organizations by improving decision making, streamlining costs or enhancing business processes. However, many of these projects fail to deliver the expected value. It has been observed that a key reason many data science projects don't succeed is not technical in nature, but rather, the process aspect of the project. The lack of established and mature methodologies for executing data science projects has been frequently noted as a reason for these project failures. To help move the field forward, this study presents a systematic review of research focused on the adoption of big data science process frameworks. The goal of the review was to identify (1) the key themes, with respect to current research on how teams execute data science projects, (2) the most common approaches regarding how data science projects are organized, managed and coordinated, (3) the activities involved in a data science projects life cycle, and (4) the implications for future research in this field. In short, the review identified 68 primary studies thematically classified in six categories. Two of the themes (workflow and agility) accounted for approximately 80\% of the identified studies. The findings regarding workflow approaches consist mainly of adaptations to CRISP-DM (vs entirely new proposed methodologies). With respect to agile approaches, most of the studies only explored the conceptual benefits of using an agile approach in a data science project (vs actually evaluating an agile framework being used in a data science context). Hence, one finding from this research is that future research should explore how to best achieve the theorized benefits of agility. Another finding is the need to explore how to efficiently combine workflow and agile frameworks within a data science context to achieve a more comprehensive approach for project execution \textcopyright{} 2022 Saltz and Krasteva},
  publication_stage = {Final},
  source = {Scopus}
}

@article{samek2021explaining,
  title = {Explaining Deep Neural Networks and beyond: {{A}} Review of Methods and Applications},
  author = {Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J and M{\"u}ller, Klaus-Robert},
  year = {2021},
  journal = {Proceedings of the IEEE},
  volume = {109},
  number = {3},
  pages = {247--278},
  publisher = {{IEEE}}
}

@article{sandeepa2022:survey,
  title = {A Survey on Privacy for {{B5G}}/{{6G}}: {{New}} Privacy Challenges, and Research Directions},
  shorttitle = {A Survey on Privacy for {{B5G}}/{{6G}}},
  author = {Sandeepa, Chamara and Siniarski, Bartlomiej and Kourtellis, Nicolas and Wang, Shen and Liyanage, Madhusanka},
  year = {2022},
  month = nov,
  journal = {Journal of Industrial Information Integration},
  volume = {30},
  pages = {100405},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2022.100405},
  urldate = {2023-01-15},
  abstract = {Massive developments in mobile wireless telecommunication networks have been made during the last few decades. At present, mobile users are getting familiar with the latest 5G networks, and the discussion for the next generation of Beyond 5G (B5G)/6G networks has already been initiated. It is expected that B5G/6G will push the existing network capabilities to the next level, with higher speeds, enhanced reliability and seamless connectivity. To make these expectations a reality, research is progressing on new technologies, architectures, and intelligence-based decision-making processes related to B5G/6G. Privacy considerations are a crucial aspect that requires further attention in such developments, as billions of people and devices will be transmitting data through the upcoming network. However, the main recognition remains biased towards the network security. A discussion focused on privacy of B5G/6G is lacking at the moment. To address the gap, this paper provides a comprehensive survey on privacy-related aspects of B5G/6G networks. First, it discusses a taxonomy of different privacy perspectives. Based on the taxonomy, the paper then conceptualizes a set of challenges that appear as barriers to reach privacy preservation. Next, this work provides a set of solutions applicable to the proposed architecture of B5G/6G networks to mitigate the challenges. It also provides an overview of standardization initiatives for privacy preservation. Finally, the paper concludes with a roadmap of future directions, which will be an arena for new research towards privacy-enhanced B5G/6G networks. This work provides a basis for privacy aspects that will significantly impact peoples' daily lives when using these future networks.},
  langid = {english},
  keywords = {6G,Artificial intelligence,Beyond 5G,Explainable AI,Machine learning,Privacy issues,Privacy solutions,Survey},
  file = {/Users/guru/Zotero/storage/5DUI2YP9/S2452414X22000723.html}
}

@article{Santhanam2022,
  type = {Article},
  title = {Bots in Software Engineering: A Systematic Mapping Study},
  author = {Santhanam, Sivasurya and Hecking, Tobias and Schreiber, Andreas and Wagner, Stefan},
  year = {2022},
  journal = {PeerJ Computer Science},
  volume = {8},
  doi = {10.7717/peerj-cs.866},
  abstract = {Bots have emerged from research prototypes to deployable systems due to the recent developments in machine learning, natural language processing and understanding techniques. In software engineering, bots range from simple automated scripts to decision-making autonomous systems. The spectrum of applications of bots in software engineering is so wide and diverse, that a comprehensive overview and categorization of such bots is needed. Existing works considered selective bots to be analyzed and failed to provide the overall picture. Hence it is significant to categorize bots in software engineering through analyzing why, what and how the bots are applied in software engineering. We approach the problem with a systematic mapping study based on the research articles published in this topic. This study focuses on classification of bots used in software engineering, the various dimensions of the characteristics, the more frequently researched area, potential research spaces to be explored and the perception of bots in the developer community. This study aims to provide an introduction and a broad overview of bots used in software engineering. Discussions of the feedback and results from several studies provide interesting insights and prospective future directions. \textcopyright{} Copyright 2022 Santhanam et al.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Santos2022174,
  type = {Conference Paper},
  title = {Hits and {{Misses}}: {{Newcomers}}' Ability to Identify {{Skills}} Needed for {{OSS}} Tasks},
  author = {Santos, Italo and Wiese, Igor and Steinmacher, Igor and Sarma, Anita and Gerosa, Marco A.},
  year = {2022},
  series = {Proceedings - 2022 {{IEEE International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}}, {{SANER}} 2022},
  pages = {174--183},
  doi = {10.1109/SANER53432.2022.00032},
  abstract = {Participation in Open Source Software (OSS) projects offers real software development experience for students and other newcomers seeking to develop their skills. However, onboarding to an OSS project brings various challenges, including finding a suitable task among various open issues. Selecting an appropriate starter task requires newcomers to identify the skills needed to solve a project issue and avoiding tasks too far from their skill set. However, little is known about how effective newcomers are in identifying the skills needed to resolve an issue. We asked 154 undergrad students to evaluate issues from OSS projects and infer the skills needed to contribute. Students reported a total of 94 skills, which we classified into 10 categories. We compared the students' answers to those collected from 6 professional developers. In general, students misidentified and missed several skills (f-measure=0.37). Students had results closer to professional developers for skills related to database, operating infrastructure, programming concepts, and programming language, and they had worse results in identifying skills related to debugging and program comprehension. Our results can help educators who seek to use OSS as part of their courses and OSS communities that want to label newcomer-friendly issues to facilitate onboarding of new contributors. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{saravanan2020comparative,
  title = {Comparative Analysis of Software Life Cycle Models},
  booktitle = {2020 2nd International Conference on Advances in Computing, Communication Control and Networking ({{ICACCCN}})},
  author = {Saravanan, T and Jha, Sumit and Sabharwal, Gautam and Narayan, Shubham},
  year = {2020},
  pages = {906--909},
  organization = {{IEEE}}
}

@article{sarker2021machine,
  title = {Machine Learning: {{Algorithms}}, Real-World Applications and Research Directions},
  author = {Sarker, Iqbal H},
  year = {2021},
  journal = {SN Computer Science},
  volume = {2},
  number = {3},
  pages = {1--21},
  publisher = {{Springer}}
}

@article{Sarro2017898,
  type = {Article},
  title = {Adaptive Multi-Objective Evolutionary Algorithms for Overtime Planning in Software Projects},
  author = {Sarro, Federica and Ferrucci, Filomena and Harman, Mark and Manna, Alessandra and Ren, Jian},
  year = {2017},
  journal = {IEEE Transactions on Software Engineering},
  volume = {43},
  number = {10},
  pages = {898--917},
  doi = {10.1109/TSE.2017.2650914},
  abstract = {Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size). \textcopyright{} 2017 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{saura2022:assessing,
  title = {Assessing Behavioral Data Science Privacy Issues in Government Artificial Intelligence Deployment},
  author = {Saura, Jose Ramon and {Ribeiro-Soriano}, Domingo and {Palacios-Marqu{\'e}s}, Daniel},
  year = {2022},
  month = oct,
  journal = {Government Information Quarterly},
  volume = {39},
  number = {4},
  pages = {101679},
  issn = {0740-624X},
  doi = {10.1016/j.giq.2022.101679},
  urldate = {2023-01-15},
  abstract = {In today's global culture where the Internet has established itself as the main tool for communication and commerce, the capability to massively analyze and predict citizens' behavior has become a priority for governments in terms of collective intelligence and security. At the same time, in the context of novel possibilities that artificial intelligence (AI) brings to governments in terms of understanding and developing collective behavior analysis, important concerns related to citizens' privacy have emerged. In order to identify the main uses that governments make of AI and to define citizens' concerns about their privacy, in the present study, we undertook a systematic review of the literature, conducted in-depth interviews, and applied data-mining techniques. Based on our results, we classified and discussed the risks to citizens' privacy according to the types of AI strategies used by governments that may affect collective behavior and cause massive behavior modification. Our results revealed 11 uses of AI strategies used by the government to improve their interaction with citizens, organizations in cities, services provided by public institutions or the economy, among other areas. In relation to citizens' privacy when AI is used by governments, we identified 8 topics related to human behavior predictions, intelligence decision making, decision automation, digital surveillance, data privacy law and regulation, and the risk of behavior modification. The paper concludes with a discussion of the development of regulations focused on the ethical design of citizen data collection, where implications for governments are presented aimed at regulating security, ethics, and data privacy. Additionally, we propose a research agenda composed by 16 research questions to be investigated in further research.},
  langid = {english},
  keywords = {Artificial intelligence,Behavioral data sciences,Collective behavior analysis,Governments,Privacy,Surveillance capitalism}
}

@article{schreckenberg2021:developing,
  title = {Developing a Maturity-Based Workflow for the Implementation of {{ML-applications}} Using the Example of a Demand Forecast},
  author = {Schreckenberg, Felix and Moroff, Nikolas Ulrich},
  year = {2021},
  month = jan,
  journal = {Procedia Manufacturing},
  series = {10th {{CIRP Sponsored Conference}} on {{Digital Enterprise Technologies}} ({{DET}} 2020) \textendash{} {{Digital Technologies}} as {{Enablers}} of {{Industrial Competitiveness}} and {{Sustainability}}},
  volume = {54},
  pages = {31--38},
  issn = {2351-9789},
  doi = {10.1016/j.promfg.2021.07.006},
  urldate = {2023-01-15},
  abstract = {The aim of the article is to present a guideline that has been developed in the form of a workflow to identify the capability of an organisation to implement machine learning (ML) applications on the one hand and, on the other hand, to describe a maturity-dependent procedure for the development of an ML application based on this knowledge. With the help of the guideline, application-specific requirements can be identified based on the phases of the development process of an ML application adapted to the corporate environment. The article begins with the motivation for using machine learning methods and presents the challenges in implementing these methods. Based on a literature review, a maturity-based approach is designed and the developed and adapted development phases from the literature are described in a more detailed way. The individual characteristics of certain phases are specified based on the maturity level. As well, the weighting of certain maturity dimensions of the respective phase is highlighted. The article ends with an outlook on the further development of the created guideline.},
  langid = {english},
  keywords = {artificial intelligenz,challenges AI,maturity-based workflow},
  file = {/Users/guru/Zotero/storage/ZK3FZ9QV/Schreckenberg_Moroff_2021_Developing a maturity-based workflow for the implementation of ML-applications_Procedia Manufacturing.pdf;/Users/guru/Zotero/storage/HMVP2WM4/S2351978921001396.html}
}

@article{schuh2020:using,
  title = {Using {{AI}} to {{Facilitate Technology Management}} \textendash{} {{Designing}} an {{Automated Technology Radar}}},
  author = {Schuh, G{\"u}nther and Hicking, Jan and Stroh, Max-Ferdinand and Benning, Justus},
  year = {2020},
  month = jan,
  journal = {Procedia CIRP},
  series = {53rd {{CIRP Conference}} on {{Manufacturing Systems}} 2020},
  volume = {93},
  pages = {419--424},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2020.04.089},
  urldate = {2023-01-15},
  abstract = {The number of available technologies is constantly rising. Be it additive manufacturing, artificial intelligence (AI) or distributed ledger technologies. The choice of the right technologies may decide the fate of a company. Due to the overwhelming amount of information sources, regular technology market research becomes increasingly challenging, especially for SMEs. In order to assist the technology management process, the authors will introduce the architecture of an automated, AI-based technology radar. The architecture will automatically collect data from relevant sources, assess the relevance of the respective technology (i.e. their maturity level) and then visualize it on the radar map.},
  langid = {english},
  keywords = {artificial intelligence,natural language processing,technology management,technology radar,web crawling},
  file = {/Users/guru/Zotero/storage/P7V547DL/Schuh et al_2020_Using AI to Facilitate Technology Management – Designing an Automated_Procedia CIRP.pdf;/Users/guru/Zotero/storage/QUETURMW/S221282712030723X.html}
}

@inproceedings{Schuh2020419,
  type = {Conference Paper},
  title = {Using {{AI}} to Facilitate Technology Management - {{Designing}} an Automated Technology Radar},
  author = {Schuh, G{\"u}nther and Hicking, Jan and Stroh, Max-Ferdinand and Benning, Justus},
  year = {2020},
  series = {Procedia {{CIRP}}},
  volume = {93},
  pages = {419--424},
  doi = {10.1016/j.procir.2020.04.089},
  abstract = {The number of available technologies is constantly rising. Be it additive manufacturing, artificial intelligence (AI) or distributed ledger technologies. The choice of the right technologies may decide the fate of a company. Due to the overwhelming amount of information sources, regular technology market research becomes increasingly challenging, especially for SMEs. In order to assist the technology management process, the authors will introduce the architecture of an automated, AI-based technology radar. The architecture will automatically collect data from relevant sources, assess the relevance of the respective technology (i.e. their maturity level) and then visualize it on the radar map. \textcopyright{} 2020 The Authors.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{serban2020:adoption,
  title = {Adoption and {{Effects}} of {{Software Engineering Best Practices}} in {{Machine Learning}}},
  booktitle = {Proceedings of the 14th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Serban, Alex and {van der Blom}, Koen and Hoos, Holger and Visser, Joost},
  year = {2020},
  month = oct,
  series = {{{ESEM}} '20},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3382494.3410681},
  urldate = {2023-01-15},
  abstract = {Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.},
  isbn = {978-1-4503-7580-1},
  keywords = {best practices,machine learning engineering,survey}
}

@article{serrano2022:verification,
  title = {Verification and {{Validation}} for Data Marketplaces via a Blockchain and Smart Contracts},
  author = {Serrano, Will},
  year = {2022},
  month = dec,
  journal = {Blockchain: Research and Applications},
  volume = {3},
  number = {4},
  pages = {100100},
  issn = {2096-7209},
  doi = {10.1016/j.bcra.2022.100100},
  urldate = {2023-01-15},
  abstract = {Actual challenges with data in physical infrastructure include: 1) the adversity of its velocity based on access and retrieval, thus integration; 2) its value as its intrinsic quality; 3) its extensive volume with a limited variety in terms of systems; and finally, 4) its veracity, as data can be modified to obtain an economical advantage. Physical infrastructure design based on Agile project management and minimum viable products provides benefits against the traditional waterfall method. Agile supports an early return on investment that promotes circular reinvesting while making the product more adaptable to variable social-economical environments. However, Agile also presents inherent issues due to its iterative approach. Furthermore, project information requires an efficient record of the aims, requirements, and governance not only for the investors, owners, or users but also to keep evidence in future health \& safety and other statutory compliance. In order to address these issues, this article presents a Validation and Verification (V\&V) model for data marketplaces with a hierarchical process; each data V\&V stage provides a layer of data abstraction, value-added services, and authenticity based on Artificial Intelligence (AI). In addition, this proposed solution applies Distributed Ledger Technology (DLT) for a decentralised approach where each user keeps and maintains the data within a ledger. The presented model is validated in real data marketplace applications: 1) live data for the Newcastle Urban Observatory Smart City Project, where data are collected from sensors embedded within the smart city via APIs; 2) static data for University College London (UCL)\textemdash Real Estate\textemdash PEARL Project, where different project users and stakeholders introduce data into a Project Information Model (PIM).},
  langid = {english},
  keywords = {Artificial intelligence,Blockchain,Data marketplace,Distributed ledger technology,Project information model,Real estate,Smart buildings,Smart cities,Smart contracts},
  file = {/Users/guru/Zotero/storage/G6NTTQV6/Serrano_2022_Verification and Validation for data marketplaces via a blockchain and smart_Blockchain Research and Applications.pdf;/Users/guru/Zotero/storage/MSU86AW7/S2096720922000410.html}
}

@article{shameem2023:genetic,
  title = {Genetic Algorithm Based Probabilistic Model for Agile Project Success in Global Software Development},
  author = {Shameem, Mohammad and Nadeem, Mohammad and Zamani, Abu Taha},
  year = {2023},
  month = jan,
  journal = {Applied Soft Computing},
  pages = {109998},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2023.109998},
  urldate = {2023-01-15},
  abstract = {Software development organizations use agile practices in the global software development (GSD) environment to leverage these approaches in terms of low development cost, continuous project delivery, and high-quality product. Despite the benefits, using the agile process in GSD organizations is not a straightforward task because the underlying projects inhibit additional risk factors that could lead agile projects to failure. GSD organizations are continuously improving their process management activities to improve the rate of agile project success. Project success could be improved if the project manager or organization management has preliminary information about the derailment of the project features. This work aims to investigate the most influential features of agile projects in determining the project outcomes and develop a cost-effective, effort-based prediction model to improve the probability of successful completion in the globally distributed environment. To do so, a nature-inspired optimization algorithm i.e., genetic algorithm (GA), has been employed to achieve these goals. In the proposed model, GA considers the probability of success with respect to cost to determine probable project outcomes. An efficacy measure is formulated as a fitness function in GA which maximizes the success of agile project outcomes relative to cost. The optimization model has been tested with two different prediction models i.e., Naive Bayes classifier (NBC) and logistic regression (LR). We performed the experiment on data gathered through the survey administered from the globally distributed agile projects. The results demonstrate that prediction models calculate the efficacy for best solutions as 0.531 and 0.5850 for NBC and LR, respectively. Moreover, the ranking of each project feature based on their relative cost identified using NBC and LR have more similarities. The t test results are significant, i.e., t = 6.068, p = 0.001 {$<$} 0.005, which indicates that no significant differences have been observed between the ranking assigned by two different methods ( NBC and LR). The results reveal that the developed prediction model based on identified eight agile project features that the GSD organization management and agile team need to focus more on to facilitate cost-effective successful implementation of agile projects.},
  langid = {english},
  keywords = {Agile development,Genetic algorithm,GSD,Prediction,Success probability},
  file = {/Users/guru/Zotero/storage/YH5LN8UB/S1568494623000169.html}
}

@article{Shastri2021,
  type = {Article},
  title = {The Role of the Project Manager in Agile Software Development Projects},
  author = {Shastri, Yogeshwar and Hoda, Rashina and Amor, Robert},
  year = {2021},
  journal = {Journal of Systems and Software},
  volume = {173},
  doi = {10.1016/j.jss.2020.110871},
  abstract = {Agile teams are not meant to have project managers. Instead, agile methods such as Scrum and XP define roles such as product owner, scrum master, and coach. Studies have uncovered the existence of the project manager in agile projects, pointing to disconnect between theory and practice. To address this gap, a Grounded Theory study with a mixed methods approach was conducted using multiple sources of data including over 45 h of interviews with 39 software practitioners and quantitative data from 57 questionnaire respondents. We present and describe the project manager's role in agile projects in terms of (a) everyday activities: facilitating, mentoring, negotiating, coordinating, and protecting, performed by the project manager using; (b) three management approaches: hard, moderate, and soft; (c) four traditional project management activities continued to be performed by them, including: tracking project progress, reporting on project status, budgeting and forecasting, and managing personnel; and (d) the influence of the presence of the project manager on the frequency with which agile activities are carried out by the teams. Our study highlights the continued presence of the role of the project manager in agile software projects as a part of the transition from traditional to agile ways of working. \textcopyright{} 2020 Elsevier Inc.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Shehab20201129,
  type = {Article},
  title = {({{AIAM2019}}) Artificial Intelligence in Software Engineering and Inverse: {{Review}}},
  author = {Shehab, Mohammad and Abualigah, Laith and Jarrah, Muath Ibrahim and Alomari, Osama Ahmad and Daoud, Mohammad Sh.},
  year = {2020},
  journal = {International Journal of Computer Integrated Manufacturing},
  volume = {33},
  number = {10-11},
  pages = {1129--1144},
  doi = {10.1080/0951192X.2020.1780320},
  abstract = {Artificial Intelligence (AI) and Software Engineering are considered as significant fields to solve various problems. However, there are weaknesses in certain problem-solving in each field. Thus, this paper is a broad-based review of using artificial intelligence (AI) to improve software engineering (SE), and vice versa. As well as it intends to review the techniques developed in artificial intelligence from the standpoint of their application in software engineering. The aim of this review is highlighted in how the previous study benefited from incorporating the advantages of both fields. The researchers and practitioners on AI and SE belong to a wide range of audiences from the domains of optimization, engineering, data mining, clustering, etc., who will benefit from this study and areas for potential future research. \textcopyright{} 2020 Informa UK Limited, trading as Taylor \& Francis Group.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{shen2014:aligning,
  title = {Aligning Ontology-Based Development with Service Oriented Systems},
  author = {Shen, Jun and Beydoun, Ghassan and Low, Graham and Wang, Lijuan},
  year = {2014},
  month = mar,
  journal = {Future Generation Computer Systems},
  series = {Special {{Section}}: {{The Management}} of {{Cloud Systems}}, {{Special Section}}: {{Cyber-Physical Society}} and {{Special Section}}: {{Special Issue}} on {{Exploiting Semantic Technologies}} with {{Particularization}} on {{Linked Data}} over {{Grid}} and {{Cloud Architectures}}},
  volume = {32},
  pages = {263--273},
  issn = {0167-739X},
  doi = {10.1016/j.future.2013.08.005},
  urldate = {2023-01-15},
  abstract = {This paper argues for placing ontologies at the centre of the software development life cycle for distributed component-based systems and, in particular, for service-oriented systems. It presents an ontology-based development process which relies on three levels of abstraction using ontologies: architecture layer, application layer and domain layer. The paper discusses the key roles of ontologies with respect to the various abstraction layers and their corresponding impact on the concomitant workproducts. In addition, a peer-to-peer-based service selecting and composing tool is suggested as a way of supporting the process. The paper presents the architecture of the proposed tool and illustrates the whole process in the development of a mobile banking application based on dynamic Web services.},
  langid = {english},
  keywords = {Agents,Multi-agent systems,Ontologies,Peer-to-peer systems,Service oriented systems,Software development life cycle},
  file = {/Users/guru/Zotero/storage/UHP5RGR9/S0167739X13001702.html}
}

@article{sikder2018survey,
  title = {A Survey on Sensor-Based Threats to Internet-of-Things (Iot) Devices and Applications},
  author = {Sikder, Amit Kumar and Petracca, Giuseppe and Aksu, Hidayet and Jaeger, Trent and Uluagac, A Selcuk},
  year = {2018},
  journal = {arXiv preprint arXiv:1802.02041},
  eprint = {1802.02041},
  archiveprefix = {arxiv}
}

@inproceedings{Sildatke2020603,
  type = {Conference Paper},
  title = {Automated Software Quality Monitoring in Research Collaboration Projects},
  author = {Sildatke, Michael and Karwanni, Hendrik and Kraft, Bodo and Schmidts, Oliver and Z{\"u}ndorf, Albert},
  year = {2020},
  series = {Proceedings - 2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering Workshops}}, {{ICSEW}} 2020},
  pages = {603--610},
  doi = {10.1145/3387940.3391478},
  abstract = {In collaborative research projects, both researchers and practitioners work together solving business-critical challenges. These projects often deal with ETL processes, in which humans extract information from non-machine-readable documents by hand. AI-based machine learning models can help to solve this problem. Since machine learning approaches are not deterministic, their quality of output may decrease over time. This fact leads to an overall quality loss of the application which embeds machine learning models. Hence, the software qualities in development and production may differ. Machine learning models are black boxes. That makes practitioners skeptical and increases the inhibition threshold for early productive use of research prototypes. Continuous monitoring of software quality in production offers an early response capability on quality loss and encourages the use of machine learning approaches. Furthermore, experts have to ensure that they integrate possible new inputs into the model training as quickly as possible. In this paper, we introduce an architecture pattern with a reference implementation that extends the concept of Metrics Driven Research Collaboration with an automated software quality monitoring in productive use and a possibility to auto-generate new test data coming from processed documents in production. Through automated monitoring of the software quality and auto-generated test data, this approach ensures that the software quality meets and keeps requested thresholds in productive use, even during further continuous deployment and changing input data. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Silva2021,
  type = {Conference Paper},
  title = {A Comparative Analysis of Agile Teamwork Quality Models},
  author = {Silva, Manuel and Freire, Arthur and Perkusich, Mirko and Albuquerque, Danyllo and Gorg{\^o}nio, Kyller Costa and Almeida, Hyggo and Perkusich, Angelo and Guimaraes, Everton},
  year = {2021},
  series = {2021 29th {{International Conference}} on {{Software}}, {{Telecommunications}} and {{Computer Networks}}, {{SoftCOM}} 2021},
  abstract = {[Background] The literature reports multiple models (or instruments) for measuring Teamwork Quality (TWQ) for Agile Software Development However, these models have different constructs and measures, and there is a lack of empirical evidence comparing them. [Goal] This study fills this gap by analyzing if two agile TWQ models yields equivalent results. [Method] First, we mapped the models' variables given then-definitions. Then, we collected data using both a BN model (Baysean Network model), TWQ-BN, and Structural Equation Modeling, TWQ-SEM, by interviewing 162 team members from two software development companies. We analyzed the collected data by applying the Bland-Altaian method. [Results] We obtained enough evidence to conclude that the results for Communication, Coordination, Cohesion and Mutual Support are not equivalent Conversely, we did not have enough evidence to claim that the models do not agree for measuring Effort and Balance of member contribution. [Conclusions] The results of this study detail how two state-of-the-art agile TWQ compare in terms of their measures as well as potential research areas for further investigation. \textcopyright{} SoftCOM 2021. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{silva2021:comparative,
  title = {A Comparative Analysis of Agile Teamwork Quality Models},
  booktitle = {2021 International Conference on Software, Telecommunications and Computer Networks ({{SoftCOM}})},
  author = {Silva, Manuel and Freire, Arthur and Perkusich, Mirko and Albuquerque, Danyllo and Gorg{\^o}nio, Kyller Costa and Almeida, Hyggo and Perkusich, Angelo and Guimar{\~a}es, Everton},
  year = {2021},
  month = sep,
  pages = {1--6},
  issn = {1847-358X},
  doi = {10.23919/SoftCOM52868.2021.9559062},
  abstract = {[Background] The literature reports multiple models (or instruments) for measuring Teamwork Quality (TWQ) for Agile Software Development However, these models have different constructs and measures, and there is a lack of empirical evidence comparing them. [Goal] This study fills this gap by analyzing if two agile TWQ models yields equivalent results. [Method] First, we mapped the models' variables given their definitions. Then, we collected data using both a BN model (Baysean Network model), TWQ-BN, and Structural Equation Modeling, TWQ-SEM, by interviewing 162 team members from two software development companies. We analyzed the collected data by applying the Bland-Altman method. [Results] We obtained enough evidence to conclude that the results for Communication, Coordination, Cohesion and Mutual Support are not equivalent. Conversely, we did not have enough evidence to claim that the models do not agree for measuring Effort and Balance of member contribution. [Conclusions] The results of this study detail how two state-of-the-art agile TWQ compare in terms of their measures as well as potential research areas for further investigation.},
  keywords = {Agile Software Development,Analytical models,Area measurement,Baysean Networks,Computational modeling,Data models,Instruments,Mathematical models,Software,Structural Equation Modeling,Teamwork}
}

@article{Silva2022153,
  type = {Article},
  title = {A Comparative Analysis of Agile Teamwork Quality Measurement Models},
  author = {Silva, Manuel and Perkusich, Mirko and Freire, Arthur and Albuquerque, Danyllo and Gorg{\^o}nio, Kyller Costa and Almeida, Hyggo and Perkusich, Angelo and Guimar{\~a}es, Everton},
  year = {2022},
  journal = {Journal of Communications Software and Systems},
  volume = {18},
  number = {2},
  pages = {153--164},
  doi = {10.24138/JCOMSS-2021-0177},
  abstract = {Multiple models (or instruments) for measuring Teamwork Quality (TWQ) for Agile Software Development can be found in the literature. Regardless, such models have different constructs and measures, with no empirical evidence for comparing them. This study analyzed two agile TWQ models, resulting in equivalent results. We mapped the models' variables given their definitions. We then collected data using both a Bayesian Network model, namely the TWQ-BN model, and Structural Equation Modeling, namely the TWQ-SEM model. We interviewed 162 team members from two software development companies. We analyzed the data using the Bland-Altman method. We obtained enough evidence to conclude that the results for Communication, Coordination, Cohesion and Mutual Support variables are not equivalent. On the other hand, we did not have enough evidence to claim that the models do not agree for measuring Effort and Balance of member contribution variables. The results of this study detail how two state-of-the-art agile TWQs compare in terms of their measures as well as potential research areas for further investigation. \textcopyright{} 2022 University of Split. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary}
}

@inproceedings{singh2013review,
  title = {A Review on Software Quality Models},
  booktitle = {2013 International Conference on Communication Systems and Network Technologies},
  author = {Singh, Brijendra and Kannojia, Suresh Prasad},
  year = {2013},
  pages = {801--806},
  organization = {{IEEE}}
}

@article{Singh2014297,
  type = {Article},
  title = {Agile Knowledge Management: A Survey of {{Indian}} Perceptions},
  author = {Singh, Amitoj and Singh, Kawaljeet and Sharma, Neeraj},
  year = {2014},
  journal = {Innovations in Systems and Software Engineering},
  volume = {10},
  number = {4},
  pages = {297--315},
  doi = {10.1007/s11334-014-0237-z},
  abstract = {This paper is a pivot step to find out the level of KM adoption of Indian software organizations which are using agile practices for software development. After reviewing the literature it was found although many practitioners believe that KM practices are embodied in agile methodologies but it is not documented efficiently. Survey was used to take perception of agile organizations. KM practices are grouped into seven factors and respondents are taken from Indian software industry. Organizations are divided into different demographics: size, core area, type. Different statistical tests are used to conclude outputs of the questionnaire. It is found that Indian software industry working with agile practices lacks in providing any formal head for KM positioning. Learning and sharing through discussion forms is the most used practices among all respondents of organizations, whereas least accepted practice is dependent upon documents for transfer of knowledge. Indian organizations also lack in appointing a formal head who can provide guidelines for agile practice. \textcopyright{} 2014, Springer-Verlag London.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Singh2022,
  type = {Article},
  title = {Task Allocation in Distributed Agile Software Development Environment Using Unsupervised Learning},
  author = {Singh, Madan and Chauhan, Naresh and Popli, Rashmi},
  year = {2022},
  journal = {Journal of Engineering Research (Kuwait)},
  volume = {10},
  doi = {10.36909/jer.ICMET.17167},
  abstract = {In the paper, a novel approach for task allocation in DASD environment has been proposed. In the approach, new tasks (in the form of user - stories), are allocated to an employee, who is found to be 'best', on the basis of classification and rank ordering. For applying classification and rank ordering on data set of employees, Meta - Classifier Based Prediction Model (MCBPM) has been used that applied unsupervised learning. Results show that MCBPM - based task allocations provides accurate suggestions for the activity. \textcopyright{} 2022 University of Kuwait. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{singh2022:extensive,
  title = {Extensive Performance Analysis of {{OpenDayLight}} ({{ODL}}) and {{Open Network Operating System}} ({{ONOS}}) {{SDN}} Controllers},
  author = {Singh, Avtar and Kaur, Navjot and Kaur, Harpreet},
  year = {2022},
  month = nov,
  journal = {Microprocessors and Microsystems},
  volume = {95},
  pages = {104715},
  issn = {0141-9331},
  doi = {10.1016/j.micpro.2022.104715},
  urldate = {2023-01-15},
  abstract = {The term Software Defined Network or SDN is one of the many emerging technologies that has been revolutionizing the concept of networking. By decoupling the Data and Control plane, it has lot to offer in the field of information technology. The Control Plane is regarded as the brain of the network, which is a centralized unit of SDN network along with the controller. Selection of SDN controller is one of the most important task in designing of SDN based Network. Through this paper, we intend to compare two eminent controllers: OpenDayLight and ONOS controllers, according to their efficacy. Mininet, which is meant for demonstrating the simulation of switching and end-devices, has been taken into account. Apart from this, we have used Wireshark Packet Analyser and iperf, which provides real-time traffic flow between Mininet and the Controller. Performance of the controller plays an important role in the network and answers question like how it is going to perform. There are the various factors that helps in evaluating the performance of SDN controller. After analysing, it has been found that ONOS has better performance, with respect to throughput, TCP Window Scaling, TCP/UDP Bandwidth, Burst Rate, Jitter, Goodput, TCP Stevens Graph, Round Trip Time (RTT), and its usability. This paper helps us to understand performance abilities between ONOS and ODL in medium-to-large scale SDN Networks.},
  langid = {english},
  keywords = {Burst-rate,Goodput,Jitter,ODL,ONOS,RTT,SDN,TCP/UDP bandwidth,Throughput,Wireshark}
}

@inproceedings{Singha2022499,
  type = {Conference Paper},
  title = {{{LEAPER}}: {{Fast}} and Accurate {{FPGA-based}} System Performance Prediction via Transfer Learning},
  author = {Singha, Gagandeep and Diamantopoulosb, Dionysios and {Gomez-Lunaa}, Juan and Stuijkc, Sander and Corporaalc, Henk and Mutlua, Onur},
  year = {2022},
  series = {Proceedings - {{IEEE International Conference}} on {{Computer Design}}: {{VLSI}} in {{Computers}} and {{Processors}}},
  volume = {2022-October},
  pages = {499--508},
  doi = {10.1109/ICCD56317.2022.00080},
  abstract = {Machine learning has recently gained traction as a way to overcome the slow accelerator generation and implementation process on an FPGA. It can be used to build performance and resource usage models that enable fast early-stage design space exploration. However, these models suffer from three main limitations. First, training requires large amounts of data (features extracted from design synthesis and implementation tools), which is cost-inefficient because of the time-consuming accelerator design and implementation process. Second, a model trained for a specific environment cannot predict performance or resource usage for a new, unknown environment. In a cloud system, renting a platform for data collection to build an ML model can significantly increase the total-cost-ownership (TCO) of a system. Third, ML-based models trained using a limited number of samples are prone to overfitting. To overcome these limitations, we propose LEAPER, a transfer learning-based approach for prediction of performance and resource usage in FPGA-based systems. The key idea of LEAPER is to transfer an ML-based performance and resource usage model trained for a low-end edge environment to a new, high-end cloud environment to provide fast and accurate predictions for accelerator implementation. Experimental results show that LEAPER (1) provides, on average across six workloads and five FPGAs, 85\% accuracy when we use our transferred model for prediction in a cloud environment with 5-shot learning and (2) reduces design-space exploration time for accelerator implementation on an FPGA by 10\texttimes, from days to only a few hours. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@book{sommerville2011software,
  title = {Software Engineering, 9/{{E}}},
  author = {Sommerville, Ian},
  year = {2011},
  publisher = {{Pearson Education India}}
}

@article{sotelomonge2021:conceptualization,
  title = {Conceptualization and Cases of Study on Cyber Operations against the Sustainability of the Tactical Edge},
  author = {Sotelo Monge, Marco Antonio and Maestre Vidal, Jorge},
  year = {2021},
  month = dec,
  journal = {Future Generation Computer Systems},
  volume = {125},
  pages = {869--890},
  issn = {0167-739X},
  doi = {10.1016/j.future.2021.07.016},
  urldate = {2023-01-15},
  abstract = {The last decade consolidated the cyberspace as fifth domain of military operations, which extends its preliminarily intelligence and information exchange purposes towards enabling complex offensive and defensive operations supported/supportively of parallel kinetic domain actuations. Although there is a plethora of well documented cases on strategic and operational interventions of cyber commands, the cyber tactical military edge is still a challenge, where cyber fires barely integrate to the traditional joint targeting cycle due to, among others, long planning/development times, asymmetric effects, strict target reachability requirements, or the fast propagation of collateral damage; the latter rapidly deriving on hybrid impacts (political, economic, social, etc.) and evidencing significant socio-technical gaps. In this context, it is expected that Tactical Clouds disruptively facilitate cyber operations at the edge while exposing the rest of the digital assets of the operation to them. On these grounds, the main purpose of the conducted research is to review and in depth analyze the risks and opportunities of jeopardizing the sustainability of the military Tactical Clouds at their cyber edge. Along with a 1) comprehensively formulation of the researched problematic, the study 2) formalizes the Tactical Denial of Sustainability (TDoS) concept; 3) introduces the phasing, potential attack surfaces, terrains and impact of TDoS attacks; 4) emphasizes the related human and socio-technical aspects; 5) analyzes the threats/opportunities inherent to their impact on the cloud energy efficiency; 6) reviews their implications at the military cyber thinking for tactical operations; 7) illustrates five extensive CONOPS that facilitate the understanding of the TDoS concept; and given the high novelty of the discussed topics, this paper 8) paves the way for further research and development actions.},
  langid = {english},
  keywords = {Cyber defense,Economical Denial of Sustainability,Military operations,Situational Awareness,Tactical Denial of Sustainability},
  file = {/Users/guru/Zotero/storage/9NFTZUDG/S0167739X21002788.html}
}

@inproceedings{srivastava2017scrum,
  title = {{{SCRUM}} Model for Agile Methodology},
  booktitle = {2017 International Conference on Computing, Communication and Automation ({{ICCCA}})},
  author = {Srivastava, Apoorva and Bhardwaj, Sukriti and Saraswat, Shipra},
  year = {2017},
  pages = {864--869},
  organization = {{IEEE}}
}

@article{steidl2023:pipeline,
  title = {The Pipeline for the Continuous Development of Artificial Intelligence Models\textemdash{{Current}} State of Research and Practice},
  author = {Steidl, Monika and Felderer, Michael and Ramler, Rudolf},
  year = {2023},
  month = jan,
  journal = {Journal of Systems and Software},
  pages = {111615},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111615},
  urldate = {2023-01-15},
  abstract = {Companies struggle to continuously develop and deploy Artificial Intelligence (AI) models to complex production systems due to AI characteristics while assuring quality. To ease the development process, continuous pipelines for AI have become an active research area where consolidated and in-depth analysis regarding the terminology, triggers, tasks, and challenges is required. This paper includes a Multivocal Literature Review (MLR) where we consolidated 151 relevant formal and informal sources. In addition, nine-semi structured interviews with participants from academia and industry verified and extended the obtained information. Based on these sources, this paper provides and compares terminologies for Development and Operations (DevOps) and Continuous Integration (CI)/Continuous Delivery (CD) for AI, Machine Learning Operations (MLOps), (end-to-end) lifecycle management, and Continuous Delivery for Machine Learning (CD4ML). Furthermore, the paper provides an aggregated list of potential triggers for reiterating the pipeline, such as alert systems or schedules. In addition, this work uses a taxonomy creation strategy to present a consolidated pipeline comprising tasks regarding the continuous development of AI. This pipeline consists of four stages: Data Handling, Model Learning, Software Development and System Operations. Moreover, we map challenges regarding pipeline implementation, adaption, and usage for the continuous development of AI to these four stages.},
  langid = {english},
  keywords = {CI/CD for AI,Continuous (end-to-end) lifecycle pipeline for AI,Continuous development of AI,DevOps for AI,MLOps,Multivocal literature review,primary},
  file = {/Users/guru/Zotero/storage/9YYAKFZR/Steidl et al. - 2023 - The pipeline for the continuous development of art.pdf;/Users/guru/Zotero/storage/CPTZ7699/1-s2.0-S0164121223000109-main.pdf;/Users/guru/Zotero/storage/VIJ3YU74/S0164121223000109.html}
}

@article{Stirbu2022,
  type = {Article},
  title = {Continuous Design Control for Machine Learning in Certified Medical Systems},
  author = {Stirbu, Vlad and Granlund, Tuomas and Mikkonen, Tommi},
  year = {2022},
  journal = {Software Quality Journal},
  doi = {10.1007/s11219-022-09601-5},
  abstract = {Continuous software engineering has become commonplace in numerous fields. However, in regulating intensive sectors, where additional concerns need to be taken into account, it is often considered difficult to apply continuous development approaches, such as devops. In this paper, we present an approach for using pull requests as design controls, and apply this approach to machine learning in certified medical systems leveraging model cards, a novel technique developed to add explainability to machine learning systems, as a regulatory audit trail. The approach is demonstrated with an industrial system that we have used previously to show how medical systems can be developed in a continuous fashion. \textcopyright{} 2022, The Author(s).},
  publication_stage = {Article in press},
  source = {Scopus},
  keywords = {extra_paper,primary},
  file = {/Users/guru/Zotero/storage/VSNINBB2/Stirbu et al. - 2022 - Continuous design control for machine learning in .pdf;/Users/guru/Zotero/storage/ZDRKVMP4/s11219-022-09601-5.pdf}
}

@article{storey2020:who,
  title = {The Who, What, How of Software Engineering Research: A Socio-Technical Framework},
  shorttitle = {The Who, What, How of Software Engineering Research},
  author = {Storey, Margaret-Anne and Ernst, Neil A. and Williams, Courtney and Kalliamvakou, Eirini},
  year = {2020},
  month = sep,
  journal = {Empirical Software Engineering},
  volume = {25},
  number = {5},
  pages = {4097--4129},
  issn = {1382-3256, 1573-7616},
  doi = {10.1007/s10664-020-09858-z},
  urldate = {2022-12-26},
  abstract = {Software engineering is a socio-technical endeavor, and while many of our contributions focus on technical aspects, human stakeholders such as software developers are directly affected by and can benefit from our research and tool innovations. In this paper, we question how much of our research addresses human and social issues, and explore how much we study human and social aspects in our research designs. To answer these questions, we developed a socio-technical research framework to capture the main beneficiary of a research study (the who), the main type of research contribution produced (the what), and the research strategies used in the study (how we methodologically approach delivering relevant results given the who and what of our studies). We used this Who-What-How framework to analyze 151 papers from two well-cited publishing venues\textemdash the main technical track at the International Conference on Software Engineering, and the Empirical Software Engineering Journal by Springer\textemdash to assess how much this published research explicitly considers human aspects. We find that although a majority of these papers claim the contained research should benefit human stakeholders, most focus predominantly on technical contributions. Although our analysis is scoped to two venues, our results suggest a need for more diversification and triangulation of research strategies. In particular, there is a need for strategies that aim at a deeper understanding of human and social aspects of software development practice to balance the design and evaluation of technical innovations. We recommend that the framework should be used in the design of future studies in order to steer software engineering research towards explicitly including human and social concerns in their designs, and to improve the relevance of our research for human stakeholders.},
  langid = {english},
  file = {/Users/guru/Zotero/storage/STP95JAF/Storey et al. - 2020 - The who, what, how of software engineering researc.pdf}
}

@article{studer2021towards,
  title = {Towards {{CRISP-ML}} ({{Q}}): A Machine Learning Process Model with Quality Assurance Methodology},
  author = {Studer, Stefan and Bui, Thanh Binh and Drescher, Christian and Hanuschkin, Alexander and Winkler, Ludwig and Peters, Steven and M{\"u}ller, Klaus-Robert},
  year = {2021},
  journal = {Machine Learning and Knowledge Extraction},
  volume = {3},
  number = {2},
  pages = {392--413},
  publisher = {{Multidisciplinary Digital Publishing Institute}}
}

@inproceedings{Subramonyam20211529,
  type = {Conference Paper},
  title = {Towards {{A}} Process Model for Co-Creating {{AI}} Experiences},
  author = {Subramonyam, Hariharan and Seifert, Colleen and Adar, Eytan},
  year = {2021},
  series = {{{DIS}} 2021 - {{Proceedings}} of the 2021 {{ACM Designing Interactive Systems Conference}}: {{Nowhere}} and {{Everywhere}}},
  pages = {1529--1543},
  doi = {10.1145/3461778.3462012},
  abstract = {Thinking of technology as a design material is appealing. It encourages designers to explore the material's properties to understand its capabilities and limitations - a prerequisite to generative design thinking. However, as a material, AI resists this approach because its properties only emerge as part of the user experience design. Therefore, designers and AI engineers must collaborate in new ways to create both the material and its application experience. We investigate the co-creation process through a design study with 10 pairs of designers and engineers. We find that design gprobes' with user data are a useful tool in defining AI materials. Through data probes, designers construct designerly representations of the envisioned AI experience (AIX) to identify desirable AI characteristics. Data probes facilitate divergent design thinking, material testing, and design validation. Based on our findings, we propose a process model for co-creating AIX and offer design considerations for incorporating data probes in AIX design tools. \textcopyright{} 2021 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Subramonyam2022,
  type = {Conference Paper},
  title = {Solving Separation-of-Concerns Problems in Collaborative Design of Human-{{AI}} Systems through Leaky Abstractions},
  author = {Subramonyam, Hariharan and Im, Jane and Seifert, Colleen and Adar, Eytan},
  year = {2022},
  series = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},
  doi = {10.1145/3491102.3517537},
  abstract = {In conventional software development, user experience (UX) designers and engineers collaborate through separation of concerns (SoC): designers create human interface specifications, and engineers build to those specifications. However, we argue that Human-AI systems thwart SoC because human needs must shape the design of the AI interface, the underlying AI sub-components, and training data. How do designers and engineers currently collaborate on AI and UX design? To find out, we interviewed 21 industry professionals (UX researchers, AI engineers, data scientists, and managers) across 14 organizations about their collaborative work practices and associated challenges. We find that hidden information encapsulated by SoC challenges collaboration across design and engineering concerns. Practitioners describe inventing ad-hoc representations exposing low-level design and implementation details (which we characterize as leaky abstractions) to "puncture"SoC and share information across expertise boundaries. We identify how leaky abstractions are employed to collaborate at the AI-UX boundary and formalize a process of creating and using leaky abstractions. \textcopyright{} 2022 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/TAWTKE7P/Subramonyam et al. - 2022 - Solving separation-of-concerns problems in collabo.pdf;/Users/guru/Zotero/storage/YASD966W/3491102.3517537.pdf}
}

@article{Sumarsono2022,
  type = {Article},
  title = {Development of a Mobile Health Infrastructure for Non-Communicable Diseases Using Design Science Research Method: A Case Study},
  author = {Sumarsono, Surahyo and Sakkinah, Intan Sulistyaningrum and Permanasari, Adhistya Erna and Pranggono, Bernardi},
  year = {2022},
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  doi = {10.1007/s12652-022-04322-w},
  abstract = {In this paper, we share our experience in designing and developing a mobile health (mHealth) infrastructure for non-communicable diseases (NCD) in rural areas in Indonesia called NusaHealth. The NusaHealth project builds a digital healthcare infrastructure involving universities, healthcare providers, and communities in an mHealth approach that puts patients at the center of health care. The Design Science Research Methodology (DSRM) framework was adopted in designing the mHealth application. The infrastructure to connect the mobile device network with the hospital information system was also developed. Our finding indicates that designing and developing an mHealth solution for rural areas in developing countries needs comprehensive approach and the implementation process should involve related partners and stakeholders. The NusaHealth was implemented in rural areas in Yogyakarta province in Indonesia. While Yogyakarta province successfully implemented the NusaHealth in rural areas, different activities need to be conducted to enhance community health by developing a formal mHealth system supported by local health district offices' policies and regulations. \textcopyright{} 2022, The Author(s).},
  publication_stage = {Article in press},
  source = {Scopus}
}

@article{Sutikno20221399,
  type = {Article},
  title = {Insights on the Internet of Things: Past, Present, and Future Directions},
  author = {Sutikno, Tole and Thalmann, Daniel},
  year = {2022},
  journal = {Telkomnika (Telecommunication Computing Electronics and Control)},
  volume = {20},
  number = {6},
  pages = {1399--1420},
  doi = {10.12928/TELKOMNIKA.v20i6.22028},
  abstract = {The internet of things (IoT) is rapidly expanding and improving operations in a wide range of real-world applications, from consumer IoT and enterprise IoT to manufacturing and industrial IoT (IIoT). Consumer markets, wearable devices, healthcare, smart buildings, agriculture, and smart cities are just a few examples. This paper discusses the current state of the IoT ecosystem, its primary applications and benefits, important architectural stages, some of the problems and challenges it faces, and its future. This paper explains how an appropriate IoT architecture that saves data, analyzes it, and recommends corrective action improves the process's ground reality. The IoT system architecture is divided into three layers: device, gateway, and platform. This then cascades into the four stages of the IoT architectural layout: sensors and actuators; gateways and data acquisition systems; edge IT data processing; and datacenter and cloud, which use high-end apps to collect data, evaluate it, process it, and provide remedial solutions. This elegant combination provides excellent value in automatic action. In the future, IoT will continue to serve as the foundation for many technologies. Machine learning will become more popular in the coming years as IoT networks take center stage in a variety of industries. \textcopyright{} This is an open access article under the CC BY-SA license.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{suzuki2020:roomshift,
  title = {{{RoomShift}}: {{Room-scale Dynamic Haptics}} for {{VR}} with {{Furniture-moving Swarm Robots}}},
  shorttitle = {{{RoomShift}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Suzuki, Ryo and Hedayati, Hooman and Zheng, Clement and Bohn, James L. and Szafir, Daniel and Do, Ellen Yi-Luen and Gross, Mark D. and Leithinger, Daniel},
  year = {2020},
  month = apr,
  series = {{{CHI}} '20},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3313831.3376523},
  urldate = {2023-01-15},
  abstract = {RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.},
  isbn = {978-1-4503-6708-0},
  keywords = {haptic interfaces,room-scale haptics,swarm robots,virtual reality},
  file = {/Users/guru/Zotero/storage/3BIGREGE/Suzuki et al_2020_RoomShift_Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.pdf}
}

@inproceedings{syahputri2020:does,
  title = {Does System Based on Artificial Intelligence Need Software Engineering Method? {{Systematic}} Review},
  booktitle = {2020 Fifth International Conference on Informatics and Computing ({{ICIC}})},
  author = {Syahputri, Irdina Wanda and Ferdiana, Ridi and Kusumawardani, Sri Suning},
  year = {2020},
  month = nov,
  pages = {1--6},
  doi = {10.1109/ICIC50835.2020.9288582},
  abstract = {Software engineering is the most important stage in developing a system. Software engineering is used to facilitate developers in developing systems in the form of a mobile, web, or artificial intelligence-based system. Systematic Review is a way to find data and related problems that can strengthen a person to conduct a study. In this paper. Researchers conducted a systematic review to find whether an Artificial Intelligence-based system requires Software Engineering when designing the system. The main purpose of this systematic review is to gather prior research related to developing Artificial Intelligence-based systems from design to the implementation phase and discover what methods are they common use in developing their systems and define what is the reason behind they selected method or even does not use Software Engineering methods in developing them.},
  keywords = {agile for AI system,AI method,Artificial intelligence,Informatics,primary,Software engineering,software engineering method,systematic review,Systematics,waterfall for AI system},
  file = {/Users/guru/Zotero/storage/G7PGBXV6/Syahputri et al_2020_Does system based on artificial intelligence need software engineering method_2020 fifth international conference on informatics and computing (ICIC).pdf}
}

@inproceedings{Szopinski2020,
  type = {Conference Paper},
  title = {Because Your Taxonomy Is Worth It: {{Towards}} a Framework for Taxonomy Evaluation},
  author = {Szopinski, Daniel and Schoormann, Thorsten and Kundisch, Dennis},
  year = {2020},
  series = {27th {{European Conference}} on {{Information Systems}} - {{Information Systems}} for a {{Sharing Society}}, {{ECIS}} 2019},
  abstract = {Taxonomies constitute one fundamental type of artefact in design science, describing and classifying existing or future objects of a domain. Taxonomies support researchers and practitioners with analysing and understanding a domain, which in turn is a prerequisite for theory building. Despite the increasing interest in taxonomies (and methodological guidance for building them), there is hardly any guidance for researchers on how to rigorously evaluate taxonomies. Based on a literature analysis, this study sheds light on the question of whether, when, and how researchers currently evaluate taxonomies. We critically synthesize and comprehensively review 306 articles that are concerned with taxonomies. Surprisingly, we find that taxonomies are rarely evaluated in IS research, nor is there any consistency in terms of methods used for evaluations. We describe the methods used by IS researchers to evaluate taxonomies after taxonomy building has been completed. Being the first to systematically analyse taxonomy evaluation, we propose a preliminary version of a framework for taxonomy evaluation which enables researchers to choose among the wide range of taxonomy evaluation methods available. Our study advances an informed and purposeful evaluation of taxonomies and contributes to bridging the gap between abstract design science evaluation strategies and concrete taxonomy evaluation methods. \textcopyright{} 27th European Conference on Information Systems - Information Systems for a Sharing Society, ECIS 2019. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{takeuchi2018:obtaining,
  title = {Obtaining {{Exhaustive Answer Set}} for {{Q}}\&{{A-based Inquiry System}} Using {{Customer Behavior}} and {{Service Function Modeling}}},
  author = {Takeuchi, Hironori and Masuda, Satoshi and Miyamoto, Kohtaroh and Akihara, Shiki},
  year = {2018},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 22nd {{International Conference}}, {{KES-2018}}, {{Belgrade}}, {{Serbia}}},
  volume = {126},
  pages = {986--995},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2018.08.033},
  urldate = {2023-01-15},
  abstract = {When customers are interested in a service or intend to buy it, they sometimes have questions on that service. In this study, we considered an inquiry system in which customers ask questions on a specific service and obtain correct information on the service. For such an inquiry system, a question-answering (Q\&A) technology is needed. Many programming modules for such a technology have been developed and can be easily used for system development. In many Q\&A technologies, machine-learning techniques are involved, and we need to prepare training data consisting of pairs of an answer and assumed questions. For training-data preparation, an answer set for a service should be defined as the first step and the answer set should cover all the information on the service that customers may ask about. By using a customer-behavior model and introducing a service-function model, we propose a method of effectively collecting knowledge information for an answer set on a service. Through a case study, we show that we can collect exhaustive knowledge information for an answer set with our method compared to the case in which domain experts collect knowledge information in their own way. For an actual project, we also considered an actual inquiry-system-development project, with training data obtained with the proposed method, and showed that the system covers almost all the information on the service that customers may ask after a user test.},
  langid = {english},
  keywords = {Answer Set,Customer Behavior Model,Inquiry System,Service Function Model}
}

@inproceedings{takeuchi2021:reference,
  title = {Reference Model for Agile Development of Machine Learning-Based Service Systems},
  booktitle = {2021 28th Asia-Pacific Software Engineering Conference Workshops ({{APSEC}} Workshops)},
  author = {Takeuchi, Hironori and Kaiya, Haruhiko and Nakagawa, Hiroyuki and Ogata, Shinpei},
  year = {2021},
  month = dec,
  pages = {17--20},
  doi = {10.1109/APSECW53869.2021.00014},
  abstract = {In this study, as a reusable knowledge for the effective system development we built a reference agile development model for machine learning(ML)-based systems. For this purpose, we collected project data on ML-based service system development projects (ML projects). From the collected practice data, we identified the common activities in these projects to extend the existing development models such as a generic agile development model and the workflow model for ML projects. Through the reference model represented by ArchiMate that is an enterprise modeling language, it was confirmed that the project participants from the business division can understand the activities that they should be involved in.},
  keywords = {agile development,Business,Conferences,Data models,Machine learning-based system,primary,reference model,Software engineering},
  file = {/Users/guru/Zotero/storage/QEZ372HQ/Takeuchi et al. - 2021 - Reference model for agile development of machine l.pdf}
}

@inproceedings{Takeuchi202117,
  type = {Conference Paper},
  title = {Reference Model for Agile Development of Machine Learning-Based Service Systems},
  author = {Takeuchi, Hironori and Kaiya, Haruhiko and Nakagawa, Hiroyuki and Ogata, Shinpei},
  year = {2021},
  series = {Proceedings - {{Asia-Pacific Software Engineering Conference}}, {{APSEC}}},
  pages = {17--20},
  doi = {10.1109/APSECW53869.2021.00014},
  abstract = {In this study, as a reusable knowledge for the effective system development we built a reference agile development model for machine learning(ML)-based systems. For this purpose, we collected project data on ML-based service system development projects (ML projects). From the collected practice data, we identified the common activities in these projects to extend the existing development models such as a generic agile development model and the workflow model for ML projects. Through the reference model represented by ArchiMate that is an enterprise modeling language, it was confirmed that the project participants from the business division can understand the activities that they should be involved in. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary,repeated_study}
}

@article{takeuchi2022:method,
  title = {Method for {{Constructing Machine Learning Project Canvas Based}} on {{Enterprise Architecture Modeling}}},
  author = {Takeuchi, Hironori and Ito, Yu and Yamamoto, Shuichiro},
  year = {2022},
  month = jan,
  journal = {Procedia Computer Science},
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 26th {{International Conference KES2022}}},
  volume = {207},
  pages = {425--434},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2022.09.077},
  urldate = {2023-01-15},
  abstract = {In this study, we consider projects in which systems are developed using machine learning (ML) techniques. An ML project canvas has been proposed to represent the project so that stakeholders can have a common understanding of the project. In many cases, this canvas must be constructed by business division practitioners without sufficient support from data scientists and the quality of the canvas model is dependent on the skills or experience of the practitioner. Therefore, we propose a method for constructing a project-specific project canvas model using the business\textendash AI alignment model, and confirm the effectiveness of the method through the analysis of ML project practices.},
  langid = {english},
  keywords = {business–IT alignment,canvas model,Machine learning}
}

@article{Takeuchi2022725,
  type = {Article},
  title = {Constructing Reusable Knowledge for Machine Learning Projects Based on Project Practices},
  author = {Takeuchi, Hironori and Imazaki, Kota and Kuno, Noriyoshi and Doi, Takuo and Motohashi, Yosuke},
  year = {2022},
  journal = {Intelligent Decision Technologies},
  volume = {16},
  number = {4},
  pages = {725--735},
  doi = {10.3233/IDT-220252},
  abstract = {Recently, machine learning (ML) techniques have been introduced into various domains. This study focuses on projects for the development of ML-based service systems in which ML techniques are applied to enterprise functions. In these projects, constructing reusable knowledge on projects that develop ML-based service systems is important to effectively implement such projects. Here, the collection of insights and development of architecture and design patterns for ML-based service systems are considered. We propose a method for collecting insights by referring to a development model based on project practices and developing patterns for ML projects as an enterprise architecture model. Through a practice, we attempt to collect insights as best practices and construct design patterns for ML projects using the proposed method. \textcopyright{} 2022 - IOS Press. All rights reserved.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/RER2QVDT/Takeuchi et al. - 2022 - Constructing reusable knowledge for machine learni.pdf}
}

@article{Tambon2022,
  type = {Article},
  title = {How to Certify Machine Learning Based Safety-Critical Systems? {{A}} Systematic Literature Review},
  author = {Tambon, Florian and Laberge, Gabriel and An, Le and Nikanjam, Amin and Mindom, Paulina Stevia Nouwou and Pequignot, Yann and Khomh, Foutse and Antoniol, Giulio and Merlo, Ettore and Laviolette, Fran{\c c}ois},
  year = {2022},
  journal = {Automated Software Engineering},
  volume = {29},
  number = {2},
  doi = {10.1007/s10515-022-00337-x},
  abstract = {Context: Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called ``safety-critical'' systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches. Objective: This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question ``How to Certify Machine Learning Based Safety-critical Systems?''. Method: We conduct a Systematic Literature Review (SLR) of research papers published between 2015 and 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted. Results: The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of ML models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mentioned main pillars that are for now mainly studied separately. Conclusion: We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions. \textcopyright{} 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
  publication_stage = {Final},
  source = {Scopus}
}

@incollection{tanque2021:chapter,
  title = {Chapter 2 - {{Knowledge Representation}} and {{Reasoning}} in {{AI-Based Solutions}} and {{IoT Applications}}},
  booktitle = {Artificial {{Intelligence}} to {{Solve Pervasive Internet}} of {{Things Issues}}},
  author = {Tanque, Marcus},
  editor = {Kaur, Gurjit and Tomar, Pradeep and Tanque, Marcus},
  year = {2021},
  month = jan,
  pages = {13--49},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-818576-6.00002-2},
  urldate = {2023-01-15},
  abstract = {Artificial intelligence (AI)-based solutions, knowledge representation and reasoning, and the Internet of Things applications have transformed how researchers and practitioners view the analytical and computational capabilities. The disruptive evolution of these technologies has encouraged researchers and practitioners to develop integrated AI-based analytical solutions needed for solving pervasive issues affecting computational applications. The capabilities include AI, knowledge Representation and Reasoning and Internet of Things. Such capabilities are designed to support AI-based solutions, knowledge representation and reasoning, and the Internet of Things (IoT) applications. These technology trends involve relevant computational areas, that is, intelligent devices, sensors, autonomous vehicles, robotics, virtual reality, augmented intelligence, and others. The study addresses and validates solutions on how researchers can solve issues that affect AI, knowledge representation and reasoning, and IoT applications.},
  isbn = {978-0-12-818576-6},
  langid = {english},
  keywords = {and reasoning,artificial general networks,Artificial intelligence,artificial neural networks,cognitive informatics,cognitive science,deep learning,intelligent machine,Internet of Things,knowledge representation,machine learning},
  file = {/Users/guru/Zotero/storage/FAURA8UM/B9780128185766000022.html}
}

@article{tao2019testing,
  title = {Testing and Quality Validation for Ai Software\textendash Perspectives, Issues, and Practices},
  author = {Tao, Chuanqi and Gao, Jerry and Wang, Tiexin},
  year = {2019},
  journal = {IEEE access : practical innovations, open solutions},
  volume = {7},
  pages = {120164--120175},
  publisher = {{IEEE}}
}

@article{tepjit2019:state,
  title = {The State of Framework Development for Implementing Reasoning Mechanisms in Smart Cyber-Physical Systems: {{A}} Literature Review},
  shorttitle = {The State of Framework Development for Implementing Reasoning Mechanisms in Smart Cyber-Physical Systems},
  author = {Tepjit, Sirasak and Horv{\'a}th, Imre and Rus{\'a}k, Zolt{\'a}n},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Design and Engineering},
  volume = {6},
  number = {4},
  pages = {527--541},
  issn = {2288-4300},
  doi = {10.1016/j.jcde.2019.04.002},
  urldate = {2023-01-15},
  abstract = {Smart CPSs (S-CPSs) have been evolving beyond what was identified by the traditional definitions of CPSs. The objective of our research is to investigate the concepts and implementations of reasoning processes for S-CPSs, and more specifically, the frameworks proposed for the fuzzy front end of their reasoning mechanisms. The objectives of the paper are: (i) to analyze the framework concepts and implementations of CPS, (ii) to review the literature concerning system-level reasoning and its enablers from the points of view of the processed knowledge, building awareness, reasoning mechanisms, decision making, and adaptation. Our findings are: (i) awareness and adaptation behaviors are considered as system-level smartness of S-CPSs that are not achieved by traditional design approaches; (ii) model-based and composability approaches insufficiently support the development of reasoning mechanisms for S-CPSs; (iii) frameworks for development of reasoning in S-CPS should support compositional design. Based on the conclusions above, we argue that coping with the challenges of compositionality requires both software-level integration and holistic fusion of knowledge by means of semantic transformations. This entails the need for a multi aspect framework that is able to capture at least conceptual, functional, architectural, informational, interoperation, and behavioral aspects. It needs further investigation if a compositionality enabling framework should appear in the form of a meta-framework (abstract) or in the form of a semantically integrated (concrete) framework.},
  langid = {english},
  keywords = {Compositionality,Cyber-physical systems,Development framework,System level reasoning,System smartness},
  file = {/Users/guru/Zotero/storage/QJUS3UYJ/Tepjit et al_2019_The state of framework development for implementing reasoning mechanisms in_Journal of Computational Design and Engineering.pdf;/Users/guru/Zotero/storage/TNFQ62T4/S2288430018302719.html}
}

@article{tian2022:relationships,
  title = {Relationships between Software Architecture and Source Code in Practice: {{An}} Exploratory Survey and Interview},
  shorttitle = {Relationships between Software Architecture and Source Code in Practice},
  author = {Tian, Fangchao and Liang, Peng and Babar, Muhammad Ali},
  year = {2022},
  month = jan,
  journal = {Information and Software Technology},
  volume = {141},
  pages = {106705},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2021.106705},
  urldate = {2023-01-15},
  abstract = {Context Software Architecture (SA) and Source Code (SC) are two intertwined artefacts that represent the interdependent design decisions made at different levels of abstractions - High-Level (HL) and Low-Level (LL). An understanding of the relationships between SA and SC is expected to bridge the gap between SA and SC for supporting maintenance and evolution of software systems. Objective We aimed at exploring practitioners' understanding about the relationships between SA and SC. Method We used a mixed-method that combines an online survey with 87 respondents and an interview with 8 participants to collect the views of practitioners from 37 countries about the relationships between SA and SC. Results Our results reveal that: practitioners mainly discuss five features of relationships between SA and SC; a few practitioners have adopted dedicated approaches and tools in the literature for identifying and analyzing the relationships between SA and SC despite recognizing the importance of such information for improving a system's quality attributes, especially maintainability and reliability. It is felt that cost and effort are the major impediments that prevent practitioners from identifying, analyzing, and using the relationships between SA and SC. Conclusions The results have empirically identified five features of relationships between SA and SC reported in the literature from the perspective of practitioners and a systematic framework to manage the five features of relationships should be developed with dedicated approaches and tools considering the cost and benefit of maintaining the relationships.},
  langid = {english},
  keywords = {Relationship industrial survey,Software architecture,Source code}
}

@article{Tipaldi2018482,
  type = {Article},
  title = {Development Strategies for the Satellite Flight Software On-Board {{Meteosat Third Generation}}},
  author = {Tipaldi, Massimo and Legendre, Cedric and Koopmann, Olliver and Ferraguto, Massimo and Wenker, Ralf and D'Angelo, Gianni},
  year = {2018},
  journal = {Acta Astronautica},
  volume = {145},
  pages = {482--491},
  doi = {10.1016/j.actaastro.2018.02.020},
  abstract = {Nowadays, satellites are becoming increasingly software dependent. Satellite Flight Software (FSW), that is to say, the application software running on the satellite main On-Board Computer (OBC), plays a relevant role in implementing complex space mission requirements. In this paper, we examine relevant technical approaches and programmatic strategies adopted for the development of the Meteosat Third Generation Satellite (MTG) FSW. To begin with, we present its layered model-based architecture, and the means for ensuring a robust and reliable interaction among the FSW components. Then, we focus on the selection of an effective software development life cycle model. In particular, by combining plan-driven and agile approaches, we can fulfill the need of having preliminary SW versions. They can be used for the elicitation of complex system-level requirements as well as for the initial satellite integration and testing activities. Another important aspect can be identified in the testing activities. Indeed, very demanding quality requirements have to be fulfilled in satellite SW applications. This manuscript proposes a test automation framework, which uses an XML-based test procedure language independent of the underlying test environment. Finally, a short overview of the MTG FSW sizing and timing budgets concludes the paper. \textcopyright{} 2018 IAA},
  publication_stage = {Final},
  source = {Scopus}
}

@article{tomiyama2019:development,
  title = {Development Capabilities for Smart Products},
  author = {Tomiyama, Tetsuo and Lutters, Eric and Stark, Rainer and Abramovici, Michael},
  year = {2019},
  month = jan,
  journal = {CIRP Annals},
  volume = {68},
  number = {2},
  pages = {727--750},
  issn = {0007-8506},
  doi = {10.1016/j.cirp.2019.05.010},
  urldate = {2023-01-15},
  abstract = {Smart products supported by new step-changing technologies, such as Internet of Things and artificial intelligence, are now emerging in the market. Smart products are cyber physical systems with services through Internet connection. For example, smart vehicles equipped with advanced embedded intelligence are connected to other vehicles, people, and environment, and offer innovative data-driven services. Since smart products are software-intensive, data-driven, and service-conscious, their development clearly needs new capabilities underpinned by advanced tools, methods, and models. This paper reviews the status and trends of these emerging development technologies such as model-based systems engineering and digital twin.},
  langid = {english},
  keywords = {Design,Model-based systems engineering,Product development}
}

@article{tsai2023:crowdtesting,
  title = {Crowdtesting {{Practices}} and {{Models}}: {{An Empirical Approach}}},
  shorttitle = {Crowdtesting {{Practices}} and {{Models}}},
  author = {Tsai, Wei-Tek and Zhang, Li and Hu, Shufeng and Fan, Zizheng and Wang, Qianyu},
  year = {2023},
  month = feb,
  journal = {Information and Software Technology},
  volume = {154},
  pages = {107103},
  issn = {0950-5849},
  doi = {10.1016/j.infsof.2022.107103},
  urldate = {2023-01-15},
  abstract = {Context Crowdsourced software testing (CST) has received significant attention. After these years, CST has made new progress and changes. Objective While current literature lists many CST challenges, this paper analyzes industrial CST practices, finds that many challenges already have practical solutions, summarizes their commonalities, and comes up with new CST models and processes. Method We look for well-known CST websites to participate in and take a secret and unobtrusive approach where customers, platform managers, and fellow workers do not know that we are mainly interested in CST research. We then register at selected CST websites, collect any public documents such as whitepapers, open rules, and public training materials, and join as many test tasks as possible. Results We analyze the confrontation and collaboration among clients, platforms, and workers in the CST sessions. Clients want to get as much bug information as possible for a small amount of pay, but workers want to get paid as much as possible for a small amount of bug information. We also study the process and method of selecting suitable CST workers. Based on these, this paper proposes three future research directions. Conclusion Data security and privacy at CST are paramount. If this problem can be overcome, CST will have wider applications. Additionally, the integration of workers, internal workers, software automation, and artificial intelligence will be major drivers for CST. It is also critical to develop a standardized CST structure and processes, and this will push the field to grow significantly.},
  langid = {english},
  keywords = {Crowdsourced software development (CSD),Crowdsourced software testing (CST)}
}

@article{turabieh2019:iterated,
  title = {Iterated Feature Selection Algorithms with Layered Recurrent Neural Network for Software Fault Prediction},
  author = {Turabieh, Hamza and Mafarja, Majdi and Li, Xiaodong},
  year = {2019},
  month = may,
  journal = {Expert Systems with Applications},
  volume = {122},
  pages = {27--42},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2018.12.033},
  urldate = {2023-01-15},
  abstract = {Software fault prediction (SFP) is typically used to predict faults in software components. Machine learning techniques (e.g., classification) are widely used to tackle this problem. With the availability of the huge amount of data that can be obtained from mining software historical repositories, it becomes possible to have some features (metrics) that are not correlated with the faults, which consequently mislead the learning algorithm and thus decrease its performance. One possible solution to eliminate those metrics is Feature Selection (FS). In this paper, a novel FS approach is proposed to enhance the performance of a layered recurrent neural network (L-RNN), which is used as a classification technique for the SFP problem. Three different wrapper FS algorithms (i.e, Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), and Binary Ant Colony Optimization (BACO)) were employed iteratively. To assess the performance of the proposed approach, 19 real-world software projects from PROMISE repository are investigated and the experimental results are discussed. Receiver operating characteristic - area under the curve (ROC-AUC) is used as a performance measure. The results are compared with other state-of-art approaches including Na\"ive Bayes (NB), Artificial Neural Network (ANN), logistic regression (LR), the k-nearest neighbors (k-NN) and C4.5 decision trees, in terms of area under the curve (AUC). Our results have demonstrated that the proposed approach can outperform other existing methods.},
  langid = {english},
  keywords = {Feature selection,Layered recurrent neural network,Software fault prediction},
  file = {/Users/guru/Zotero/storage/95MGRRCK/Turabieh et al_2019_Iterated feature selection algorithms with layered recurrent neural network for_Expert Systems with Applications.pdf;/Users/guru/Zotero/storage/9PXXFQ2W/S0957417418308030.html}
}

@inproceedings{turhan2015:4th,
  title = {4th International Workshop on Realizing {{AI}} Synergies in Software Engineering ({{RAISE}} 2015)},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Software Engineering}} - {{Volume}} 2},
  author = {Turhan, Burak and Bener, Ay{\c s}e and Harrison, Rachel and Miranskyy, Andriy and Meri{\c c}li, {\c C}etin and Minku, Leandro L.},
  year = {2015},
  month = may,
  series = {{{ICSE}} '15},
  pages = {991--992},
  publisher = {{IEEE Press}},
  address = {{Florence, Italy}},
  urldate = {2023-01-15},
  abstract = {This workshop is the fourth in the series and continued to build upon the work carried out at the previous iterations of the International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, which were held at ICSE in 2012, 2013 and 2014. RAISE 2015 brought together researchers and practitioners from the artificial intelligence (AI) and software engineering (SE) disciplines to build on the interdisciplinary synergies that exist and to stimulate further interaction across these disciplines. Mutually beneficial characteristics have appeared in the past few decades and are still evolving due to new challenges and technological advances. Hence, the question that motivates and drives the RAISE Workshop series is: "Are SE and AI researchers ignoring important insights from AI and SE?". To pursue this question, RAISE'15 explored not only the application of AI techniques to SE problems but also the application of SE techniques to AI problems. RAISE not only strengthens the AI-and-SE community but also continues to develop a roadmap of strategic research directions for AI and SE.}
}

@inproceedings{Unudulmaz2020375,
  type = {Conference Paper},
  title = {{{TMMI}} Integration with Agile and Test Process},
  author = {Unudulmaz, Ahmet and Kallpslz, Oya},
  year = {2020},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {375--378},
  doi = {10.1145/3383219.3386124},
  abstract = {Test Maturity Model Integration (TMMI) has a staged architecture and is a detailed model for process improvement. This model focus on test process improvements and contains five maturity levels. Applying this approach to the project has a positive impact on product quality and test effort. Besides positive impact, Test Maturity Model Integration contains heavy documentation. Applying scrum practice with this model can reduce the documentation and improve test process quality. In this case, Test Maturity Model Integration (TMMI) Level-2 Managed step is mapped with agile and test practices. Test Policy and Strategy, Test Planning, Test Monitoring and Control, Test Execution and Test Environment areas are discussed and they matched with not only scrum practice but also some test practices (Risk Based Testing, Use Case Testing). In the end, TMMI coverage percentage will be given and improvements areas will be defined. TMMI Level 4 metrics will be used for a development project and results will be showed. This model also can be adapted different areas, which is using agile and scrum practice. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{ur2013analysis,
  title = {Analysis of Requirement Engineering Processes, Tools/Techniques and Methodologies},
  author = {{ur Rehman}, Tousif and Khan, Muhammad Naeem Ahmed and Riaz, Naveed},
  year = {2013},
  journal = {International Journal of Information Technology and Computer Science (IJITCS)},
  volume = {5},
  number = {3},
  pages = {40}
}

@article{usman2018:developing,
  title = {Developing and Using Checklists to Improve Software Effort Estimation: {{A}} Multi-Case Study},
  shorttitle = {Developing and Using Checklists to Improve Software Effort Estimation},
  author = {Usman, Muhammad and Petersen, Kai and B{\"o}rstler, J{\"u}rgen and Santos Neto, Pedro},
  year = {2018},
  month = dec,
  journal = {Journal of Systems and Software},
  volume = {146},
  pages = {286--309},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2018.09.054},
  urldate = {2023-01-15},
  abstract = {Expert judgment based effort estimation techniques are widely used for estimating software effort. In the absence of process support, experts may overlook important factors during estimation, leading to inconsistent estimates. This might cause underestimation, which is a common problem in software projects. This multi-case study aims to improve expert estimation of software development effort. Our goal is two-fold: 1) to propose a process to develop and evolve estimation checklists for agile teams, and 2) to evaluate the usefulness of the checklists in improving expert estimation processes. The use of checklists improved the accuracy of the estimates in two case companies. In particular, the underestimation bias was reduced to a large extent. For the third case, we could not perform a similar analysis, due to the unavailability of historical data. However, when checklist was used in two sprints, the estimates were quite accurate (median Balanced Relative Error (BRE) bias of -0.05). The study participants from the case companies observed several benefits of using the checklists during estimation, such as increased confidence in estimates, improved consistency due to help in recalling relevant factors, more objectivity in the process, improved understanding of the tasks being estimated, and reduced chances of missing tasks.},
  langid = {english},
  keywords = {Agile software development,Case study,Checklist,Expert judgment based effort estimation},
  file = {/Users/guru/Zotero/storage/AP9N23JN/S0164121218302073.html}
}

@article{uysal2022:machine,
  title = {Machine Learning-Enabled Healthcare Information Systems in View of {{Industrial Information Integration Engineering}}},
  author = {Uysal, Murat Pasa},
  year = {2022},
  month = nov,
  journal = {Journal of Industrial Information Integration},
  volume = {30},
  pages = {100382},
  issn = {2452-414X},
  doi = {10.1016/j.jii.2022.100382},
  urldate = {2023-01-15},
  abstract = {Recent studies on Machine learning (ML) and its industrial applications report that ML-enabled systems may be at a high risk of failure or they can easily fall short of business objectives. Cutting-edge developments in this field have increased complexity and also brought new challenges for enterprise information integration. This situation can even get worse when considering the vital importance of ML-enabled healthcare information systems (HEIS). Therefore, the main argument of this paper is that we need to adopt the principles of Industrial Information Integration Engineering (IIIE) for the design, development, and deployment processes of ML-enabled systems. A mixed research paradigm is adopted, and therefore, this study is conducted by following the guidelines and principles of Action Research, Design Science Research, and IIIE. The contributions of this study are two-fold: (a) to draw researchers' and practitioners' attention to the integration problems of ML-enabled systems and discuss them in view of IIIE, and (b) to propose an enterprise integration architecture for ML-enabled HEIS of a university hospital, which is designed and developed by following the guidelines and principles of IIIE.},
  langid = {english},
  keywords = {Enterprise architecture,Healthcare information system,Hospital Information System,Industrial Information Integration Engineering,Machine learning,System architecture}
}

@article{vaidhyanathan2022:agile4mls,
  title = {{{Agile4MLS}}\textemdash{{Leveraging}} Agile Practices for Developing Machine Learning-Enabled Systems: {{An}} Industrial Experience},
  author = {Vaidhyanathan, Karthik and Chandran, Anish and Muccini, Henry and Roy, Regi},
  year = {2022},
  month = nov,
  journal = {IEEE Software},
  volume = {39},
  number = {6},
  pages = {43--50},
  issn = {1937-4194},
  doi = {10.1109/MS.2022.3195432},
  abstract = {Increased adoption of machine learning (ML) in business has led to a surge in ML-enabled systems in the market giving rise to new challenges. This article provides insights into Agile4MLS, highlighting the challenges we faced and the process we followed.},
  keywords = {6G mobile communication,Agile,Computer science,Development Methodology,Learning systems,Machine learning,Mathematics,ML-enabled systems,primary,Software engineering}
}

@article{Vaidhyanathan202243,
  type = {Article},
  title = {{{Agile4MLS}}\textemdash{{Leveraging}} Agile Practices for Developing Machine Learning-Enabled Systems an Industrial Experience},
  author = {Vaidhyanathan, Karthik and Chandran, Anish and Muccini, Henry and Roy, Regi},
  year = {2022},
  journal = {IEEE Software},
  volume = {39},
  number = {6},
  pages = {43--50},
  doi = {10.1109/MS.2022.3195432},
  abstract = {Increased adoption of machine learning (ML) in business has led to a surge in ML-enabled systems in the market giving rise to new challenges. This article provides insights into Agile4MLS, highlighting the challenges we faced and the process we followed. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary,repeated_study}
}

@inproceedings{Vakkuri2021,
  type = {Conference Paper},
  title = {Time for {{AI}} ({{Ethics}}) Maturity Model Is Now},
  author = {Vakkuri, Ville and Jantunen, Marianna and Halme, Erika and Kemell, Kai-Kristian and {Nguyen-Duc}, Anh and Mikkonen, Tommi and Abrahamsson, Pekka},
  year = {2021},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {2808},
  abstract = {There appears to be a common agreement that ethical concerns are of high importance when it comes to systems equipped with some sort of Artificial Intelligence (AI). Demands for ethical AI are declared from all directions. As a response, in recent years, public bodies, governments, and universities have rushed in to provide a set of principles to be considered when AI based systems are designed and used. We have learned, however, that high-level principles do not turn easily into actionable advice for practitioners. Hence, also companies are publishing their own ethical guidelines to guide their AI development. This paper argues that AI software is still software and needs to be approached from the software development perspective. The software engineering paradigm has introduced maturity model thinking, which provides a roadmap for companies to improve their performance from the selected viewpoints known as the key capabilities. We want to voice out a call for action for the development of a maturity model for AI software. We wish to discuss whether the focus should be on AI ethics or, more broadly, the quality of an AI system, called a maturity model for the development of AI systems. Copyright \textcopyright{} 2021 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
  publication_stage = {Final},
  source = {Scopus}
}

@article{vakkuri2021:eccola,
  title = {{{ECCOLA}} \textemdash{} {{A}} Method for Implementing Ethically Aligned {{AI}} Systems},
  author = {Vakkuri, Ville and Kemell, Kai-Kristian and Jantunen, Marianna and Halme, Erika and Abrahamsson, Pekka},
  year = {2021},
  month = dec,
  journal = {Journal of Systems and Software},
  volume = {182},
  pages = {111067},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111067},
  urldate = {2023-01-15},
  abstract = {Artificial Intelligence (AI) systems are becoming increasingly widespread and exert a growing influence on society at large. The growing impact of these systems has also highlighted potential issues that may arise from their utilization, such as data privacy issues, resulting in calls for ethical AI systems. Yet, how to develop ethical AI systems remains an important question in the area. How should the principles and values be converted into requirements for these systems, and what should developers and the organizations developing these systems do? To further bridge this gap in the area, in this paper, we present a method for implementing AI ethics: ECCOLA. Following a cyclical action research approach, ECCOLA has been iteratively developed over the course of multiple years, in collaboration with both researchers and practitioners.},
  langid = {english},
  keywords = {AI ethics,Artificial intelligence,Ethics,Implementing,Method},
  file = {/Users/guru/Zotero/storage/WYQ859T5/Vakkuri et al_2021_ECCOLA — A method for implementing ethically aligned AI systems_Journal of Systems and Software.pdf;/Users/guru/Zotero/storage/8R6BRT52/S0164121221001643.html}
}

@inproceedings{vakkuri2021:technical,
  title = {Technical Briefing: {{Hands-on}} Session on the Development of Trustworthy {{AI}} Software},
  booktitle = {2021 {{IEEE}}/{{ACM}} 43rd International Conference on Software Engineering: {{Companion}} Proceedings ({{ICSE-Companion}})},
  author = {Vakkuri, Ville and Kemell, Kai-Kristian and Abrahamsson, Pekka},
  year = {2021},
  month = may,
  pages = {332--333},
  issn = {2574-1926},
  doi = {10.1109/ICSE-Companion52605.2021.00142},
  abstract = {Following various real-world incidents involving both purely digital and cyber-physical Artificial Intelligence (AI) systems, AI Ethics has become a prominent topic of discussion in both research and practice, accompanied by various calls for trustworthy AI systems. Failures are often costly, and many of them stem from issues that could have been avoided during development. For example, AI ethics issues, such as data privacy are currently highly topical. However, implementing AI ethics in practice remains a challenge for organizations. Various guidelines have been published to aid companies in doing so, but these have not seen widespread adoption and may feel impractical. In this technical briefing, we discuss how to implement AI ethics. We showcase a method developed for this purpose, ECCOLA, which is based on academic research. ECCOLA is intended to make AI ethics more practical for developers in order to make it easier to incorporate into AI development to create trustworthy AI systems. It is a sprint-based and adaptive tool designed for agile development that facilitates reflection within the development team and helps developers make ethics into tangible product backlog items.},
  keywords = {Artificial intelligence,Design methods,Ethics,Guidelines,primary,Reflection,Software,Software engineering,Tools},
  file = {/Users/guru/Zotero/storage/LDP94TBK/Vakkuri et al_2021_Technical briefing_2021 IEEEACM 43rd international conference on software engineering Companion proceedings (ICSE-Companion).pdf}
}

@inproceedings{Vakkuri2021332,
  type = {Conference Paper},
  title = {Technical Briefing: {{Hands-on}} Session on the Development of Trustworthy {{AI}} Software},
  author = {Vakkuri, Ville and Kemell, Kai-Kristian and Abrahamsson, Pekka},
  year = {2021},
  series = {Proceedings - {{International Conference}} on {{Software Engineering}}},
  pages = {332--333},
  doi = {10.1109/ICSE-Companion52605.2021.00142},
  abstract = {Following various real-world incidents involving both purely digital and cyber-physical Artificial Intelligence (AI) systems, AI Ethics has become a prominent topic of discussion in both research and practice, accompanied by various calls for trustworthy AI systems. Failures are often costly, and many of them stem from issues that could have been avoided during development. For example, AI ethics issues, such as data privacy are currently highly topical. However, implementing AI ethics in practice remains a challenge for organizations. Various guidelines have been published to aid companies in doing so, but these have not seen widespread adoption and may feel impractical. In this technical briefing, we discuss how to implement AI ethics. We showcase a method developed for this purpose, ECCOLA, which is based on academic research. ECCOLA is intended to make AI ethics more practical for developers in order to make it easier to incorporate into AI development to create trustworthy AI systems. It is a sprint-based and adaptive tool designed for agile development that facilitates reflection within the development team and helps developers make ethics into tangible product backlog items. \textcopyright{} 2021 IEEE.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary,repeated_study}
}

@article{vale2016:twentyeight,
  title = {Twenty-Eight Years of Component-Based Software Engineering},
  author = {Vale, Tassio and Crnkovic, Ivica and {de Almeida}, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat{\~a} Cerqueira and Meira, Silvio Romero de Lemos},
  year = {2016},
  month = jan,
  journal = {Journal of Systems and Software},
  volume = {111},
  pages = {128--148},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2015.09.019},
  urldate = {2023-01-15},
  abstract = {The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
  langid = {english},
  keywords = {Component-based software development,Component-based software engineering,Software component,Systematic mapping study},
  file = {/Users/guru/Zotero/storage/DH7CMXEA/S0164121215002095.html}
}

@inproceedings{valenzuela-toledo2019:search,
  title = {Search Based Risk Reduction Supporting the Intelligent Components Selection Process},
  booktitle = {2019 {{IEEE CHILEAN}} Conference on Electrical, Electronics Engineering, Information and Communication Technologies ({{CHILECON}})},
  author = {{Valenzuela-Toledo}, Pablo and Cares, Carlos and Di{\'e}guez, Mauricio},
  year = {2019},
  month = nov,
  pages = {1--7},
  doi = {10.1109/CHILECON47746.2019.8987553},
  abstract = {In Component-Based Software Engineering, the problem of selecting software components involve several risk factors. Traditionally, these have been identified and mitigated using software project management techniques. However, the new demand for intelligent components has added complexity to the process. Despite the success and technological advances, its development in an environment ready for production faces many challenges. There are numbers of technical issues that limit their adoption and selection, due to the introduction of new risk factors, different from traditional ones. Thus, our goal is to formulate a technique to minimize the risk in the intelligent component selection process. To achieve this goal, first, we review the literature to Figure out how software project management takes care of the risk and to identify and classify the risks factor associated with intelligent software components. Second, we formulate the component selection problem as a search based optimization problem. And Third, we illustrate our proposal by presenting an example in the context of an air pollution forecasting component. As a result, we were able to: (1) identify a lack of useful tools to manage the risk factor effectively in a software project; (2) we classify intelligent component associated risk; and (3) we introduce a risk management technique that supports the component selection process by maximizing requirement accomplishment, that is, minimizing the risk of the provision of the functionalities that satisfy the requirements. Overall, our work is an initial step in using search-based optimization in risk management to the component selection process.},
  keywords = {Component Selection Problem,Risk Management,Search-Based Software Engineering},
  file = {/Users/guru/Zotero/storage/HIERA2XS/Valenzuela-Toledo et al_2019_Search based risk reduction supporting the intelligent components selection_2019 IEEE CHILEAN conference on electrical, electronics engineering, information and communication tec.pdf}
}

@article{valle-cruz2020:assessing,
  title = {Assessing the Public Policy-Cycle Framework in the Age of Artificial Intelligence: {{From}} Agenda-Setting to Policy Evaluation},
  shorttitle = {Assessing the Public Policy-Cycle Framework in the Age of Artificial Intelligence},
  author = {{Valle-Cruz}, David and Criado, J. Ignacio and {Sandoval-Almaz{\'a}n}, Rodrigo and {Ruvalcaba-Gomez}, Edgar A.},
  year = {2020},
  month = oct,
  journal = {Government Information Quarterly},
  volume = {37},
  number = {4},
  pages = {101509},
  issn = {0740-624X},
  doi = {10.1016/j.giq.2020.101509},
  urldate = {2023-01-15},
  abstract = {Governments are increasingly employing artificial intelligence (AI) enabled services though this is still a relatively new concept that is in nascent stages of implementation. Despite growing emphasis by governments on employing AI-enabled services, many citizens are skeptical of their benefits; this makes an analysis of AI-enabled services an important area of research, especially from the perspective of citizens. This paper employs IT assimilation theory and public value theory to develop a theoretical model that examines whether the introduction of AI-enabled services would generate public value for citizens in India. The model employs the Partial Least Square-Structural Equation Modeling (PLS-SEM) technique to examine how risk factors impact the uptake of AI-enabled services in India. Based on 315 interviews conducted in India, the study highlights that the breadth and depth assimilation of AI-enabled services positively impacts and enhances the satisfaction of citizens, which in turn generates public value. Artificial Intelligence is increasingly being used by public sector organisations. Previous research highlighted that the use of AI technologies in government could improve policy making processes, public service delivery and the internal management of public administrations. In this article, we explore to which extent the use of AI in the public sector impacts these core governance functions. Findings from the review of a sample of 250 cases across the European Union, show that AI is used mainly to support improving public service delivery, followed by enhancing internal management and only in a limited number assist directly or indirectly policy decision-making. The analysis suggests that different types of AI technologies and applications are used in different governance functions, highlighting the need to further in-depth investigation to better understand the role and impact of use in what is being defined the governance ``of, with and by AI''. A variety of studies have put forward heterogeneous stage models of e-government development since 2000. However, there is lack of research on a general framework of the stages of e-government development and further theoretical foundations for the stages of e-government development seldom are discussed. Therefore, this study focuses on providing a general framework of the stages of e-government development from public value perspective. First, this study conceptualizes public value evolution from public management paradigms. Specifically, the evolution of three kinds of public value, that is, transparency, efficiency, and engagement, is identified from public management paradigms. Second, a general framework of three stages of e-government development is proposed to investigate e-government evolution approach, which consistes of information stage, transaction stage, and engagement stage. Every stage of e-government development aims at providing different focus of public value. Third, a case study is provided to demonstrate the feasibility of the three-stage-model of e-government development. Eventually, the practical implications of this model of e-government development are analyzed accordingly. Public administrations invest heavily in the development of `smart' public services, including autonomous public service robots. Since public service robots are designed to operate unsupervised, robots must interact in an ethically acceptable way with citizens. Robots are often designed to provide a comfortable interaction with citizens, which can be achieved by making the robot's appearance and actions more human-like. This raises the question whether a human-like design affects the ethicalness evaluation of a robot's actions. In a laboratory experiment with eye-tracking (n1 = 156) and a representative, online vignette experiment (n2 = 1339), we find that a more human-like robot design draws more visual attention than a robot with a less human-like design. However, the robot's appearance does not affect the ethicalness evaluation of the robot's behavior. In contrast, our results show that it is not the more human-like appearance that influences evaluations of ethicalness, but a robot's ethical actions influence the extent to which it is perceived as human. We frame our findings in the scientific and practitioner debates on ethical rule-setting for (public) service robots. Artificial intelligence has become an important tool for governments around the world. However, it is not clear to what extent artificial intelligence can improve decision-making, and some policy domains have not been the focus of most recent studies, including the public budget process. More specifically, budget allocation is one of the areas in which AI may have greatest potential. Therefore, this study attempts to contribute to this gap in our existing knowledge by answering the following research question: To what extent can artificial intelligence techniques help distribute public spending to increase GDP, decrease inflation and reduce the Gini index? In order to respond to this question, this article proposes an algorithmic approach on how budget inputs (specific expenditures) are processed to generate certain outputs (economic, political, and social outcomes). The authors use the multilayer perceptron and a multiobjective genetic algorithm to analyze World Bank Open Data from 1960 to 2019, including 217 countries. The advantages of implementing this type of decision support system in public expenditures allocation arise from the ability to process large amounts of data and to find patterns that are not easy to detect, which include multiple non-linear relationships. Some technical aspects of the expenditure allocation process could be improved with the help of these kinds of techniques. In addition, the results of the AI-based approach are consistent with the findings of the scientific literature on public budgets, using traditional statistical techniques. With increasing ubiquity of artificial intelligence (AI) in modern societies, individual countries and the international community are working hard to create an innovation-friendly, yet safe, regulatory environment. Adequate regulation is key to maximize the benefits and minimize the risks stemming from AI technologies. Developing regulatory frameworks is, however, challenging due to AI's global reach, agency problems present in regulation, and the existence of widespread misconceptions about the very notion of regulation. This paper makes three claims: (1) Based on interdisciplinary insights, we show that AI-related challenges cannot be tackled effectively without sincere international coordination supported by robust, consistent domestic, regional, and international governance arrangements. (2) Against this backdrop, we propose the establishment of an international AI governance framework to spearhead initiatives to create a consistent, global enabling regulatory environment, which is necessary for the successful and responsible adoption of AI technologies. To facilitate the practical implementation of our recommendation, we provide a simplified impact assessment on regulatory architecture and governance design options, appropriate to the scope of the paper. (3) We draw attention to communication challenges, which we believe are underestimated barriers hindering contemporary efforts to develop AI regulatory regimes. We argue that a fundamental change of mindset regarding the nature of regulation is necessary to remove these, and put forward some recommendations on how to achieve this. The Open Government Partnership (OGP) is a prominent international initiative promoting open and responsive government. This includes efforts to socialize norms for civic participation in government institutions. Noting the close alignment of discourses on open government and e-participation, this analysis considers whether comparative data on countries' e-participation performance provides evidence of socialization by OGP. Comparative analysis suggests that OGP membership is correlated with higher e-participation scores, that this is not solely attributable to national political factors, but that alignment of national traditions and structures with civic participation norms has a positive moderating effect on OGP's relationship to e-participation. OGP's effect on collaborative e-decision-making is consistently more pronounced than OGP's effect on e-participation generally. This supports the assertion that OGP membership socializes participation norms in government institutions, and that this socialization effect is more pronounced in more democratic countries and in regard to more advanced participation norms. This study examines recently reformed classification frameworks in the United Kingdom, Australia, and New Zealand to better understand the changing nature of administrative secrecy. The analysis investigates and compares the drivers and outcomes of these classification reforms. While these case countries largely share incentives for and objectives of reform, they do not develop a uniform approach to restructuring their classification frameworks. Further, their reform processes vary, from incremental reforms in Australia to a more sweeping revamp of the UK's classification system. Overall, reform efforts point to a trend of increased simplification and proactive risk management. The classification frameworks in case countries have moved away from a dichotomous understanding of secrecy and transparency toward a gradient logic embedded within broader considerations regarding information management, exploitation and sharing. The institutionalisation of Smart City Functions (SCF) within local authorities creates significant IT Governance (ITG) challenges, including the need to foster a triadic alignment between the overall organization, the IT function and the SCF. Building on existing literature on ITG, smart cities, and the emerging conversation on Adaptive Governance in the public sector, the following exploratory question has been formulated for this study: How are ITG structural mechanisms implemented in city authorities to oversee and govern smart cities? To address this question, a qualitative multiple case study was carried out across three city authorities in Ireland characterised by diversity in their ITG structural arrangements to govern portfolios of smart city initiatives. From this analysis three types of ITG structural implementation are proposed named: Detached, Integrated, and Traditional. These are compared and discussed in relation to: (1) orientation of each approach; (2) decision-making authority; (3) alignment with the IT function and the overall municipal organization; and (4) the challenge perceived by the SCFs under each ITG arrangement. This research contributes to the academic conversation on adaptive governance, smart cities as well as to the broader ongoing debate on ITG in the public sector. Many policy-makers are struggling to understand participatory governance in the midst of technological changes. Advances in information and communication technologies (ICTs) continue to have an impact on the ways that policy-makers and citizens engage with each other throughout the policy-making process. A set of developments in the areas of opening government data, advanced analytics, visualization, simulation, and gaming, and ubiquitous citizen access using mobile and personalized applications is shaping the interactions between policy-makers and citizens. Yet the impact of these developments on the policy-makers is unclear. The changing roles and need for new capabilities required from the government are analyzed in this paper using two case studies. Salient new roles for policy-makers are outlined focused on orchestrating the policy-making process. Research directions are identified including understand the behavior of users, aggregating and analyzing content from scattered resources, and the effective use of the new tools. Understanding new policy-makers roles will help to bridge the gap between the potential of tools and technologies and the organizational realities and political contexts. We argue that many examples are available that enable learning from others, in both directions, developed countries experiences are useful for developing countries and experiences from the latter are valuable for the former countries. Driven by `success stories' reported by private sector firms, government agencies have also started adopting various Artificial Intelligence (AI) technologies in diverse domains (e.g. health, taxation, and education); however, extensive research is required in order to exploit the full potential of AI in the public sector, and leverage various AI technologies to address important problems/needs. This paper makes a contribution in this direction: it presents a novel approach, as well as the architecture of an ICT platform supporting it, for the advanced exploitation of a specific AI technology, namely chatbots, in the public sector in order to address a crucial issue: the improvement of communication between government and citizens (which has for long time been problematic). The proposed approach builds on natural language processing, machine learning and data mining technologies, and leverages existing data of various forms (such as documents containing legislation and directives, structured data from government agencies' operational systems, social media data, etc.), in order to develop a new digital channel of communication between citizens and government. Making use of appropriately structured and semantically annotated data, this channel enables `richer' and more expressive interaction of citizens with government in everyday language, facilitating and advancing both information seeking and conducting of transactions. Compared to existing digital channels, the proposed approach is appropriate for a wider range of citizens' interactions, with higher levels of complexity, ambiguity and uncertainty. In close co-operation with three Greek government agencies (the Ministry of Finance, a social security organization, and a big local government organization), this approach has been validated through a series of application scenarios. Governments increasingly use algorithmic models to inform their policy making process. Many suggest that employing such quantifications will lead to more efficient, more effective or otherwise better quality policy making. Yet, it remains unclear to what extent these benefits materialize and if so, how they are brought about. This paper draws on the sociology and policy science literature to study how algorithmic models, a particular type of quantification, are used in policy analysis. It presents the outcomes of 38 unstructured interviews with data scientists, policy analysts, and policy makers that work with algorithmic models in government. Based on an in-depth analysis of these interviews, I conclude that the usefulness of algorithmic models in policy analysis is best understood in terms of the commensurability of these quantifications. However, these broad communicative and organizational benefits can only be brought about if algorithmic models are handled with care. Otherwise, they may propagate bias, exclude particular social groups, and will entrench existing worldviews.},
  langid = {english},
  file = {/Users/guru/Zotero/storage/4XKZ4WI8/S0740624X20302884.html}
}

@article{vangeest2021:design,
  title = {Design of a Reference Architecture for Developing Smart Warehouses in Industry 4.0},
  author = {{van Geest}, Maarten and Tekinerdogan, Bedir and Catal, Cagatay},
  year = {2021},
  month = jan,
  journal = {Computers in Industry},
  volume = {124},
  pages = {103343},
  issn = {0166-3615},
  doi = {10.1016/j.compind.2020.103343},
  urldate = {2023-01-15},
  abstract = {Smart warehousing aims at increasing the overall service quality, productivity, and efficiency while minimizing the costs and failures. For designing the reference architecture, we apply a domain-driven architecture design approach and use the architecture design knowledge as presented in the software architecture design literature. We first provide the results of a thorough domain analysis process to smart warehouses to identify the key concerns that shape the architecture of smart warehouses. The domain model is presented using feature diagrams that show the common and variant features of smart warehouses. The domain analysis process is followed by the architecture design process, whereby we have used architecture viewpoints for modeling the reference architecture. Different businesses require different kinds of smart warehouses. Therefore, we present the generic business process model for both traditional warehouses and smart warehouses. The business modeling process is followed by the architecture design process, whereby we have used architecture viewpoints for modeling the reference architecture. Once the reference architecture is designed, a case study has been used to evaluate the proposed reference architecture. The case study has been conducted at a large warehouse in the food industry and illustrates the overall design method and presents the lessons learned.},
  langid = {english},
  keywords = {Case study research,Reference architecture,Smart warehouses,Software architecture},
  file = {/Users/guru/Zotero/storage/9L6EK856/van Geest et al_2021_Design of a reference architecture for developing smart warehouses in industry 4_Computers in Industry.pdf;/Users/guru/Zotero/storage/DZLBCGQ9/S0166361520305777.html}
}

@article{vizcaíno2016:validated,
  title = {A Validated Ontology for Global Software Development},
  author = {Vizca{\'i}no, Aurora and Garc{\'i}a, Felix and Piattini, Mario and Beecham, Sarah},
  year = {2016},
  month = may,
  journal = {Computer Standards \& Interfaces},
  volume = {46},
  pages = {66--78},
  issn = {0920-5489},
  doi = {10.1016/j.csi.2016.02.004},
  urldate = {2023-01-15},
  abstract = {The global software development (GSD) paradigm has, over the last 15fifteen years, shifted from being novel and ground breaking to being widely adopted and mainstream. This wide adoption is partly owing to the many benefits provided by GSD, such as reduced labour costs, proximity to new markets and access to a diverse and experienced skills pool. Yet taking advantage of these benefits is far from straightforward, and research literature now includes a proliferation of guidelines, reviews and models to support the GSD industry. Although this active area of study is firmly established as a research area in its own right, the boundaries between general software engineering and GSD are somewhat confused and poorly defined. In an effort to consolidate our understanding of GSD, we have developed an ontology in order to capture the most relevant terms, concepts and relationships related to the goals, barriers and features of GSD projects. The study we present here builds on research conducted in a collaboration project between industry and academia, in which we developed an ontology in order to provide practitioners with a ``common language and conceptualisation''. Its successful outcome encouraged us to create a broader ontology that captures the current trends in GSD literature. The key ontology, along with its three subontologies, are the result of a review of the relevant literature, together with several expert evaluations. This ontology can serve as a useful introduction to GSD for researchers who are new to the paradigm. Moreover, practitioners can take advantage of it in order to contextualise their projects and predict and detect possible barriers. What is more, using a common language will help both researchers and practitioners to avoid ambiguities and misunderstanding.},
  langid = {english},
  keywords = {Distributed software development (DSD),Global software development (GSD),Global software engineering (GSE),Meta-model,Ontology},
  file = {/Users/guru/Zotero/storage/R76UXRTP/S0920548916300046.html}
}

@inproceedings{vora2017home,
  title = {Home-Based Exercise System for Patients Using {{IoT}} Enabled Smart Speaker},
  booktitle = {2017 {{IEEE}} 19th International Conference on E-Health Networking, Applications and Services ({{Healthcom}})},
  author = {Vora, Jayneel and Tanwar, Sudeep and Tyagi, Sudhanshu and Kumar, Neeraj and Rodrigues, Joel JPC},
  year = {2017},
  pages = {1--6},
  organization = {{IEEE}}
}

@article{vyhmeister2022:lessons,
  title = {Lessons Learn on Responsible {{AI}} Implementation: The {{ASSISTANT}} Use Case},
  shorttitle = {Lessons Learn on Responsible {{AI}} Implementation},
  author = {Vyhmeister, Eduardo and Castane, Gabriel G. and Buchholz, Johan and {\"O}stberg, Per-Olov},
  year = {2022},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {10th {{IFAC Conference}} on {{Manufacturing Modelling}}, {{Management}} and {{Control MIM}} 2022},
  volume = {55},
  number = {10},
  pages = {377--382},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2022.09.422},
  urldate = {2023-01-15},
  abstract = {Currently, pioneer companies are working hard to construct applied ethical frameworks in different sectors for using AI components that generate trust in their clients and workforce. However, independent of these few companies, there is still a considerable gap between understanding the impact of using responsible AI components, the implications of the lack of use, and what is currently applied in the industrial sector. Given that industry has shown an increased commitment to incorporating AI components, works focus on broadening the understanding of manufacturing sector stakeholders of what approaches could be considered within AI life-cycle, reducing the gap between principles and actionable requirements, and defining fundamental considerations based on risk management for incorporating, and managing, AI-based on responsible AI are required. In this work, we present a summary of the most suitable approaches that can be used for implementation and the lessons learned from a European Funded project (ASSISTANT).},
  langid = {english},
  keywords = {AI,AI en manufacturing,AI ethics,design methodology for HMS,Human centered automation,responsible AI,standardisation},
  file = {/Users/guru/Zotero/storage/KQGY6HI8/Vyhmeister et al_2022_Lessons learn on responsible AI implementation_IFAC-PapersOnLine.pdf;/Users/guru/Zotero/storage/DCT3RDQB/S2405896322017086.html}
}

@inproceedings{Vyhmeister2022377,
  type = {Conference Paper},
  title = {Lessons Learn on Responsible {{AI}} Implementation: The {{ASSISTANT}} Use Case},
  author = {Vyhmeister, Eduardo and Castane, Gabriel G. and Buchholz, Johan and {\"O}stberg, Per-Olov},
  year = {2022},
  series = {{{IFAC-PapersOnLine}}},
  volume = {55},
  pages = {377--382},
  doi = {10.1016/j.ifacol.2022.09.422},
  abstract = {Currently, pioneer companies are working hard to construct applied ethical frameworks in different sectors for using AI components that generate trust in their clients and workforce. However, independent of these few companies, there is still a considerable gap between understanding the impact of using responsible AI components, the implications of the lack of use, and what is currently applied in the industrial sector. Given that industry has shown an increased commitment to incorporating AI components, works focus on broadening the understanding of manufacturing sector stakeholders of what approaches could be considered within AI life-cycle, reducing the gap between principles and actionable requirements, and defining fundamental considerations based on risk management for incorporating, and managing, AI-based on responsible AI are required. In this work, we present a summary of the most suitable approaches that can be used for implementation and the lessons learned from a European Funded project (ASSISTANT). \textcopyright{} 2022 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Wang2022223,
  type = {Conference Paper},
  title = {Exploring Efficient Microservice Level Parallelism},
  author = {Wang, Xinkai and Li, Chao and Zhang, Lu and Hou, Xiaofeng and Chen, Quan and Guo, Minyi},
  year = {2022},
  series = {Proceedings - 2022 {{IEEE}} 36th {{International Parallel}} and {{Distributed Processing Symposium}}, {{IPDPS}} 2022},
  pages = {223--233},
  doi = {10.1109/IPDPS53621.2022.00030},
  abstract = {The microservice architecture has recently become a driving trend in the cloud by disaggregating a monolithic application into many scenario-oriented service blocks (microservices). The decomposition process results in a highly dynamic execution scenario, in which various chained microservices contend for computing resources in different ways. While parallelism has been exploited at both the instruction/thread level and the task/request level, very limited work has been done with the grain-size of a microservice. Current parallel processing solutions are sub-optimal as they neither capture the unique characteristics of microservices nor consider the uncertainty arises in the microservice environment. In this work we introduce microservice level parallelism (MLP), a technique that aims to precisely coalesce and align parallel microservice chains for better system performance and resource utilization. We identify major issues that prevent servers from effectively exploiting MLP and we define metrics that can guide MLP optimization. We propose v-MLP, a volatility-aware MLP that is able to adapt to a highly heterogeneous and dynamic microservice environment. We show that v-MLP can reduce tail latency by up to 50\% and improve resource utilization by up to 15 \% under various scenarios. \textcopyright{} 2022 IEEE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{waseem2020:systematic,
  title = {A {{Systematic Mapping Study}} on {{Microservices Architecture}} in {{DevOps}}},
  author = {Waseem, Muhammad and Liang, Peng and Shahin, Mojtaba},
  year = {2020},
  month = dec,
  journal = {Journal of Systems and Software},
  volume = {170},
  pages = {110798},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2020.110798},
  urldate = {2023-01-15},
  abstract = {Context: Applying Microservices Architecture (MSA) in DevOps has received significant attention in recent years. However, there exists no comprehensive review of the state of research on this topic. Objective: This work aims to systematically identify, analyze, and classify the literature on MSA in DevOps. Methods: A Systematic Mapping Study (SMS) has been conducted on the literature published between January 2009 and July 2018. Results: Forty-seven studies were finally selected and the key results are: (1) Three themes on the research on MSA in DevOps are ``microservices development and operations in DevOps'', ``approaches and tool support for MSA based systems in DevOps'', and ``MSA migration experiences in DevOps''. (2) 24 problems with their solutions regarding implementing MSA in DevOps are identified. (3) MSA is mainly described by using boxes and lines. (4) Most of the quality attributes are positively affected when employing MSA in DevOps. (5) 50 tools that support building MSA based systems in DevOps are collected. (6) The combination of MSA and DevOps has been applied in a wide range of application domains. Conclusion: The results and findings will benefit researchers and practitioners to conduct further research and bring more dedicated solutions for the issues of MSA in DevOps.},
  langid = {english},
  keywords = {DevOps,Microservices Architecture,Systematic Mapping Study}
}

@incollection{washizaki2017:chapter,
  title = {Chapter {{One}} - {{Pitfalls}} and {{Countermeasures}} in {{Software Quality Measurements}} and {{Evaluations}}},
  booktitle = {Advances in {{Computers}}},
  author = {Washizaki, Hironori},
  editor = {Memon, Atif M.},
  year = {2017},
  month = jan,
  volume = {107},
  pages = {1--22},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.adcom.2017.06.003},
  urldate = {2023-01-15},
  abstract = {This chapter discusses common pitfalls and their countermeasures in software quality measurements and evaluations based on research and practical achievements. The pitfalls include negative Hawthorne effects, organization misalignment, uncertain future, and self-certified quality. Corresponding countermeasures include goal-oriented multidimensional measurements, alignment visualization and exhaustive identification of rationales, prediction incorporating uncertainty and machine learning-based measurement improvement, and standard/pattern-based evaluation.},
  langid = {english},
  keywords = {Goal orientation,GQM,ISO/IEC 25000,SEMAT,Software measurement,Software metrics,Software patterns,Software quality},
  file = {/Users/guru/Zotero/storage/QFN4CFXG/Washizaki_2017_Chapter One - Pitfalls and Countermeasures in Software Quality Measurements and_Advances in Computers.pdf;/Users/guru/Zotero/storage/BNDKZ84R/S0065245817300293.html}
}

@article{Weflen2022,
  type = {Article},
  title = {An Influence Diagram Approach to Automating Lead Time Estimation in {{Agile Kanban}} Project Management},
  author = {Weflen, Eric and MacKenzie, Cameron A. and Rivero, Iris V.},
  year = {2022},
  journal = {Expert Systems with Applications},
  volume = {187},
  doi = {10.1016/j.eswa.2021.115866},
  abstract = {This research introduces an influence diagram based approach to estimating task lead times for Agile Kanban project management. Derived from the principles of lean manufacturing, Agile methodologies including Scrum, Scrumban, and Kanban are common in the software industry and are spreading to other fields. Many teams estimate task delivery to better manage stakeholder expectations and improve decision making. However, the current technique involves calculating a team's story point completion velocity, requiring hours of weekly effort while overlooking the addition, removal, and reprioritization of backlog tasks. While these factors may be critical in Scrum, teams practicing Kanban experience these conditions frequently due to the emphasis on continuous integration and reprioritization. Our alternative approach applies an influence diagram, or Bayesian belief network, to model the uncertainties affecting the lead time. To partially automate the estimation process, an influence diagram based expert system is developed and populated with data from a practicing Kanban team and used to generate a cumulative distribution function to facilitate the communication of probabilistic estimates. A sensitivity analysis is conducted to better understand how each factor influences lead times. This system can assist Kanban teams' stakeholder communication and reduce estimation workload through a more holistic model of lead times. \textcopyright{} 2021},
  publication_stage = {Final},
  source = {Scopus}
}

@article{white2021:environment,
  title = {Environment Modeling for Evaluating System Variants in Model-Based Systems Engineering},
  author = {White, Dustin and Sahlab, Nada and Jazdi, Nasser and Weyrich, Michael},
  year = {2021},
  month = jan,
  journal = {Procedia CIRP},
  series = {54th {{CIRP CMS}} 2021 - {{Towards Digitalized Manufacturing}} 4.0},
  volume = {104},
  pages = {750--755},
  issn = {2212-8271},
  doi = {10.1016/j.procir.2021.11.126},
  urldate = {2023-01-15},
  abstract = {Model-based systems engineering is a methodology for the interdisciplinary system development using different domain models. Considering the intended system's environment and context of usage in early design phases is a way of bridging the gap from system's design to its real-life applicability. Simulations represent a possible approach to consider different constellations but are effortful. Therefore, we present a generic environment modeling approach to evaluate different environmental models with their respective environmental, user and system's impact based on generated system variants derived from system requirements. In this sequential process, we consistently link and evaluate system variants to possible system contexts.},
  langid = {english},
  keywords = {environment model,Model-based systems engineering,system context awareness,variant generation,variant management},
  file = {/Users/guru/Zotero/storage/YJSU5A2M/White et al_2021_Environment modeling for evaluating system variants in model-based systems_Procedia CIRP.pdf;/Users/guru/Zotero/storage/R3YVS588/S2212827121010246.html}
}

@article{wirth2008brief,
  title = {A Brief History of Software Engineering},
  author = {Wirth, Niklaus},
  year = {2008},
  journal = {IEEE Annals of the History of Computing},
  volume = {30},
  number = {3},
  pages = {32--39},
  publisher = {{IEEE}}
}

@inproceedings{Wolf202086,
  type = {Conference Paper},
  title = {Sensemaking Practices in the Everyday Work of {{AI}}/{{ML}} Software Engineering},
  author = {Wolf, Christine T. and Paine, Drew},
  year = {2020},
  series = {Proceedings - 2020 {{IEEE}}/{{ACM}} 42nd {{International Conference}} on {{Software Engineering Workshops}}, {{ICSEW}} 2020},
  pages = {86--92},
  doi = {10.1145/3387940.3391496},
  abstract = {This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and "upstream"models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities. \textcopyright{} 2020 ACM.},
  publication_stage = {Final},
  source = {Scopus},
  keywords = {primary},
  file = {/Users/guru/Zotero/storage/53TNH5RV/eScholarship UC item 3b8526kw (dragged).pdf}
}

@article{Wolferts2022,
  type = {Article},
  title = {Differences between Remote and Analog Design Thinking through the Lens of Distributed Cognition},
  author = {Wolferts, Daniel and Stein, Elisabeth and Bernards, Ann-Kathrin and Reiners, Ren{\'e}},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  doi = {10.3389/frai.2022.915922},
  abstract = {Due to the huge surge in remote work all over the world caused by the COVID-19 pandemic, today's work is largely defined by tools for information exchange as well as new complex problems that must be solved. Design Thinking offers a well-known and established methodological approach for iterative, collaborative and interdisciplinary problem solving. Still, recent circumstances shed a new light on how to facilitate Design Thinking activities in a remote rather than an analog way. Due to Design Thinking's high production of artifacts and its focus on communication and interaction between team members, the theory of Distributed Cognition, specifically the Distributed Cognition for Teamwork (DiCoT) framework, provides an interesting perspective on the recent going-remote of Design Thinking activities. For this, we first highlight differences of analog vs. remote Design Thinking by analyzing corresponding literature from the recent years. Next, we apply the DiCoT framework to those findings, pointing out implications for practical facilitation of Design Thinking activities in an analog and remote setting. Finally, we discuss opportunities through artificial intelligence-based technologies and methods. Copyright \textcopyright{} 2022 Wolferts, Stein, Bernards and Reiners.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Wu2022175,
  type = {Conference Paper},
  title = {A Preliminary Study of Bots Usage in Open Source Community},
  author = {Wu, Xiaojun and Gao, Anze and Zhang, Yang and Wang, Tao and Tang, Yi},
  year = {2022},
  series = {{{ACM International Conference Proceeding Series}}},
  pages = {175--180},
  doi = {10.1145/3545258.3545284},
  abstract = {Bots are seen as a promising approach in software development, which help to deal with the ever-increasing complexity of modern software engineering and development. The number of bots in open source community, such as GitHub, has expanded substantially over the last three years. Due to its increasing popularity, it is essential to characterize the current usage of bots in practices. In this paper, we present an empirical study of bots usage in GitHub community. By analyzing 7,399 projects from GitHub, we find that 4,148 (56\%) projects have used bots. Through automatic identification and manual detection, we collect a total of 196 bots. We then analyze and classify them into 4 categories and 14 topics. Finally, we discuss some raised implications for bots in current GitHub community. \textcopyright{} 2022 Association for Computing Machinery.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Xia2022,
  type = {Article},
  title = {Artificial Intelligence for Higher Education Development and Teaching Skills},
  author = {Xia, Xiaolin and Li, Xiaojun},
  year = {2022},
  journal = {Wireless Communications and Mobile Computing},
  volume = {2022},
  doi = {10.1155/2022/7614337},
  abstract = {In the context of the information age, the development of artificial intelligence is flourishing, and it is becoming more and more closely integrated with economic life, which is an inevitable trend of future social development. Education is also inseparable from the support of science and technology. The penetration of artificial intelligence has changed traditional teaching models and methods. Under the influence of artificial intelligence technology, college education is developing more and more in the direction of informatization and intelligence. For college teachers, the study and application of artificial intelligence is an important and necessary means to achieve professional development. It is more important to train students to take independent self-study exams and focus on applying social skills, so that students can better adapt to the coming era of intelligence. Therefore, the purpose of this article is to explore the development of higher education and the improvement of teaching skills based on artificial intelligence and to analyze the problems and solutions in the process of higher education development. This article will use the research methods of specific problems and specific analysis to compare the data and draw conclusions. The research results show that in the information-based education innovation created by knowledge sharing, the teaching goals and methods are constantly changing. And about 85\% of the students believe that the development prospect of intelligent teaching is good, which verifies the feasibility of artificial intelligence technology in the development of college education. Only by training students' imagination, creativity, critical thinking, and autonomous learning can they adapt to today's rapidly developing society. Therefore, active learning and research on artificial intelligence is the embodiment of cultivating lifelong learning ability, improving rich teaching skills, constantly summing up experience, actively improving and trimming relevant change programs, and jointly promoting the rapid development of higher education under artificial intelligence. \textcopyright{} 2022 Xiaolin Xia and Xiaojun Li.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{xu2021:will,
  title = {Will Bots Take over the Supply Chain? {{Revisiting}} Agent-Based Supply Chain Automation},
  shorttitle = {Will Bots Take over the Supply Chain?},
  author = {Xu, Liming and Mak, Stephen and Brintrup, Alexandra},
  year = {2021},
  month = nov,
  journal = {International Journal of Production Economics},
  volume = {241},
  pages = {108279},
  issn = {0925-5273},
  doi = {10.1016/j.ijpe.2021.108279},
  urldate = {2023-01-15},
  abstract = {Agent-based systems have the capability to fuse information from many distributed sources and create better plans faster. This feature makes agent-based systems naturally suitable to address the challenges in Supply Chain Management (SCM). Although agent-based supply chains systems have been proposed since the early 2000s; industrial uptake of them has been lagging. The reasons quoted include the immaturity of technology, a lack of interoperability with supply chain information systems, and a lack of trust in Artificial Intelligence (AI). In this paper, we revisit the agent-based supply chain and review the state of the art. We find that agent-based technology has matured, and other supporting technologies that are penetrating supply chains; are filling in gaps, leaving the concept applicable to a wider range of functions. For example, the ubiquity of IoT technology helps agents ``sense'' the state of affairs in a supply chain and opens up new possibilities for automation. Digital ledgers help securely transfer data between third parties, making agent-based information sharing possible, without the need to integrate Enterprise Resource Planning (ERP) systems. Learning functionality in agents enables agents to move beyond automation and towards autonomy. We note this convergence effect through conceptualising an agent-based supply chain framework, reviewing its components, and highlighting research challenges that need to be addressed in moving forward.},
  langid = {english},
  keywords = {Agent,Agent-based systems,Automation,Autonomy,Supply chains},
  file = {/Users/guru/Zotero/storage/8A9KISXP/S0925527321002553.html}
}

@article{Yaacoub2022115,
  type = {Article},
  title = {Robotics Cyber Security: Vulnerabilities, Attacks, Countermeasures, and Recommendations},
  author = {Yaacoub, Jean-Paul A. and Noura, Hassan N. and Salman, Ola and Chehab, Ali},
  year = {2022},
  journal = {International Journal of Information Security},
  volume = {21},
  number = {1},
  pages = {115--158},
  doi = {10.1007/s10207-021-00545-8},
  abstract = {The recent digital revolution led robots to become integrated more than ever into different domains such as agricultural, medical, industrial, military, police (law enforcement), and logistics. Robots are devoted to serve, facilitate, and enhance the human life. However, many incidents have been occurring, leading to serious injuries and devastating impacts such as the unnecessary loss of human lives. Unintended accidents will always take place, but the ones caused by malicious attacks represent a very challenging issue. This includes maliciously hijacking and controlling robots and causing serious economic and financial losses. This paper reviews the main security vulnerabilities, threats, risks, and their impacts, and the main security attacks within the robotics domain. In this context, different approaches and recommendations are presented in order to enhance and improve the security level of robotic systems such as multi-factor device/user authentication schemes, in addition to multi-factor cryptographic algorithms. We also review the recently presented security solutions for robotic systems. \textcopyright{} 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{yang2021:quality,
  title = {Quality Monitoring and Assessment of Deployed Deep Learning Models for Network {{AIOps}}},
  author = {Yang, Lixuan and Rossi, Dario},
  year = {2021},
  month = nov,
  journal = {IEEE Network},
  volume = {35},
  number = {6},
  pages = {84--90},
  issn = {1558-156X},
  doi = {10.1109/MNET.001.2100227},
  abstract = {Artificial intelligence (AI) has recently attracted a lot of attention, transitioning from research labs to a wide range of successful deployments in many fields, which is particularly true for deep learning (DL) techniques. Ultimately, DL models, being software artifacts, need to be regularly maintained and updated: AIOps is the logical extension of the DevOps software development practices to AI software applied to network operation and management. In the life cycle of a DL model deployment, it is important to assess the quality of deployed models, to detect ``stale'' models and prioritize their update. In this article, we cover the issue in the context of network management, proposing simple but effective techniques for quality assessment of individual inference, and for overall model quality tracking over multiple inferences, that we apply to two use cases, representative of the network management and image recognition fields.},
  keywords = {Artificial intelligence,Context modeling,Deep learning,Image recognition,Monitoring,primary,Quality assessment,Software},
  file = {/Users/guru/Zotero/storage/CI2YJBY7/Yang_Rossi_2021_Quality monitoring and assessment of deployed deep learning models for network_IEEE Network.pdf}
}

@article{yang2023:adoption,
  title = {Adoption of Information and Digital Technologies for Sustainable Smart Manufacturing Systems for Industry 4.0 in Small, Medium, and Micro Enterprises ({{SMMEs}})},
  author = {Yang, Li and Zou, Haobo and Shang, Chao and Ye, Xiaoming and Rani, Pratibha},
  year = {2023},
  month = mar,
  journal = {Technological Forecasting and Social Change},
  volume = {188},
  pages = {122308},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2022.122308},
  urldate = {2023-01-15},
  abstract = {An enterprise reference model in a conventional system paradigm guides users when selecting manufacturing elements, configuring the required elements into a manufacturing system, modeling the system options for evaluative processes, and comparing the system solutions with the predefined performance metrics. At present, digital innovation has a close connection with firms' sustainability. Sustainability and digital innovation are two essential components of the circular economy. In this study, an integrated decision-making framework called the q-ROF-MEREC-RS-DNMA is developed. In this approach, the q-ROF-MEREC-RS method is applied to compute the subjective and objective weights of criteria to the adoption of information and digital technologies for sustainable smart manufacturing systems for Industry 4.0 in small, medium, and micro enterprises (SMMEs), and the q-ROF-DNMA model is used to assess the preferences of industries over different criteria to the adoption of information and digital technologies for sustainable smart manufacturing systems for Industry 4.0 in SMMEs. An empirical case study to evaluate the main criteria for the adoption of information and digital technologies for sustainable smart manufacturing systems for Industry 4.0 in small, medium, and micro enterprises (SMMEs) is taken. Also, comparison and sensitivity investigation are made to show the superiority of the developed framework.},
  langid = {english},
  keywords = {DNMA,Industry 4.0,MEREC,Multi-criteria decision-making,Q-rung orthopair fuzzy sets,Rank sum weight},
  file = {/Users/guru/Zotero/storage/HMNZDV95/S0040162522008290.html}
}

@inproceedings{Yildirim2022,
  type = {Conference Paper},
  title = {How Experienced Designers of Enterprise Applications Engage {{AI}} as a Design Material},
  author = {Yildirim, Nur and Kass, Alex and Tung, Teresa and Upton, Connor and Costello, Donnacha and Giusti, Robert and Lacin, Sinem and Lovic, Sara and O'Neill, James M and Meehan, Rudi O'Reilly and {\'O} Loide{\'a}in, Eoin and Pini, Azzurra and Corcoran, Medb and Hayes, Jeremiah and Cahalane, Diarmuid J and Shivhare, Gaurav and Castoro, Luigi and Caruso, Giovanni and Oh, Changhoon and McCann, James and Forlizzi, Jodi and Zimmerman, John},
  year = {2022},
  series = {Conference on {{Human Factors}} in {{Computing Systems}} - {{Proceedings}}},
  doi = {10.1145/3491102.3517491},
  abstract = {HCI research has explored AI as a design material, suggesting that designers can envision AI's design opportunities to improve UX. Recent research claimed that enterprise applications offer an opportunity for AI innovation at the user experience level. We conducted design workshops to explore the practices of experienced designers who work on cross-functional AI teams in the enterprise. We discussed how designers successfully work with and struggle with AI. Our findings revealed that designers can innovate at the system and service levels. We also discovered that making a case for an AI feature's return on investment is a barrier for designers when they propose AI concepts and ideas. Our discussions produced novel insights on designers' role on AI teams, and the boundary objects they used for collaborating with data scientists. We discuss the implications of these findings as opportunities for future research aiming to empower designers in working with data and AI. \textcopyright{} 2022 Owner/Author.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{yoshida2016:fsx,
  title = {{{FSX}}: Fine-Grained Incremental Unit Test Generation for {{C}}/{{C}}++ Programs},
  shorttitle = {{{FSX}}},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Yoshida, Hiroaki and Tokumoto, Susumu and Prasad, Mukul R. and Ghosh, Indradeep and Uehara, Tadahiro},
  year = {2016},
  month = jul,
  series = {{{ISSTA}} 2016},
  pages = {106--117},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2931037.2931055},
  urldate = {2023-01-15},
  abstract = {Automated unit test generation bears the promise of significantly reducing test cost and hence improving software quality. However, the maintenance cost of the automatically generated tests presents a significant barrier to adoption of this technology. To address this challenge, we propose a novel technique for automated and fine-grained incremental generation of unit tests through minimal augmentation of an existing test suite. The technique uses iterative, incremental refinement of test-drivers and symbolic execution, guided by a diagnostics engine. The diagnostics engine works off a novel precise and efficient byte-level dynamic dependence analysis built using Reduced Ordered Binary Decision Diagrams (ROBDDs). We present a tool FSX implementing this technique and evaluate it under two practical use-cases of incremental unit test generation, on five revisions of the open-source software iPerf, as well as on 3 large subjects, comprising more than 60 thousand lines of code, from in-house commercial network products. The evaluation shows that FSX can generate high-quality unit tests on large industrial software while minimizing the maintenance cost of the overall test-suite.},
  isbn = {978-1-4503-4390-9},
  keywords = {Automatic test generation,symbolic execution,unit testing},
  file = {/Users/guru/Zotero/storage/3DQT89L6/Yoshida et al_2016_FSX_Proceedings of the 25th International Symposium on Software Testing and Analysis.pdf}
}

@article{Yuan2014,
  type = {Conference Paper},
  title = {A Systematic Survey of Self-Protecting Software Systems},
  author = {Yuan, Eric and Esfahani, Naeem and Malek, Sam},
  year = {2014},
  journal = {ACM Transactions on Autonomous and Adaptive Systems},
  volume = {8},
  number = {4},
  doi = {10.1145/2555611},
  abstract = {Self-protecting software systems are a class of autonomic systems capable of detecting and mitigating security threats at runtime. They are growing in importance, as the stovepipe static methods of securing software systems have been shown to be inadequate for the challenges posed by modern software systems. Self-protection, like other self- properties, allows the system to adapt to the changing environment through autonomic means without much human intervention, and can thereby be responsive, agile, and cost effective. While existing research has made significant progress towards autonomic and adaptive security, gaps and challenges remain. This article presents a significant extension of our preliminary study in this area. In particular, unlike our preliminary study, here we have followed a systematic literature review process, which has broadened the scope of our study and strengthened the validity of our conclusions. By proposing and applying a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area, we have identified key patterns, trends and challenges in the existing approaches, which reveals a number of opportunities that will shape the focus of future research efforts. \textcopyright{} 2014 ACM.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{Za2018268,
  type = {Article},
  title = {Exploring Foundations for Using Simulations in {{IS}} Research},
  author = {Za, Stefano and Spagnoletti, Paolo and Winter, Robert and Mettler, Tobias},
  year = {2018},
  journal = {Communications of the Association for Information Systems},
  volume = {42},
  number = {1},
  pages = {268--300},
  doi = {10.17705/1CAIS.04210},
  abstract = {Researchers in many fields have adopted simulation to understand a system's behavior by imitating it through an artificial object that exhibits nearly identical behavior. Although simulation approaches have been widely adopted for theory building in fields such as engineering, computer science, management, and social sciences, researchers in the IS field often overlook their potential. In this paper, we examine how IS research uses different simulation approaches and, thereby, provide insights and methodological recommendations for future studies. From reviewing the literature on simulation studies published in top-tier IS journals, we define three classes of simulations: the self-organizing, the elementary, and the situated. We identify a set of stylized facts for characterizing the ways in which IS simulation studies present the premise, the inference, and the contribution. As a result, we provide guidance to future simulation researchers in designing and presenting their findings. \textcopyright{} 2018 by the Association for Information Systems.},
  publication_stage = {Final},
  source = {Scopus}
}

@inproceedings{Zanin2020,
  type = {Conference Paper},
  title = {Model-Based Testing in Agile Projects: {{An}} Approach Based on Domain-Specific Languages},
  author = {Zanin, Aline and Zorzo, Avelino Fracisco and Nunes, Henry Cabral},
  year = {2020},
  series = {23rd {{Iberoamerican Conference}} on {{Software Engineering}}, {{CIbSE}} 2020},
  abstract = {Model-Based Testing (MBT) can bring several benefits to software quality. However, generally, MBT is applied in traditional software development lifecycle models, with few studies exploring its application in agile software development context. Hence, usually, agile development teams (AT) do not benefit from the advantages that the MBT technique provides, for example, reuse of artifacts and traceability between requirements and test artifacts. Thus, this article presents an approach for applying MBT in agile software development teams. This approach is based on the use of a semi-natural language to write scenarios for the automatic generation of models and test scripts. To exemplify the application of this approach, we also present a Domain-Specific Language (DSL) called Aquila, in which new functional test related keywords are added to the Gherkin DSL. We also present, based on a literature review, the majors challenges and difficulties of applying MBT in AT. To validate the proposed approach a Focus Group study was used. \textcopyright{} CIbSE 2020.},
  publication_stage = {Final},
  source = {Scopus}
}

@article{zhang2003machine,
  title = {Machine Learning and Software Engineering},
  author = {Zhang, Du and Tsai, Jeffrey JP},
  year = {2003},
  journal = {Software Quality Journal},
  volume = {11},
  number = {2},
  pages = {87--119},
  publisher = {{Springer}}
}

@article{zhang2022:riskaware,
  title = {Towards Risk-Aware Artificial Intelligence and Machine Learning Systems: {{An}} Overview},
  shorttitle = {Towards Risk-Aware Artificial Intelligence and Machine Learning Systems},
  author = {Zhang, Xiaoge and Chan, Felix T. S. and Yan, Chao and Bose, Indranil},
  year = {2022},
  month = aug,
  journal = {Decision Support Systems},
  volume = {159},
  pages = {113800},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2022.113800},
  urldate = {2023-01-15},
  abstract = {The adoption of artificial intelligence (AI) and machine learning (ML) in risk-sensitive environments is still in its infancy because it lacks a systematic framework for reasoning about risk, uncertainty, and their potentially catastrophic consequences. In high-impact applications, inference on risk and uncertainty will become decisive in the adoption of AI/ML systems. To this end, there is a pressing need for a consolidated understanding on the varied risks arising from AI/ML systems, and how these risks and their side effects emerge and unfold in practice. In this paper, we provide a systematic and comprehensive overview of a broad array of inherent risks that can arise in AI/ML systems. These risks are grouped into two categories: data-level risk (e.g., data bias, dataset shift, out-of-domain data, and adversarial attacks) and model-level risk (e.g., model bias, misspecification, and uncertainty). In addition, we highlight the research needs for developing a holistic framework for risk management dedicated to AI/ML systems to hedge the corresponding risks. Furthermore, we outline several research related challenges and opportunities along with the development of risk-aware AI/ML systems. Our research has the potential to significantly increase the credibility of deploying AI/ML models in high-stakes decision settings for facilitating safety assurance, and preventing systems from unintended consequences.},
  langid = {english},
  keywords = {Artificial intelligence and machine learning,Risk analysis,Risk management,Safety assurance,Uncertainty},
  file = {/Users/guru/Zotero/storage/A5L5MLQ8/S0167923622000719.html}
}
